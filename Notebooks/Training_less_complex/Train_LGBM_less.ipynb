{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f68850-3923-4ea7-adb2-7bbc418977e2",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec387e8-70fb-462c-be58-c11894f0148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import joblib\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as cpickle\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a542560-1b4d-468e-9278-1a31c8207cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing each csv files\n",
    "orders = pd.read_csv(\"/Users/data/Desktop/instacart-market-basket-analysis/csvs/orders.csv\")\n",
    "order_products_train = pd.read_csv(\"/Users/data/Desktop/instacart-market-basket-analysis/csvs/order_products__train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b6d651-8958-4bc1-9f21-84d16cfbe015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing features \n",
    "users_df =  pd.read_pickle(\"/Users/data/Desktop/Features@Library/Features/users_df.pkl\")\n",
    "products_df =  pd.read_pickle(\"/Users/data/Desktop/Features@Library/Features/products_df.pkl\")\n",
    "user_x_product =  pd.read_pickle(\"/Users/data/Desktop/Features@Library/Features/uxp_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a01a15-cc7a-4071-a0a9-0dcddccf0106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>times_brought</th>\n",
       "      <th>uxp_total_reordered</th>\n",
       "      <th>uxp_first_order_num</th>\n",
       "      <th>uxp_avg_cart_pos</th>\n",
       "      <th>uxp_min_cart_pos</th>\n",
       "      <th>uxp_max_cart_pos</th>\n",
       "      <th>uxp_median_cart_pos</th>\n",
       "      <th>uxp_last_order_num</th>\n",
       "      <th>...</th>\n",
       "      <th>reordered_ratio</th>\n",
       "      <th>total_num_orders</th>\n",
       "      <th>user_total_prods</th>\n",
       "      <th>user_unique_prods</th>\n",
       "      <th>user_average_basket</th>\n",
       "      <th>user_order_starts_at</th>\n",
       "      <th>avg_no_prds_each_purchase</th>\n",
       "      <th>median_no_prds_each_purchase</th>\n",
       "      <th>min_no_prds_each_purchase</th>\n",
       "      <th>max_no_prds_each_purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>343</td>\n",
       "      <td>59</td>\n",
       "      <td>18</td>\n",
       "      <td>0.172012</td>\n",
       "      <td>431534</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10258</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>343</td>\n",
       "      <td>59</td>\n",
       "      <td>18</td>\n",
       "      <td>0.172012</td>\n",
       "      <td>431534</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>343</td>\n",
       "      <td>59</td>\n",
       "      <td>18</td>\n",
       "      <td>0.172012</td>\n",
       "      <td>431534</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12427</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>343</td>\n",
       "      <td>59</td>\n",
       "      <td>18</td>\n",
       "      <td>0.172012</td>\n",
       "      <td>431534</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13032</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>343</td>\n",
       "      <td>59</td>\n",
       "      <td>18</td>\n",
       "      <td>0.172012</td>\n",
       "      <td>431534</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  product_id  times_brought  uxp_total_reordered  \\\n",
       "0        1         196             10                    9   \n",
       "1        1       10258              9                    8   \n",
       "2        1       10326              1                    0   \n",
       "3        1       12427             10                    9   \n",
       "4        1       13032              3                    2   \n",
       "\n",
       "   uxp_first_order_num  uxp_avg_cart_pos  uxp_min_cart_pos  uxp_max_cart_pos  \\\n",
       "0                    1          1.400000                 1                 4   \n",
       "1                    2          3.333333                 2                 6   \n",
       "2                    5          5.000000                 5                 5   \n",
       "3                    1          3.300000                 1                 9   \n",
       "4                    2          6.333333                 5                 8   \n",
       "\n",
       "   uxp_median_cart_pos  uxp_last_order_num  ...  reordered_ratio  \\\n",
       "0                  1.0                  10  ...         0.694915   \n",
       "1                  3.0                  10  ...         0.694915   \n",
       "2                  5.0                   5  ...         0.694915   \n",
       "3                  2.5                  10  ...         0.694915   \n",
       "4                  6.0                  10  ...         0.694915   \n",
       "\n",
       "   total_num_orders  user_total_prods  user_unique_prods  user_average_basket  \\\n",
       "0               343                59                 18             0.172012   \n",
       "1               343                59                 18             0.172012   \n",
       "2               343                59                 18             0.172012   \n",
       "3               343                59                 18             0.172012   \n",
       "4               343                59                 18             0.172012   \n",
       "\n",
       "   user_order_starts_at  avg_no_prds_each_purchase  \\\n",
       "0                431534                        5.9   \n",
       "1                431534                        5.9   \n",
       "2                431534                        5.9   \n",
       "3                431534                        5.9   \n",
       "4                431534                        5.9   \n",
       "\n",
       "   median_no_prds_each_purchase  min_no_prds_each_purchase  \\\n",
       "0                           5.5                          4   \n",
       "1                           5.5                          4   \n",
       "2                           5.5                          4   \n",
       "3                           5.5                          4   \n",
       "4                           5.5                          4   \n",
       "\n",
       "   max_no_prds_each_purchase  \n",
       "0                          9  \n",
       "1                          9  \n",
       "2                          9  \n",
       "3                          9  \n",
       "4                          9  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging user_x_product features with users features\n",
    "data = user_x_product.merge(users_df, on='user_id', how='left')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d499a4-b3d0-4ea1-875a-19ed016fe4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>times_brought</th>\n",
       "      <th>uxp_total_reordered</th>\n",
       "      <th>uxp_first_order_num</th>\n",
       "      <th>uxp_avg_cart_pos</th>\n",
       "      <th>uxp_min_cart_pos</th>\n",
       "      <th>uxp_max_cart_pos</th>\n",
       "      <th>uxp_median_cart_pos</th>\n",
       "      <th>uxp_last_order_num</th>\n",
       "      <th>...</th>\n",
       "      <th>prod_tot_reorders</th>\n",
       "      <th>prod_reordered_ratio</th>\n",
       "      <th>avg_cart_position</th>\n",
       "      <th>median_cart_position</th>\n",
       "      <th>sum_cart_position</th>\n",
       "      <th>min_cart_position</th>\n",
       "      <th>max_cart_position</th>\n",
       "      <th>std_cart_position</th>\n",
       "      <th>aisle_reorder_ratio</th>\n",
       "      <th>department_reorder_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>27791</td>\n",
       "      <td>0.776480</td>\n",
       "      <td>3.721774</td>\n",
       "      <td>2.0</td>\n",
       "      <td>133206</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>4.110813</td>\n",
       "      <td>0.638832</td>\n",
       "      <td>0.653460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10258</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1389</td>\n",
       "      <td>0.713772</td>\n",
       "      <td>4.277492</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8324</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.567502</td>\n",
       "      <td>0.519170</td>\n",
       "      <td>0.574180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3603</td>\n",
       "      <td>0.652009</td>\n",
       "      <td>4.191097</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23160</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>0.718104</td>\n",
       "      <td>0.649913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12427</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>4797</td>\n",
       "      <td>0.740735</td>\n",
       "      <td>4.760037</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30826</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>4.782450</td>\n",
       "      <td>0.591986</td>\n",
       "      <td>0.574180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13032</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>2465</td>\n",
       "      <td>0.657158</td>\n",
       "      <td>5.622767</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21091</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>5.345184</td>\n",
       "      <td>0.571584</td>\n",
       "      <td>0.560922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  product_id  times_brought  uxp_total_reordered  \\\n",
       "0        1         196             10                    9   \n",
       "1        1       10258              9                    8   \n",
       "2        1       10326              1                    0   \n",
       "3        1       12427             10                    9   \n",
       "4        1       13032              3                    2   \n",
       "\n",
       "   uxp_first_order_num  uxp_avg_cart_pos  uxp_min_cart_pos  uxp_max_cart_pos  \\\n",
       "0                    1          1.400000                 1                 4   \n",
       "1                    2          3.333333                 2                 6   \n",
       "2                    5          5.000000                 5                 5   \n",
       "3                    1          3.300000                 1                 9   \n",
       "4                    2          6.333333                 5                 8   \n",
       "\n",
       "   uxp_median_cart_pos  uxp_last_order_num  ...  prod_tot_reorders  \\\n",
       "0                  1.0                  10  ...              27791   \n",
       "1                  3.0                  10  ...               1389   \n",
       "2                  5.0                   5  ...               3603   \n",
       "3                  2.5                  10  ...               4797   \n",
       "4                  6.0                  10  ...               2465   \n",
       "\n",
       "   prod_reordered_ratio  avg_cart_position  median_cart_position  \\\n",
       "0              0.776480           3.721774                   2.0   \n",
       "1              0.713772           4.277492                   3.0   \n",
       "2              0.652009           4.191097                   3.0   \n",
       "3              0.740735           4.760037                   3.0   \n",
       "4              0.657158           5.622767                   4.0   \n",
       "\n",
       "   sum_cart_position  min_cart_position  max_cart_position  std_cart_position  \\\n",
       "0             133206                  1                 60           4.110813   \n",
       "1               8324                  1                 29           3.567502   \n",
       "2              23160                  1                 37           3.611700   \n",
       "3              30826                  1                 61           4.782450   \n",
       "4              21091                  1                 66           5.345184   \n",
       "\n",
       "   aisle_reorder_ratio  department_reorder_ratio  \n",
       "0             0.638832                  0.653460  \n",
       "1             0.519170                  0.574180  \n",
       "2             0.718104                  0.649913  \n",
       "3             0.591986                  0.574180  \n",
       "4             0.571584                  0.560922  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging product features with data.\n",
    "data = data.merge(products_df, on='product_id', how='left')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469cf483-4800-4b31-b3df-ba9582a870e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13307953, 48)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe2d555-d467-4987-9b22-5097f825db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1187899</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1492625</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2774568</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>329954</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2196797</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
       "10   1187899        1    train            11          4                  8   \n",
       "25   1492625        2    train            15          1                 11   \n",
       "38   2774568        3     test            13          5                 15   \n",
       "44    329954        4     test             6          3                 12   \n",
       "49   2196797        5    train             5          0                 11   \n",
       "\n",
       "    days_since_prior_order  \n",
       "10                    14.0  \n",
       "25                    30.0  \n",
       "38                    11.0  \n",
       "44                    30.0  \n",
       "49                     6.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first get the future orders.(train and test eval_set)\n",
    "future_orders = orders[((orders['eval_set'] == 'train') | (orders['eval_set'] == 'test'))]\n",
    "future_orders.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ed53a5-96f1-4490-945a-2e19d43578bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>eval_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>1492625</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>2774568</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>329954</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5</td>\n",
       "      <td>2196797</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  order_id eval_set\n",
       "10        1   1187899    train\n",
       "25        2   1492625    train\n",
       "38        3   2774568     test\n",
       "44        4    329954     test\n",
       "49        5   2196797    train"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_orders = future_orders[['user_id','order_id','eval_set']]\n",
    "future_orders.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66fb8db-420b-4528-aa5c-be06a81b31a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>times_brought</th>\n",
       "      <th>uxp_total_reordered</th>\n",
       "      <th>uxp_first_order_num</th>\n",
       "      <th>uxp_avg_cart_pos</th>\n",
       "      <th>uxp_min_cart_pos</th>\n",
       "      <th>uxp_max_cart_pos</th>\n",
       "      <th>uxp_median_cart_pos</th>\n",
       "      <th>uxp_last_order_num</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_cart_position</th>\n",
       "      <th>median_cart_position</th>\n",
       "      <th>sum_cart_position</th>\n",
       "      <th>min_cart_position</th>\n",
       "      <th>max_cart_position</th>\n",
       "      <th>std_cart_position</th>\n",
       "      <th>aisle_reorder_ratio</th>\n",
       "      <th>department_reorder_ratio</th>\n",
       "      <th>order_id</th>\n",
       "      <th>eval_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.721774</td>\n",
       "      <td>2.0</td>\n",
       "      <td>133206</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>4.110813</td>\n",
       "      <td>0.638832</td>\n",
       "      <td>0.653460</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10258</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>4.277492</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8324</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.567502</td>\n",
       "      <td>0.519170</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.191097</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23160</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>0.718104</td>\n",
       "      <td>0.649913</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12427</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>4.760037</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30826</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>4.782450</td>\n",
       "      <td>0.591986</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13032</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.622767</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21091</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>5.345184</td>\n",
       "      <td>0.571584</td>\n",
       "      <td>0.560922</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  product_id  times_brought  uxp_total_reordered  \\\n",
       "0        1         196             10                    9   \n",
       "1        1       10258              9                    8   \n",
       "2        1       10326              1                    0   \n",
       "3        1       12427             10                    9   \n",
       "4        1       13032              3                    2   \n",
       "\n",
       "   uxp_first_order_num  uxp_avg_cart_pos  uxp_min_cart_pos  uxp_max_cart_pos  \\\n",
       "0                    1          1.400000                 1                 4   \n",
       "1                    2          3.333333                 2                 6   \n",
       "2                    5          5.000000                 5                 5   \n",
       "3                    1          3.300000                 1                 9   \n",
       "4                    2          6.333333                 5                 8   \n",
       "\n",
       "   uxp_median_cart_pos  uxp_last_order_num  ...  avg_cart_position  \\\n",
       "0                  1.0                  10  ...           3.721774   \n",
       "1                  3.0                  10  ...           4.277492   \n",
       "2                  5.0                   5  ...           4.191097   \n",
       "3                  2.5                  10  ...           4.760037   \n",
       "4                  6.0                  10  ...           5.622767   \n",
       "\n",
       "   median_cart_position  sum_cart_position  min_cart_position  \\\n",
       "0                   2.0             133206                  1   \n",
       "1                   3.0               8324                  1   \n",
       "2                   3.0              23160                  1   \n",
       "3                   3.0              30826                  1   \n",
       "4                   4.0              21091                  1   \n",
       "\n",
       "   max_cart_position  std_cart_position  aisle_reorder_ratio  \\\n",
       "0                 60           4.110813             0.638832   \n",
       "1                 29           3.567502             0.519170   \n",
       "2                 37           3.611700             0.718104   \n",
       "3                 61           4.782450             0.591986   \n",
       "4                 66           5.345184             0.571584   \n",
       "\n",
       "   department_reorder_ratio  order_id  eval_set  \n",
       "0                  0.653460   1187899     train  \n",
       "1                  0.574180   1187899     train  \n",
       "2                  0.649913   1187899     train  \n",
       "3                  0.574180   1187899     train  \n",
       "4                  0.560922   1187899     train  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging data with future orders\n",
    "data = data.merge(future_orders,on='user_id',how='left')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17d4664-c726-428b-8a46-218e50f513da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>times_brought</th>\n",
       "      <th>uxp_total_reordered</th>\n",
       "      <th>uxp_first_order_num</th>\n",
       "      <th>uxp_avg_cart_pos</th>\n",
       "      <th>uxp_min_cart_pos</th>\n",
       "      <th>uxp_max_cart_pos</th>\n",
       "      <th>uxp_median_cart_pos</th>\n",
       "      <th>uxp_last_order_num</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_cart_position</th>\n",
       "      <th>median_cart_position</th>\n",
       "      <th>sum_cart_position</th>\n",
       "      <th>min_cart_position</th>\n",
       "      <th>max_cart_position</th>\n",
       "      <th>std_cart_position</th>\n",
       "      <th>aisle_reorder_ratio</th>\n",
       "      <th>department_reorder_ratio</th>\n",
       "      <th>order_id</th>\n",
       "      <th>eval_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.721774</td>\n",
       "      <td>2.0</td>\n",
       "      <td>133206</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>4.110813</td>\n",
       "      <td>0.638832</td>\n",
       "      <td>0.653460</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10258</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>4.277492</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8324</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.567502</td>\n",
       "      <td>0.519170</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.191097</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23160</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>0.718104</td>\n",
       "      <td>0.649913</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12427</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>4.760037</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30826</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>4.782450</td>\n",
       "      <td>0.591986</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13032</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.622767</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21091</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>5.345184</td>\n",
       "      <td>0.571584</td>\n",
       "      <td>0.560922</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  product_id  times_brought  uxp_total_reordered  \\\n",
       "0        1         196             10                    9   \n",
       "1        1       10258              9                    8   \n",
       "2        1       10326              1                    0   \n",
       "3        1       12427             10                    9   \n",
       "4        1       13032              3                    2   \n",
       "\n",
       "   uxp_first_order_num  uxp_avg_cart_pos  uxp_min_cart_pos  uxp_max_cart_pos  \\\n",
       "0                    1          1.400000                 1                 4   \n",
       "1                    2          3.333333                 2                 6   \n",
       "2                    5          5.000000                 5                 5   \n",
       "3                    1          3.300000                 1                 9   \n",
       "4                    2          6.333333                 5                 8   \n",
       "\n",
       "   uxp_median_cart_pos  uxp_last_order_num  ...  avg_cart_position  \\\n",
       "0                  1.0                  10  ...           3.721774   \n",
       "1                  3.0                  10  ...           4.277492   \n",
       "2                  5.0                   5  ...           4.191097   \n",
       "3                  2.5                  10  ...           4.760037   \n",
       "4                  6.0                  10  ...           5.622767   \n",
       "\n",
       "   median_cart_position  sum_cart_position  min_cart_position  \\\n",
       "0                   2.0             133206                  1   \n",
       "1                   3.0               8324                  1   \n",
       "2                   3.0              23160                  1   \n",
       "3                   3.0              30826                  1   \n",
       "4                   4.0              21091                  1   \n",
       "\n",
       "   max_cart_position  std_cart_position  aisle_reorder_ratio  \\\n",
       "0                 60           4.110813             0.638832   \n",
       "1                 29           3.567502             0.519170   \n",
       "2                 37           3.611700             0.718104   \n",
       "3                 61           4.782450             0.591986   \n",
       "4                 66           5.345184             0.571584   \n",
       "\n",
       "   department_reorder_ratio  order_id  eval_set  \n",
       "0                  0.653460   1187899     train  \n",
       "1                  0.574180   1187899     train  \n",
       "2                  0.649913   1187899     train  \n",
       "3                  0.574180   1187899     train  \n",
       "4                  0.560922   1187899     train  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting only the training data set\n",
    "data_train = data[data['eval_set'] == 'train']\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b9bec95-f89a-419b-90a4-02580d6d225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>times_brought</th>\n",
       "      <th>uxp_total_reordered</th>\n",
       "      <th>uxp_first_order_num</th>\n",
       "      <th>uxp_avg_cart_pos</th>\n",
       "      <th>uxp_min_cart_pos</th>\n",
       "      <th>uxp_max_cart_pos</th>\n",
       "      <th>uxp_median_cart_pos</th>\n",
       "      <th>uxp_last_order_num</th>\n",
       "      <th>...</th>\n",
       "      <th>median_cart_position</th>\n",
       "      <th>sum_cart_position</th>\n",
       "      <th>min_cart_position</th>\n",
       "      <th>max_cart_position</th>\n",
       "      <th>std_cart_position</th>\n",
       "      <th>aisle_reorder_ratio</th>\n",
       "      <th>department_reorder_ratio</th>\n",
       "      <th>order_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>reordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>133206</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>4.110813</td>\n",
       "      <td>0.638832</td>\n",
       "      <td>0.653460</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10258</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8324</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.567502</td>\n",
       "      <td>0.519170</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23160</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>0.718104</td>\n",
       "      <td>0.649913</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12427</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30826</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>4.782450</td>\n",
       "      <td>0.591986</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13032</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21091</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>5.345184</td>\n",
       "      <td>0.571584</td>\n",
       "      <td>0.560922</td>\n",
       "      <td>1187899</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  product_id  times_brought  uxp_total_reordered  \\\n",
       "0        1         196             10                    9   \n",
       "1        1       10258              9                    8   \n",
       "2        1       10326              1                    0   \n",
       "3        1       12427             10                    9   \n",
       "4        1       13032              3                    2   \n",
       "\n",
       "   uxp_first_order_num  uxp_avg_cart_pos  uxp_min_cart_pos  uxp_max_cart_pos  \\\n",
       "0                    1          1.400000                 1                 4   \n",
       "1                    2          3.333333                 2                 6   \n",
       "2                    5          5.000000                 5                 5   \n",
       "3                    1          3.300000                 1                 9   \n",
       "4                    2          6.333333                 5                 8   \n",
       "\n",
       "   uxp_median_cart_pos  uxp_last_order_num  ...  median_cart_position  \\\n",
       "0                  1.0                  10  ...                   2.0   \n",
       "1                  3.0                  10  ...                   3.0   \n",
       "2                  5.0                   5  ...                   3.0   \n",
       "3                  2.5                  10  ...                   3.0   \n",
       "4                  6.0                  10  ...                   4.0   \n",
       "\n",
       "   sum_cart_position  min_cart_position  max_cart_position  std_cart_position  \\\n",
       "0             133206                  1                 60           4.110813   \n",
       "1               8324                  1                 29           3.567502   \n",
       "2              23160                  1                 37           3.611700   \n",
       "3              30826                  1                 61           4.782450   \n",
       "4              21091                  1                 66           5.345184   \n",
       "\n",
       "   aisle_reorder_ratio  department_reorder_ratio  order_id  eval_set  \\\n",
       "0             0.638832                  0.653460   1187899     train   \n",
       "1             0.519170                  0.574180   1187899     train   \n",
       "2             0.718104                  0.649913   1187899     train   \n",
       "3             0.591986                  0.574180   1187899     train   \n",
       "4             0.571584                  0.560922   1187899     train   \n",
       "\n",
       "   reordered  \n",
       "0        1.0  \n",
       "1        1.0  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        1.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging the information contained in order_products_train csv with data_train\n",
    "data_train = data_train.merge(order_products_train[['product_id','order_id','reordered']],on=['product_id','order_id'],how='left')\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eaf641a-1bb2-4ad4-ad26-23cb534b42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Nan with 0 in data_train\n",
    "data_train['reordered'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04a4561b-57e0-484e-82ff-0a7404f3ad9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>times_brought</th>\n",
       "      <th>uxp_total_reordered</th>\n",
       "      <th>uxp_first_order_num</th>\n",
       "      <th>uxp_avg_cart_pos</th>\n",
       "      <th>uxp_min_cart_pos</th>\n",
       "      <th>uxp_max_cart_pos</th>\n",
       "      <th>uxp_median_cart_pos</th>\n",
       "      <th>uxp_last_order_num</th>\n",
       "      <th>uxp_avg_dow</th>\n",
       "      <th>uxp_median_dow</th>\n",
       "      <th>...</th>\n",
       "      <th>prod_reordered_ratio</th>\n",
       "      <th>avg_cart_position</th>\n",
       "      <th>median_cart_position</th>\n",
       "      <th>sum_cart_position</th>\n",
       "      <th>min_cart_position</th>\n",
       "      <th>max_cart_position</th>\n",
       "      <th>std_cart_position</th>\n",
       "      <th>aisle_reorder_ratio</th>\n",
       "      <th>department_reorder_ratio</th>\n",
       "      <th>reordered</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>196</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776480</td>\n",
       "      <td>3.721774</td>\n",
       "      <td>2.0</td>\n",
       "      <td>133206</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>4.110813</td>\n",
       "      <td>0.638832</td>\n",
       "      <td>0.653460</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713772</td>\n",
       "      <td>4.277492</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8324</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.567502</td>\n",
       "      <td>0.519170</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10326</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652009</td>\n",
       "      <td>4.191097</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23160</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3.611700</td>\n",
       "      <td>0.718104</td>\n",
       "      <td>0.649913</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12427</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740735</td>\n",
       "      <td>4.760037</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30826</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>4.782450</td>\n",
       "      <td>0.591986</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13032</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657158</td>\n",
       "      <td>5.622767</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21091</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>5.345184</td>\n",
       "      <td>0.571584</td>\n",
       "      <td>0.560922</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    times_brought  uxp_total_reordered  uxp_first_order_num  \\\n",
       "user_id product_id                                                            \n",
       "1       196                    10                    9                    1   \n",
       "        10258                   9                    8                    2   \n",
       "        10326                   1                    0                    5   \n",
       "        12427                  10                    9                    1   \n",
       "        13032                   3                    2                    2   \n",
       "\n",
       "                    uxp_avg_cart_pos  uxp_min_cart_pos  uxp_max_cart_pos  \\\n",
       "user_id product_id                                                         \n",
       "1       196                 1.400000                 1                 4   \n",
       "        10258               3.333333                 2                 6   \n",
       "        10326               5.000000                 5                 5   \n",
       "        12427               3.300000                 1                 9   \n",
       "        13032               6.333333                 5                 8   \n",
       "\n",
       "                    uxp_median_cart_pos  uxp_last_order_num  uxp_avg_dow  \\\n",
       "user_id product_id                                                         \n",
       "1       196                         1.0                  10     2.500000   \n",
       "        10258                       3.0                  10     2.555556   \n",
       "        10326                       5.0                   5     4.000000   \n",
       "        12427                       2.5                  10     2.500000   \n",
       "        13032                       6.0                  10     2.666667   \n",
       "\n",
       "                    uxp_median_dow  ...  prod_reordered_ratio  \\\n",
       "user_id product_id                  ...                         \n",
       "1       196                    2.5  ...              0.776480   \n",
       "        10258                  3.0  ...              0.713772   \n",
       "        10326                  4.0  ...              0.652009   \n",
       "        12427                  2.5  ...              0.740735   \n",
       "        13032                  3.0  ...              0.657158   \n",
       "\n",
       "                    avg_cart_position  median_cart_position  \\\n",
       "user_id product_id                                            \n",
       "1       196                  3.721774                   2.0   \n",
       "        10258                4.277492                   3.0   \n",
       "        10326                4.191097                   3.0   \n",
       "        12427                4.760037                   3.0   \n",
       "        13032                5.622767                   4.0   \n",
       "\n",
       "                    sum_cart_position  min_cart_position  max_cart_position  \\\n",
       "user_id product_id                                                            \n",
       "1       196                    133206                  1                 60   \n",
       "        10258                    8324                  1                 29   \n",
       "        10326                   23160                  1                 37   \n",
       "        12427                   30826                  1                 61   \n",
       "        13032                   21091                  1                 66   \n",
       "\n",
       "                    std_cart_position  aisle_reorder_ratio  \\\n",
       "user_id product_id                                           \n",
       "1       196                  4.110813             0.638832   \n",
       "        10258                3.567502             0.519170   \n",
       "        10326                3.611700             0.718104   \n",
       "        12427                4.782450             0.591986   \n",
       "        13032                5.345184             0.571584   \n",
       "\n",
       "                    department_reorder_ratio  reordered  \n",
       "user_id product_id                                       \n",
       "1       196                         0.653460        1.0  \n",
       "        10258                       0.574180        1.0  \n",
       "        10326                       0.649913        0.0  \n",
       "        12427                       0.574180        0.0  \n",
       "        13032                       0.560922        1.0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the index for train data as a combination of user-id and product-id\n",
    "data_train = data_train.set_index(['user_id','product_id'])\n",
    "\n",
    "# Dropping the eval_set and order_id columns\n",
    "data_train.drop(['eval_set','order_id'],axis=1,inplace=True)\n",
    "\n",
    "#display\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bde5d527-7638-4e63-adfb-58631d8f2c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>times_brought</th>\n",
       "      <th>uxp_total_reordered</th>\n",
       "      <th>uxp_first_order_num</th>\n",
       "      <th>uxp_avg_cart_pos</th>\n",
       "      <th>uxp_min_cart_pos</th>\n",
       "      <th>uxp_max_cart_pos</th>\n",
       "      <th>uxp_median_cart_pos</th>\n",
       "      <th>uxp_last_order_num</th>\n",
       "      <th>uxp_avg_dow</th>\n",
       "      <th>uxp_median_dow</th>\n",
       "      <th>...</th>\n",
       "      <th>prod_tot_reorders</th>\n",
       "      <th>prod_reordered_ratio</th>\n",
       "      <th>avg_cart_position</th>\n",
       "      <th>median_cart_position</th>\n",
       "      <th>sum_cart_position</th>\n",
       "      <th>min_cart_position</th>\n",
       "      <th>max_cart_position</th>\n",
       "      <th>std_cart_position</th>\n",
       "      <th>aisle_reorder_ratio</th>\n",
       "      <th>department_reorder_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3</th>\n",
       "      <th>248</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2550</td>\n",
       "      <td>0.400251</td>\n",
       "      <td>10.620782</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67665</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>8.025390</td>\n",
       "      <td>0.519170</td>\n",
       "      <td>0.574180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>0.440605</td>\n",
       "      <td>9.498920</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4398</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>7.984800</td>\n",
       "      <td>0.527615</td>\n",
       "      <td>0.653460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1193</td>\n",
       "      <td>0.492162</td>\n",
       "      <td>9.287541</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22513</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>7.441465</td>\n",
       "      <td>0.487633</td>\n",
       "      <td>0.346721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7503</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6905</td>\n",
       "      <td>0.553551</td>\n",
       "      <td>9.547379</td>\n",
       "      <td>8.0</td>\n",
       "      <td>119094</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>7.622521</td>\n",
       "      <td>0.519170</td>\n",
       "      <td>0.574180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16472</td>\n",
       "      <td>0.591157</td>\n",
       "      <td>8.822854</td>\n",
       "      <td>7.0</td>\n",
       "      <td>245840</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>7.211007</td>\n",
       "      <td>0.528005</td>\n",
       "      <td>0.402178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    times_brought  uxp_total_reordered  uxp_first_order_num  \\\n",
       "user_id product_id                                                            \n",
       "3       248                     1                    0                    2   \n",
       "        1005                    1                    0                   10   \n",
       "        1819                    3                    2                    4   \n",
       "        7503                    1                    0                    3   \n",
       "        8021                    1                    0                    2   \n",
       "\n",
       "                    uxp_avg_cart_pos  uxp_min_cart_pos  uxp_max_cart_pos  \\\n",
       "user_id product_id                                                         \n",
       "3       248                 3.000000                 3                 3   \n",
       "        1005                5.000000                 5                 5   \n",
       "        1819                2.666667                 1                 5   \n",
       "        7503                6.000000                 6                 6   \n",
       "        8021                5.000000                 5                 5   \n",
       "\n",
       "                    uxp_median_cart_pos  uxp_last_order_num  uxp_avg_dow  \\\n",
       "user_id product_id                                                         \n",
       "3       248                         3.0                   2     3.000000   \n",
       "        1005                        5.0                  10     3.000000   \n",
       "        1819                        2.0                   7     0.666667   \n",
       "        7503                        6.0                   3     3.000000   \n",
       "        8021                        5.0                   2     3.000000   \n",
       "\n",
       "                    uxp_median_dow  ...  prod_tot_reorders  \\\n",
       "user_id product_id                  ...                      \n",
       "3       248                    3.0  ...               2550   \n",
       "        1005                   3.0  ...                204   \n",
       "        1819                   0.0  ...               1193   \n",
       "        7503                   3.0  ...               6905   \n",
       "        8021                   3.0  ...              16472   \n",
       "\n",
       "                    prod_reordered_ratio  avg_cart_position  \\\n",
       "user_id product_id                                            \n",
       "3       248                     0.400251          10.620782   \n",
       "        1005                    0.440605           9.498920   \n",
       "        1819                    0.492162           9.287541   \n",
       "        7503                    0.553551           9.547379   \n",
       "        8021                    0.591157           8.822854   \n",
       "\n",
       "                    median_cart_position  sum_cart_position  \\\n",
       "user_id product_id                                            \n",
       "3       248                          9.0              67665   \n",
       "        1005                         8.0               4398   \n",
       "        1819                         7.0              22513   \n",
       "        7503                         8.0             119094   \n",
       "        8021                         7.0             245840   \n",
       "\n",
       "                    min_cart_position  max_cart_position  std_cart_position  \\\n",
       "user_id product_id                                                            \n",
       "3       248                         1                 91           8.025390   \n",
       "        1005                        1                 74           7.984800   \n",
       "        1819                        1                 68           7.441465   \n",
       "        7503                        1                 79           7.622521   \n",
       "        8021                        1                 90           7.211007   \n",
       "\n",
       "                    aisle_reorder_ratio  department_reorder_ratio  \n",
       "user_id product_id                                                 \n",
       "3       248                    0.519170                  0.574180  \n",
       "        1005                   0.527615                  0.653460  \n",
       "        1819                   0.487633                  0.346721  \n",
       "        7503                   0.519170                  0.574180  \n",
       "        8021                   0.528005                  0.402178  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# following same steps as above for test data\n",
    "\n",
    "data_test = data[data['eval_set'] == 'test']\n",
    "data_test.head()\n",
    "\n",
    "# Setting the index for train data as a combination of user-id and product-id\n",
    "data_test = data_test.set_index(['user_id','product_id'])\n",
    "\n",
    "# Dropping the eval_set and order_id columns\n",
    "data_test.drop(['eval_set','order_id'],axis=1,inplace=True)\n",
    "\n",
    "#display\n",
    "data_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9343797c-f240-4c6d-a195-a677b615f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape -  (8474661, 47)\n",
      "Test data shape -  (4833292, 46)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data shape - \",data_train.shape)\n",
    "print(\"Test data shape - \",data_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58cd14ac-d6ef-430a-9f6d-b80619f5ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting unwanted data frames\n",
    "del [data,future_orders,orders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bb4549a-3630-4005-b490-0609ec97a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from downcast import reduce\n",
    "\n",
    "# replace Nan values with mean values\n",
    "data_train.fillna(data_train.mean(), inplace=True)\n",
    "data_train = reduce(data_train)\n",
    "\n",
    "# Creating X and y(target variable)\n",
    "X = data_train.drop(['reordered','min_number_of_orders'], axis=1)\n",
    "y = data_train['reordered']\n",
    "\n",
    "#replace Nan values with mean values\n",
    "#X.fillna(X.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8a55d23-8888-4876-821b-fbef3ad06ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset into train and test split.\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10,stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cad69c54-5fad-46e0-b4af-ef2d383b6489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5932262, 45)\n",
      "(5932262,)\n",
      "(2542399, 45)\n",
      "(2542399,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4118f9b4-35ae-4cc2-bcdb-e0ae6426ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Facebook Recomendation system case study\n",
    "\n",
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    \n",
    "    B =(C/C.sum(axis=0))\n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    labels = [0,1]\n",
    "    # representing A in heatmap format\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # representing B in heatmap format\n",
    "    sns.heatmap(A, annot=True, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9110dbc-58c5-465c-be2a-37ee89b31aed",
   "metadata": {},
   "source": [
    "### Veri Setini %80 oranında küçült"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56d7f288-9897-44e4-9f9a-3e6f9665c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1186452, 45)\n",
      "(1186452,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train ve y_train veri setlerinizi yükleyin\n",
    "# X_train = ...\n",
    "# y_train = ...\n",
    "\n",
    "# Stratified sampling uygulayarak veri setini %20'ye düşürün\n",
    "X_train_strat, X_discard, y_train_strat, y_discard = train_test_split(\n",
    "    X_train, y_train, test_size=0.8, stratify=y_train, random_state=42)\n",
    "\n",
    "# Sonuçların boyutlarını kontrol etme\n",
    "print(X_train_strat.shape)\n",
    "print(y_train_strat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11ee0df0-d626-435e-8ef5-c68fb8441571",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_strat\n",
    "y_train = y_train_strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21caca34-ccbb-483e-9b6a-2c3d30e56bf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reordered\n",
       "0.0    1070417\n",
       "1.0     116035\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa5fca-e2e2-4d2e-abfc-78cc720b4708",
   "metadata": {},
   "source": [
    "# Light Gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40f1d2e3-88d1-4063-9de1-cc9b67bd6195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[LightGBM] [Info] Number of positive: 116035, number of negative: 1070417\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6189\n",
      "[LightGBM] [Info] Number of data points in the train set: 1186452, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=LGBMClassifier(objective=&#x27;binary&#x27;), n_jobs=-1,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.1, 0.05, 0.01,\n",
       "                                                          0.005],\n",
       "                                        &#x27;max_depth&#x27;: [3, 4, 5, 6, 8, 10, 12],\n",
       "                                        &#x27;num_leaves&#x27;: [8, 12, 16, 20, 24, 28,\n",
       "                                                       32, 36, 40, 44, 48, 52,\n",
       "                                                       56, 60, 64, 68, 72, 76,\n",
       "                                                       80, 84, 88]},\n",
       "                   verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(estimator=LGBMClassifier(objective=&#x27;binary&#x27;), n_jobs=-1,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.1, 0.05, 0.01,\n",
       "                                                          0.005],\n",
       "                                        &#x27;max_depth&#x27;: [3, 4, 5, 6, 8, 10, 12],\n",
       "                                        &#x27;num_leaves&#x27;: [8, 12, 16, 20, 24, 28,\n",
       "                                                       32, 36, 40, 44, 48, 52,\n",
       "                                                       56, 60, 64, 68, 72, 76,\n",
       "                                                       80, 84, 88]},\n",
       "                   verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(objective=&#x27;binary&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(estimator=LGBMClassifier(objective='binary'), n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.1, 0.05, 0.01,\n",
       "                                                          0.005],\n",
       "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12],\n",
       "                                        'num_leaves': [8, 12, 16, 20, 24, 28,\n",
       "                                                       32, 36, 40, 44, 48, 52,\n",
       "                                                       56, 60, 64, 68, 72, 76,\n",
       "                                                       80, 84, 88]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/10] START learning_rate=0.005, max_depth=3, num_leaves=24............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 1/10] END learning_rate=0.005, max_depth=3, num_leaves=24;, score=0.902 total time=  13.1s\n",
      "[CV 4/5; 2/10] START learning_rate=0.01, max_depth=5, num_leaves=60.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 2/10] END learning_rate=0.01, max_depth=5, num_leaves=60;, score=0.904 total time=  16.7s\n",
      "[CV 2/5; 4/10] START learning_rate=0.005, max_depth=3, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 4/10] END learning_rate=0.005, max_depth=3, num_leaves=48;, score=0.902 total time=  11.0s\n",
      "[CV 2/5; 5/10] START learning_rate=0.01, max_depth=8, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.414198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 5/10] END learning_rate=0.01, max_depth=8, num_leaves=84;, score=0.904 total time=  47.4s\n",
      "[CV 1/5; 8/10] START learning_rate=0.1, max_depth=6, num_leaves=12..............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 8/10] END learning_rate=0.1, max_depth=6, num_leaves=12;, score=0.909 total time=  13.9s\n",
      "[CV 4/5; 8/10] START learning_rate=0.1, max_depth=6, num_leaves=12..............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 8/10] END learning_rate=0.1, max_depth=6, num_leaves=12;, score=0.909 total time=  20.3s\n",
      "[CV 2/5; 10/10] START learning_rate=0.1, max_depth=3, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 10/10] END learning_rate=0.1, max_depth=3, num_leaves=84;, score=0.909 total time=  11.1s\n",
      "[CV 1/5; 1/10] START learning_rate=0.005, max_depth=3, num_leaves=24............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 1/10] END learning_rate=0.005, max_depth=3, num_leaves=24;, score=0.902 total time=  16.3s\n",
      "[CV 1/5; 3/10] START learning_rate=0.01, max_depth=10, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.288156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 3/10] END learning_rate=0.01, max_depth=10, num_leaves=48;, score=0.904 total time=  23.6s\n",
      "[CV 5/5; 4/10] START learning_rate=0.005, max_depth=3, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 4/10] END learning_rate=0.005, max_depth=3, num_leaves=48;, score=0.902 total time=  10.8s\n",
      "[CV 5/5; 5/10] START learning_rate=0.01, max_depth=8, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 5/10] END learning_rate=0.01, max_depth=8, num_leaves=84;, score=0.904 total time=  28.8s\n",
      "[CV 2/5; 7/10] START learning_rate=0.05, max_depth=10, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 7/10] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.909 total time=  25.6s\n",
      "[CV 1/5; 9/10] START learning_rate=0.05, max_depth=6, num_leaves=32.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 9/10] END learning_rate=0.05, max_depth=6, num_leaves=32;, score=0.909 total time=  28.5s\n",
      "[CV 2/5; 2/10] START learning_rate=0.01, max_depth=5, num_leaves=60.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 2/10] END learning_rate=0.01, max_depth=5, num_leaves=60;, score=0.904 total time=  20.0s\n",
      "[CV 5/5; 3/10] START learning_rate=0.01, max_depth=10, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 3/10] END learning_rate=0.01, max_depth=10, num_leaves=48;, score=0.904 total time=  23.7s\n",
      "[CV 3/5; 5/10] START learning_rate=0.01, max_depth=8, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 5/10] END learning_rate=0.01, max_depth=8, num_leaves=84;, score=0.904 total time=  45.5s\n",
      "[CV 2/5; 8/10] START learning_rate=0.1, max_depth=6, num_leaves=12..............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 8/10] END learning_rate=0.1, max_depth=6, num_leaves=12;, score=0.909 total time=  19.1s\n",
      "[CV 2/5; 9/10] START learning_rate=0.05, max_depth=6, num_leaves=32.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 9/10] END learning_rate=0.05, max_depth=6, num_leaves=32;, score=0.909 total time=  27.6s\n",
      "[CV 2/5; 1/10] START learning_rate=0.005, max_depth=3, num_leaves=24............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 1/10] END learning_rate=0.005, max_depth=3, num_leaves=24;, score=0.902 total time=  13.1s\n",
      "[CV 5/5; 2/10] START learning_rate=0.01, max_depth=5, num_leaves=60.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 2/10] END learning_rate=0.01, max_depth=5, num_leaves=60;, score=0.904 total time=  24.1s\n",
      "[CV 3/5; 4/10] START learning_rate=0.005, max_depth=3, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 4/10] END learning_rate=0.005, max_depth=3, num_leaves=48;, score=0.902 total time=  11.4s\n",
      "[CV 4/5; 5/10] START learning_rate=0.01, max_depth=8, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 5/10] END learning_rate=0.01, max_depth=8, num_leaves=84;, score=0.904 total time=  29.1s\n",
      "[CV 1/5; 7/10] START learning_rate=0.05, max_depth=10, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 7/10] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.909 total time=  25.5s\n",
      "[CV 5/5; 8/10] START learning_rate=0.1, max_depth=6, num_leaves=12..............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 8/10] END learning_rate=0.1, max_depth=6, num_leaves=12;, score=0.909 total time=  20.6s\n",
      "[CV 3/5; 10/10] START learning_rate=0.1, max_depth=3, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.203134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 10/10] END learning_rate=0.1, max_depth=3, num_leaves=84;, score=0.909 total time=  13.3s\n",
      "[CV 4/5; 1/10] START learning_rate=0.005, max_depth=3, num_leaves=24............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 1/10] END learning_rate=0.005, max_depth=3, num_leaves=24;, score=0.902 total time=  16.8s\n",
      "[CV 3/5; 3/10] START learning_rate=0.01, max_depth=10, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 3/10] END learning_rate=0.01, max_depth=10, num_leaves=48;, score=0.903 total time=  23.5s\n",
      "[CV 1/5; 5/10] START learning_rate=0.01, max_depth=8, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 5/10] END learning_rate=0.01, max_depth=8, num_leaves=84;, score=0.904 total time=  47.1s\n",
      "[CV 4/5; 7/10] START learning_rate=0.05, max_depth=10, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 7/10] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.910 total time=  25.4s\n",
      "[CV 4/5; 9/10] START learning_rate=0.05, max_depth=6, num_leaves=32.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 9/10] END learning_rate=0.05, max_depth=6, num_leaves=32;, score=0.909 total time=  26.1s\n",
      "[CV 5/5; 1/10] START learning_rate=0.005, max_depth=3, num_leaves=24............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 1/10] END learning_rate=0.005, max_depth=3, num_leaves=24;, score=0.902 total time=  16.7s\n",
      "[CV 2/5; 3/10] START learning_rate=0.01, max_depth=10, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 3/10] END learning_rate=0.01, max_depth=10, num_leaves=48;, score=0.904 total time=  37.2s\n",
      "[CV 2/5; 6/10] START learning_rate=0.05, max_depth=5, num_leaves=88.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 2/5; 6/10] END learning_rate=0.05, max_depth=5, num_leaves=88;, score=0.909 total time=  17.2s\n",
      "[CV 5/5; 6/10] START learning_rate=0.05, max_depth=5, num_leaves=88.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 6/10] END learning_rate=0.05, max_depth=5, num_leaves=88;, score=0.909 total time=  26.4s\n",
      "[CV 3/5; 8/10] START learning_rate=0.1, max_depth=6, num_leaves=12..............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 8/10] END learning_rate=0.1, max_depth=6, num_leaves=12;, score=0.910 total time=  14.6s\n",
      "[CV 3/5; 9/10] START learning_rate=0.05, max_depth=6, num_leaves=32.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 9/10] END learning_rate=0.05, max_depth=6, num_leaves=32;, score=0.910 total time=  19.6s\n",
      "[CV 5/5; 10/10] START learning_rate=0.1, max_depth=3, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 10/10] END learning_rate=0.1, max_depth=3, num_leaves=84;, score=0.909 total time=   7.6s\n",
      "[CV 1/5; 2/10] START learning_rate=0.01, max_depth=5, num_leaves=60.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 2/10] END learning_rate=0.01, max_depth=5, num_leaves=60;, score=0.904 total time=  26.6s\n",
      "[CV 1/5; 4/10] START learning_rate=0.005, max_depth=3, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 4/10] END learning_rate=0.005, max_depth=3, num_leaves=48;, score=0.902 total time=  11.1s\n",
      "[CV 4/5; 4/10] START learning_rate=0.005, max_depth=3, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 4/10] END learning_rate=0.005, max_depth=3, num_leaves=48;, score=0.902 total time=  15.2s\n",
      "[CV 1/5; 6/10] START learning_rate=0.05, max_depth=5, num_leaves=88.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 6/10] END learning_rate=0.05, max_depth=5, num_leaves=88;, score=0.909 total time=  17.1s\n",
      "[CV 4/5; 6/10] START learning_rate=0.05, max_depth=5, num_leaves=88.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5; 6/10] END learning_rate=0.05, max_depth=5, num_leaves=88;, score=0.909 total time=  18.0s\n",
      "[CV 5/5; 7/10] START learning_rate=0.05, max_depth=10, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 7/10] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.909 total time=  26.0s\n",
      "[CV 5/5; 9/10] START learning_rate=0.05, max_depth=6, num_leaves=32.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 9/10] END learning_rate=0.05, max_depth=6, num_leaves=32;, score=0.909 total time=  25.3s\n",
      "[CV 3/5; 2/10] START learning_rate=0.01, max_depth=5, num_leaves=60.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 2/10] END learning_rate=0.01, max_depth=5, num_leaves=60;, score=0.904 total time=  19.8s\n",
      "[CV 4/5; 3/10] START learning_rate=0.01, max_depth=10, num_leaves=48............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.281835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 3/10] END learning_rate=0.01, max_depth=10, num_leaves=48;, score=0.904 total time=  36.5s\n",
      "[CV 3/5; 6/10] START learning_rate=0.05, max_depth=5, num_leaves=88.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 6/10] END learning_rate=0.05, max_depth=5, num_leaves=88;, score=0.910 total time=  25.3s\n",
      "[CV 3/5; 7/10] START learning_rate=0.05, max_depth=10, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 7/10] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.909 total time=  38.8s\n",
      "[CV 1/5; 10/10] START learning_rate=0.1, max_depth=3, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 10/10] END learning_rate=0.1, max_depth=3, num_leaves=84;, score=0.909 total time=  10.6s\n",
      "[CV 4/5; 10/10] START learning_rate=0.1, max_depth=3, num_leaves=84.............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 10/10] END learning_rate=0.1, max_depth=3, num_leaves=84;, score=0.909 total time=   8.9s\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {\n",
    "'num_leaves': list(range(8, 92, 4)),\n",
    "'max_depth': [3, 4, 5, 6, 8, 10,12],\n",
    "'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "}\n",
    "\n",
    "lgb_model=lgb.LGBMClassifier(boosting_type='gbdt',objective='binary')\n",
    "random_cfl=RandomizedSearchCV(lgb_model,param_distributions=param_grid,verbose=10,n_jobs=-1)\n",
    "random_cfl.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c37eb4d8-3198-4ade-b692-30513fa6f59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV 1/5; 1/100] START bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6733\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV 1/5; 1/100] END bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31;, score=0.904 total time=  29.6s\n",
      "[CV 2/5; 3/100] START bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6588\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[CV 2/5; 3/100] END bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62;, score=0.909 total time=  20.0s\n",
      "[CV 5/5; 4/100] START bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5272\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[CV 5/5; 4/100] END bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21;, score=0.904 total time=  21.0s\n",
      "[CV 3/5; 6/100] START bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5292\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 3/5; 6/100] END bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51;, score=0.909 total time=  34.8s\n",
      "[CV 1/5; 8/100] START bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485[CV 3/5; 2/100] START bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5578\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[CV 3/5; 2/100] END bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29;, score=0.908 total time=  16.8s\n",
      "[CV 5/5; 2/100] START bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5574\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[CV 5/5; 2/100] END bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29;, score=0.907 total time=  14.5s\n",
      "[CV 5/5; 3/100] START bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6587\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[CV 5/5; 3/100] END bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62;, score=0.909 total time=  20.9s\n",
      "[CV 3/5; 5/100] START bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5973\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[CV 3/5; 5/100] END bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42;, score=0.910 total time=  28.7s\n",
      "[CV 2/5; 7/100] START bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[CV 5/5; 1/100] START bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6721\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV 5/5; 1/100] END bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31;, score=0.904 total time=  29.9s\n",
      "[CV 4/5; 3/100] START bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6594\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[CV 4/5; 3/100] END bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62;, score=0.909 total time=  18.6s\n",
      "[CV 2/5; 5/100] START bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5977\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[CV 2/5; 5/100] END bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42;, score=0.909 total time=  31.0s\n",
      "[CV 1/5; 7/100] START bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6296\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[CV 1/5; 7/100] END bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60;, score=0.908 total time=  43.6s\n",
      "[CV 4/5; 8/100] START bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[CV 2/5; 1/100] START bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6724\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV 2/5; 1/100] END bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31;, score=0.904 total time=  29.7s\n",
      "[CV 3/5; 3/100] START bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6589\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[CV 3/5; 3/100] END bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62;, score=0.909 total time=  19.9s\n",
      "[CV 1/5; 5/100] START bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5982\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[CV 1/5; 5/100] END bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42;, score=0.909 total time=  26.7s\n",
      "[CV 4/5; 6/100] START bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5292\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 4/5; 6/100] END bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51;, score=0.909 total time=  34.7s\n",
      "[CV 2/5; 8/100] START bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[CV 1/5; 2/100] START bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5583\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[CV 1/5; 2/100] END bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29;, score=0.907 total time=  16.4s\n",
      "[CV 4/5; 2/100] START bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.468297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5578\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[CV 4/5; 2/100] END bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29;, score=0.908 total time=  16.0s\n",
      "[CV 3/5; 4/100] START bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5292\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[CV 3/5; 4/100] END bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21;, score=0.904 total time=  24.6s\n",
      "[CV 5/5; 5/100] START bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5973\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[CV 5/5; 5/100] END bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42;, score=0.909 total time=  26.3s\n",
      "[CV 3/5; 7/100] START bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6284\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[CV 2/5; 2/100] START bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5575\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8324426408004217, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8324426408004217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9849549260809971, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9849549260809971\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8254442364744264, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254442364744264\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.21233911067827616, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.21233911067827616\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=77\n",
      "[CV 2/5; 2/100] END bagging_fraction=0.8254442364744264, bagging_freq=5, feature_fraction=0.9849549260809971, lambda_l1=0.8324426408004217, lambda_l2=0.21233911067827616, learning_rate=0.02227337188467456, max_bin=220, max_depth=3, min_data_in_leaf=77, num_leaves=29;, score=0.907 total time=  16.5s\n",
      "[CV 1/5; 3/100] START bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6598\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6118528947223795, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6118528947223795\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.645614570099021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.645614570099021\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5035331526098588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5035331526098588\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13949386065204183, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13949386065204183\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[CV 1/5; 3/100] END bagging_fraction=0.5035331526098588, bagging_freq=9, feature_fraction=0.645614570099021, lambda_l1=0.6118528947223795, lambda_l2=0.13949386065204183, learning_rate=0.032753741610845724, max_bin=279, max_depth=5, min_data_in_leaf=127, num_leaves=62;, score=0.909 total time=  19.1s\n",
      "[CV 4/5; 4/100] START bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5292\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[CV 4/5; 4/100] END bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21;, score=0.904 total time=  22.6s\n",
      "[CV 2/5; 6/100] START bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5279\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 2/5; 6/100] END bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51;, score=0.909 total time=  37.0s\n",
      "[CV 5/5; 7/100] START bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[CV 4/5; 1/100] START bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.197970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6734\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV 4/5; 1/100] END bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31;, score=0.904 total time=  31.1s\n",
      "[CV 1/5; 4/100] START bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5283\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[CV 1/5; 4/100] END bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21;, score=0.904 total time=  23.0s\n",
      "[CV 4/5; 5/100] START bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5977\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.230893825622149, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.230893825622149\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5079831261101071, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5079831261101071\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9041986740582306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9041986740582306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.24102546602601171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24102546602601171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=194, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=194\n",
      "[CV 4/5; 5/100] END bagging_fraction=0.9041986740582306, bagging_freq=9, feature_fraction=0.5079831261101071, lambda_l1=0.230893825622149, lambda_l2=0.24102546602601171, learning_rate=0.06991003428841853, max_bin=243, max_depth=10, min_data_in_leaf=194, num_leaves=42;, score=0.909 total time=  26.5s\n",
      "[CV 5/5; 6/100] START bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5272\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 5/5; 6/100] END bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51;, score=0.909 total time=  36.4s\n",
      "[CV 3/5; 8/100] START bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[CV 3/5; 1/100] START bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6720\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.15601864044243652, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15601864044243652\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7993292420985183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7993292420985183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6872700594236812, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6872700594236812\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.15599452033620265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.15599452033620265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV 3/5; 1/100] END bagging_fraction=0.6872700594236812, bagging_freq=8, feature_fraction=0.7993292420985183, lambda_l1=0.15601864044243652, lambda_l2=0.15599452033620265, learning_rate=0.010517943155978949, max_bin=287, max_depth=7, min_data_in_leaf=119, num_leaves=31;, score=0.904 total time=  31.1s\n",
      "[CV 2/5; 4/100] START bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5279\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6075448519014384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6075448519014384\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5232252063599989, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5232252063599989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9916154429033941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9916154429033941\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.17052412368729153, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17052412368729153\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n",
      "[CV 2/5; 4/100] END bagging_fraction=0.9916154429033941, bagging_freq=9, feature_fraction=0.5232252063599989, lambda_l1=0.6075448519014384, lambda_l2=0.17052412368729153, learning_rate=0.011179901333601555, max_bin=203, max_depth=11, min_data_in_leaf=79, num_leaves=21;, score=0.904 total time=  26.3s\n",
      "[CV 1/5; 6/100] START bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5283\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7553614103176525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7553614103176525\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5911180438940311, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5911180438940311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.954660201039391, subsample=1.0 will be ignored. Current value: bagging_fraction=0.954660201039391\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4251558744912447, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4251558744912447\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 1/5; 6/100] END bagging_fraction=0.954660201039391, bagging_freq=4, feature_fraction=0.5911180438940311, lambda_l1=0.7553614103176525, lambda_l2=0.4251558744912447, learning_rate=0.02475445797247794, max_bin=203, max_depth=8, min_data_in_leaf=165, num_leaves=51;, score=0.908 total time=  36.9s\n",
      "[CV 4/5; 7/100] START bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6289\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[CV 4/5; 7/100] END bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60;, score=0.909 total time=  48.4s\n",
      "[CV 1/5; 10/100] START bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6339\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[CV 2/5; 8/100] END bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16;, score=0.909 total time=  21.8s\n",
      "[CV 3/5; 9/100] START bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5786\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[CV 3/5; 9/100] END bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35;, score=0.910 total time=  21.8s\n",
      "[CV 5/5; 10/100] START bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5848\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[CV 5/5; 10/100] END bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42;, score=0.909 total time=  22.5s\n",
      "[CV 3/5; 12/100] START bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6826\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 3/5; 12/100] END bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22;, score=0.909 total time=  24.1s\n",
      "[CV 1/5; 14/100] START bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6290\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[CV 2/5; 7/100] END bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60;, score=0.909 total time=  43.6s\n",
      "[CV 5/5; 8/100] START bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6331\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[CV 5/5; 8/100] END bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16;, score=0.909 total time=  20.7s\n",
      "[CV 2/5; 10/100] START bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5858\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[CV 2/5; 10/100] END bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42;, score=0.909 total time=  23.6s\n",
      "[CV 1/5; 12/100] START bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6827\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 1/5; 12/100] END bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22;, score=0.909 total time=  23.9s\n",
      "\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6348\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[CV 1/5; 8/100] END bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16;, score=0.909 total time=  21.1s\n",
      "[CV 1/5; 9/100] START bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5787\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[CV 1/5; 9/100] END bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35;, score=0.909 total time=  22.3s\n",
      "[CV 4/5; 10/100] START bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5856\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[CV 4/5; 10/100] END bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42;, score=0.909 total time=  22.2s\n",
      "[CV 2/5; 12/100] START bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6824\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 2/5; 12/100] END bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22;, score=0.909 total time=  23.9s\n",
      "[CV 5/5; 13/100] START bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6282\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[CV 5/5; 7/100] END bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60;, score=0.909 total time=  43.6s\n",
      "[CV 5/5; 9/100] START bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.205049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5780\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[CV 5/5; 9/100] END bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35;, score=0.909 total time=  23.3s\n",
      "[CV 2/5; 11/100] START bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5460\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[CV 2/5; 11/100] END bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16;, score=0.908 total time=  19.6s\n",
      "[CV 5/5; 12/100] START bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6818\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 5/5; 12/100] END bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22;, score=0.909 total time=  23.2s\n",
      "[CV 3/5; 14/100] START bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6284\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 3/5; 14/100] END bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34;, score=0.909 total time=  16.1s\n",
      "[CV 4/5; 15/100] START bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.142434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6339\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[CV 4/5; 8/100] END bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16;, score=0.909 total time=  20.8s\n",
      "[CV 3/5; 10/100] START bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5855\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[CV 3/5; 10/100] END bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42;, score=0.909 total time=  21.1s\n",
      "[CV 5/5; 11/100] START bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5457\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[CV 5/5; 11/100] END bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16;, score=0.908 total time=  19.2s\n",
      "[CV 2/5; 13/100] START bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5943\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 2/5; 13/100] END bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35;, score=0.909 total time=  35.9s\n",
      "[CV 2/5; 15/100] START bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6334\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5426960831582485, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5426960831582485\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6404672548436904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6404672548436904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7933755828319241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7933755828319241\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.14092422497476265, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.14092422497476265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "[CV 3/5; 8/100] END bagging_fraction=0.7933755828319241, bagging_freq=9, feature_fraction=0.6404672548436904, lambda_l1=0.5426960831582485, lambda_l2=0.14092422497476265, learning_rate=0.08120871317163378, max_bin=264, max_depth=11, min_data_in_leaf=90, num_leaves=16;, score=0.909 total time=  22.3s\n",
      "[CV 4/5; 9/100] START bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5785\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[CV 4/5; 9/100] END bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35;, score=0.909 total time=  23.7s\n",
      "[CV 3/5; 11/100] START bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5463\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[CV 3/5; 11/100] END bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16;, score=0.909 total time=  19.6s\n",
      "[CV 1/5; 13/100] START bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5950\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 1/5; 13/100] END bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35;, score=0.909 total time=  35.4s\n",
      "[CV 5/5; 14/100] START bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6282\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 5/5; 14/100] END bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34;, score=0.909 total time=  15.1s\n",
      "[CV 3/5; 16/100] START bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9218742350231168, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9218742350231168\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7989499894055425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7989499894055425\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9697494707820946, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9697494707820946\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0884925020519195, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0884925020519195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "[CV 3/5; 7/100] END bagging_fraction=0.9697494707820946, bagging_freq=4, feature_fraction=0.7989499894055425, lambda_l1=0.9218742350231168, lambda_l2=0.0884925020519195, learning_rate=0.023618371929818794, max_bin=261, max_depth=10, min_data_in_leaf=101, num_leaves=60;, score=0.909 total time=  47.0s\n",
      "[CV 2/5; 9/100] START bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5786\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8154614284548342, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8154614284548342\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027610585618012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027610585618012\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861223846483287, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861223846483287\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7068573438476171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7068573438476171\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[CV 2/5; 9/100] END bagging_fraction=0.8861223846483287, bagging_freq=8, feature_fraction=0.5027610585618012, lambda_l1=0.8154614284548342, lambda_l2=0.7068573438476171, learning_rate=0.0742556809638938, max_bin=232, max_depth=7, min_data_in_leaf=60, num_leaves=35;, score=0.909 total time=  25.3s\n",
      "[CV 1/5; 11/100] START bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5464\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[CV 1/5; 11/100] END bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16;, score=0.908 total time=  23.4s\n",
      "[CV 4/5; 12/100] START bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6833\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6364104112637804, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6364104112637804\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5157145928433671, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5157145928433671\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8553314448428937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8553314448428937\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3143559810763267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3143559810763267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 4/5; 12/100] END bagging_fraction=0.8553314448428937, bagging_freq=3, feature_fraction=0.5157145928433671, lambda_l1=0.6364104112637804, lambda_l2=0.3143559810763267, learning_rate=0.053314215660646765, max_bin=293, max_depth=7, min_data_in_leaf=170, num_leaves=22;, score=0.909 total time=  23.8s\n",
      "[CV 2/5; 14/100] START bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.142464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6290\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 2/5; 14/100] END bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34;, score=0.909 total time=  15.6s\n",
      "[CV 1/5; 15/100] START bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5857\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09541011649041131, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09541011649041131\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7247253370691017, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7247253370691017\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9315517129377968, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9315517129377968\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37081825219826636, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37081825219826636\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=191, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=191\n",
      "[CV 1/5; 10/100] END bagging_fraction=0.9315517129377968, bagging_freq=8, feature_fraction=0.7247253370691017, lambda_l1=0.09541011649041131, lambda_l2=0.37081825219826636, learning_rate=0.0685399190030427, max_bin=236, max_depth=5, min_data_in_leaf=191, num_leaves=42;, score=0.909 total time=  23.2s\n",
      "[CV 4/5; 11/100] START bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5462\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8489138242660839, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8489138242660839\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9858560476945519, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9858560476945519\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7361074625809747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7361074625809747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7217295211648732, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7217295211648732\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[CV 4/5; 11/100] END bagging_fraction=0.7361074625809747, bagging_freq=5, feature_fraction=0.9858560476945519, lambda_l1=0.8489138242660839, lambda_l2=0.7217295211648732, learning_rate=0.027418567376150796, max_bin=213, max_depth=9, min_data_in_leaf=46, num_leaves=16;, score=0.909 total time=  22.2s\n",
      "[CV 3/5; 13/100] START bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5939\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 3/5; 13/100] END bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35;, score=0.910 total time=  33.9s\n",
      "[CV 5/5; 15/100] START bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6282\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[CV 5/5; 15/100] END bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51;, score=0.909 total time=  30.0s\n",
      "[CV 3/5; 17/100] START bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6296\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 1/5; 14/100] END bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34;, score=0.909 total time=  15.7s\n",
      "[CV 4/5; 14/100] START bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6289\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5393422419156507, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5393422419156507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9462794992449889, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9462794992449889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9018360384495572, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9018360384495572\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8074401551640625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8074401551640625\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 4/5; 14/100] END bagging_fraction=0.9018360384495572, bagging_freq=4, feature_fraction=0.9462794992449889, lambda_l1=0.5393422419156507, lambda_l2=0.8074401551640625, learning_rate=0.09012867349273186, max_bin=261, max_depth=3, min_data_in_leaf=148, num_leaves=34;, score=0.909 total time=  14.9s\n",
      "[CV 2/5; 16/100] START bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6220\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 2/5; 16/100] END bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46;, score=0.905 total time=  23.8s\n",
      "[CV 5/5; 17/100] START bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6783\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[CV 5/5; 17/100] END bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8;, score=0.909 total time=  12.5s\n",
      "[CV 1/5; 19/100] START bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5499\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[CV 4/5; 13/100] START bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5942\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 4/5; 13/100] END bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35;, score=0.910 total time=  29.1s\n",
      "[CV 3/5; 15/100] START bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6284\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[CV 3/5; 15/100] END bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51;, score=0.910 total time=  29.3s\n",
      "[CV 1/5; 17/100] START bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6795\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[CV 1/5; 17/100] END bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8;, score=0.909 total time=  15.8s\n",
      "[CV 4/5; 18/100] START bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6172\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 4/5; 18/100] END bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24;, score=0.909 total time=  23.9s\n",
      "[CV 5/5; 20/100] START bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6290\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[CV 2/5; 15/100] END bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51;, score=0.909 total time=  29.7s\n",
      "[CV 5/5; 16/100] START bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.202578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6214\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 5/5; 16/100] END bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46;, score=0.905 total time=  25.4s\n",
      "[CV 3/5; 19/100] START bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5497\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[CV 3/5; 19/100] END bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66;, score=0.909 total time=  14.1s\n",
      "[CV 3/5; 20/100] START bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5546\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6289\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[CV 4/5; 15/100] END bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51;, score=0.909 total time=  29.2s\n",
      "[CV 2/5; 17/100] START bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6791\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[CV 2/5; 17/100] END bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8;, score=0.909 total time=  13.6s\n",
      "[CV 2/5; 18/100] START bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6169\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 2/5; 18/100] END bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24;, score=0.909 total time=  22.5s\n",
      "[CV 1/5; 20/100] START bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5550\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6296\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.30478125815802903, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.30478125815802903\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6762844281670846, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6762844281670846\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7135538943131281, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7135538943131281\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16465585314294173, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16465585314294173\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "[CV 1/5; 15/100] END bagging_fraction=0.7135538943131281, bagging_freq=4, feature_fraction=0.6762844281670846, lambda_l1=0.30478125815802903, lambda_l2=0.16465585314294173, learning_rate=0.05573849484066699, max_bin=261, max_depth=7, min_data_in_leaf=70, num_leaves=51;, score=0.909 total time=  30.8s\n",
      "[CV 4/5; 16/100] START bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6222\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 4/5; 16/100] END bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46;, score=0.905 total time=  26.0s\n",
      "[CV 2/5; 19/100] START bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5494\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[CV 2/5; 19/100] END bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66;, score=0.909 total time=  14.9s\n",
      "[CV 4/5; 20/100] START bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5545\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.293387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5940\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.598865466488536, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.598865466488536\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9714267852789905, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9714267852789905\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8777755692715243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8777755692715243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6947849330397046, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6947849330397046\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 5/5; 13/100] END bagging_fraction=0.8777755692715243, bagging_freq=4, feature_fraction=0.9714267852789905, lambda_l1=0.598865466488536, lambda_l2=0.6947849330397046, learning_rate=0.08864444470644949, max_bin=241, max_depth=11, min_data_in_leaf=153, num_leaves=35;, score=0.909 total time=  32.6s\n",
      "[CV 1/5; 16/100] START bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6228\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 1/5; 16/100] END bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46;, score=0.905 total time=  24.1s\n",
      "[CV 4/5; 17/100] START bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6802\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[CV 4/5; 17/100] END bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8;, score=0.909 total time=  15.0s\n",
      "[CV 5/5; 18/100] START bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6158\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 5/5; 18/100] END bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24;, score=0.909 total time=  23.9s\n",
      "[CV 1/5; 21/100] START bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6210\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6788\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9977404850489419, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9977404850489419\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5740434649766999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5740434649766999\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7486242529461927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7486242529461927\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.266781014275285, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.266781014275285\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[CV 3/5; 17/100] END bagging_fraction=0.7486242529461927, bagging_freq=1, feature_fraction=0.5740434649766999, lambda_l1=0.9977404850489419, lambda_l2=0.266781014275285, learning_rate=0.09777842080410203, max_bin=291, max_depth=8, min_data_in_leaf=106, num_leaves=8;, score=0.909 total time=  13.7s\n",
      "[CV 3/5; 18/100] START bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6167\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 3/5; 18/100] END bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24;, score=0.909 total time=  22.8s\n",
      "[CV 2/5; 20/100] START bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5542\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 2/5; 20/100] END bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78;, score=0.909 total time=  22.9s\n",
      "[CV 5/5; 21/100] START bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6217\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5581020020173412, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5581020020173412\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6093821097865351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6093821097865351\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6220627611238871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6220627611238871\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4038361710580408, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4038361710580408\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 3/5; 16/100] END bagging_fraction=0.6220627611238871, bagging_freq=6, feature_fraction=0.6093821097865351, lambda_l1=0.5581020020173412, lambda_l2=0.4038361710580408, learning_rate=0.011164763475353248, max_bin=257, max_depth=6, min_data_in_leaf=31, num_leaves=46;, score=0.904 total time=  22.8s\n",
      "[CV 1/5; 18/100] START bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6177\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44778316457309164, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44778316457309164\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7654672916585682, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7654672916585682\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6197809453334862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6197809453334862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.552893089071328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.552893089071328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 1/5; 18/100] END bagging_fraction=0.6197809453334862, bagging_freq=2, feature_fraction=0.7654672916585682, lambda_l1=0.44778316457309164, lambda_l2=0.552893089071328, learning_rate=0.06130618876854239, max_bin=254, max_depth=10, min_data_in_leaf=142, num_leaves=24;, score=0.909 total time=  23.3s\n",
      "[CV 4/5; 19/100] START bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5496\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[CV 4/5; 19/100] END bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66;, score=0.909 total time=  13.0s\n",
      "[CV 2/5; 21/100] START bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6203\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[CV 2/5; 21/100] END bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30;, score=0.909 total time=  21.5s\n",
      "[CV 3/5; 22/100] START bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5243\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[CV 1/5; 19/100] END bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66;, score=0.908 total time=  13.8s\n",
      "[CV 5/5; 19/100] START bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5491\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.983423140894843, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.983423140894843\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7351503172230192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7351503172230192\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8641081743059298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641081743059298\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.39882444244455306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.39882444244455306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=143\n",
      "[CV 5/5; 19/100] END bagging_fraction=0.8641081743059298, bagging_freq=8, feature_fraction=0.7351503172230192, lambda_l1=0.983423140894843, lambda_l2=0.39882444244455306, learning_rate=0.08256102795584148, max_bin=215, max_depth=3, min_data_in_leaf=143, num_leaves=66;, score=0.909 total time=  13.0s\n",
      "[CV 3/5; 21/100] START bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6201\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[CV 3/5; 21/100] END bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30;, score=0.910 total time=  19.9s\n",
      "[CV 1/5; 23/100] START bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5636\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 1/5; 23/100] END bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40;, score=0.909 total time=  24.4s\n",
      "[CV 4/5; 24/100] START bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6751\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[CV 4/5; 24/100] END bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25;, score=0.902 total time=  18.4s\n",
      "[CV 2/5; 26/100] START bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Info] Total Bins 5542\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 5/5; 20/100] END bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78;, score=0.909 total time=  23.7s\n",
      "[CV 4/5; 22/100] START bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5259\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[CV 4/5; 22/100] END bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19;, score=0.909 total time=  14.1s\n",
      "[CV 1/5; 24/100] START bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6750\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[CV 1/5; 24/100] END bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25;, score=0.902 total time=  19.8s\n",
      "[CV 1/5; 25/100] START bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6396\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 1/5; 20/100] END bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78;, score=0.909 total time=  24.0s\n",
      "[CV 4/5; 21/100] START bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6205\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[CV 4/5; 21/100] END bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30;, score=0.910 total time=  19.3s\n",
      "[CV 3/5; 24/100] START bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6738\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[CV 3/5; 24/100] END bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25;, score=0.902 total time=  17.6s\n",
      "[CV 5/5; 24/100] START bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6738\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[CV 5/5; 24/100] END bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25;, score=0.902 total time=  18.0s\n",
      "[CV 2/5; 27/100] START bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5575\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 4/5; 20/100] END bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78;, score=0.909 total time=  23.0s\n",
      "[CV 2/5; 22/100] START bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5246\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[CV 2/5; 22/100] END bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19;, score=0.908 total time=  13.7s\n",
      "[CV 4/5; 23/100] START bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5630\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 4/5; 23/100] END bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40;, score=0.909 total time=  25.6s\n",
      "[CV 4/5; 25/100] START bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6391\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 4/5; 25/100] END bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28;, score=0.908 total time=  13.0s\n",
      "[CV 1/5; 27/100] START bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5583\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 1/5; 27/100] END bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71;, score=0.908 total time=  12.1s\n",
      "[CV 1/5; 21/100] END bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30;, score=0.909 total time=  21.6s\n",
      "[CV 1/5; 22/100] START bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5249\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[CV 1/5; 22/100] END bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19;, score=0.908 total time=  14.1s\n",
      "[CV 2/5; 23/100] START bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5626\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 2/5; 23/100] END bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40;, score=0.909 total time=  23.8s\n",
      "[CV 3/5; 25/100] START bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6386\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 3/5; 25/100] END bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28;, score=0.908 total time=  14.1s\n",
      "[CV 5/5; 26/100] START bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6517\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[CV 5/5; 26/100] END bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29;, score=0.909 total time=  19.5s\n",
      "[CV 5/5; 28/100] START bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6775643618422824, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6775643618422824\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.795446471594121, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.795446471594121\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5932592551999272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5932592551999272\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.016587828927856152, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.016587828927856152\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 3/5; 20/100] END bagging_fraction=0.5932592551999272, bagging_freq=3, feature_fraction=0.795446471594121, lambda_l1=0.6775643618422824, lambda_l2=0.016587828927856152, learning_rate=0.05364884053843169, max_bin=218, max_depth=6, min_data_in_leaf=115, num_leaves=78;, score=0.909 total time=  24.7s\n",
      "[CV 5/5; 22/100] START bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5238\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17495492709593619, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17495492709593619\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[CV 5/5; 22/100] END bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19;, score=0.909 total time=  12.4s\n",
      "[CV 5/5; 23/100] START bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5625\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 5/5; 23/100] END bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40;, score=0.909 total time=  26.7s\n",
      "[CV 1/5; 26/100] START bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6530\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[CV 1/5; 26/100] END bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29;, score=0.909 total time=  20.0s\n",
      "[CV 4/5; 27/100] START bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5578\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5177513505274801, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5177513505274801\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6987860105437611, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6987860105437611\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5871832145024958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5871832145024958\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.837710105907328, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.837710105907328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=197, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=197\n",
      "[CV 5/5; 21/100] END bagging_fraction=0.5871832145024958, bagging_freq=1, feature_fraction=0.6987860105437611, lambda_l1=0.5177513505274801, lambda_l2=0.837710105907328, learning_rate=0.06919056111873167, max_bin=256, max_depth=11, min_data_in_leaf=197, num_leaves=30;, score=0.909 total time=  19.0s\n",
      "[CV 2/5; 24/100] START bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6740\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8985541885270792, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8985541885270792\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5808143570473069, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5808143570473069\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8210158230771438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8210158230771438\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6064290596595899, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6064290596595899\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=172, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=172\n",
      "[CV 2/5; 24/100] END bagging_fraction=0.8210158230771438, bagging_freq=1, feature_fraction=0.5808143570473069, lambda_l1=0.8985541885270792, lambda_l2=0.6064290596595899, learning_rate=0.005873719903579816, max_bin=288, max_depth=5, min_data_in_leaf=172, num_leaves=25;, score=0.902 total time=  18.9s\n",
      "[CV 2/5; 25/100] START bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.349206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6391\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 2/5; 25/100] END bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28;, score=0.907 total time=  14.8s\n",
      "[CV 4/5; 26/100] START bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6526\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[CV 4/5; 26/100] END bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29;, score=0.909 total time=  20.0s\n",
      "[CV 1/5; 29/100] START bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6682\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6142750108986499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6142750108986499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.770723986913783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.770723986913783\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9821683433294356, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9821683433294356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=67\n",
      "[CV 3/5; 22/100] END bagging_fraction=0.770723986913783, bagging_freq=3, feature_fraction=0.6142750108986499, lambda_l1=0.17495492709593619, lambda_l2=0.9821683433294356, learning_rate=0.05408040967074636, max_bin=201, max_depth=3, min_data_in_leaf=67, num_leaves=19;, score=0.909 total time=  13.3s\n",
      "[CV 3/5; 23/100] START bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5628\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3492095746126609, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3492095746126609\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6695148955243504, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6695148955243504\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7791467268035488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7791467268035488\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7259556788702394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7259556788702394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 3/5; 23/100] END bagging_fraction=0.7791467268035488, bagging_freq=7, feature_fraction=0.6695148955243504, lambda_l1=0.3492095746126609, lambda_l2=0.7259556788702394, learning_rate=0.09022547469549483, max_bin=223, max_depth=8, min_data_in_leaf=139, num_leaves=40;, score=0.910 total time=  26.2s\n",
      "[CV 5/5; 25/100] START bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6381\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.44842414298624733, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.44842414298624733\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7428068767931133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7428068767931133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 5/5; 25/100] END bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28;, score=0.907 total time=  12.8s\n",
      "[CV 3/5; 27/100] START bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5578\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 3/5; 27/100] END bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71;, score=0.909 total time=  11.7s\n",
      "[CV 1/5; 28/100] START bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5738\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 1/5; 28/100] END bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44;, score=0.909 total time=  30.4s\n",
      "[CV 5/5; 29/100] START bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6520\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[CV 2/5; 26/100] END bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29;, score=0.909 total time=  17.8s\n",
      "[CV 5/5; 27/100] START bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5574\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 5/5; 27/100] END bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71;, score=0.909 total time=  13.5s\n",
      "[CV 3/5; 29/100] START bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6672\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[CV 3/5; 29/100] END bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74;, score=0.905 total time=  33.1s\n",
      "[CV 4/5; 30/100] START bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5768\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 4/5; 30/100] END bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73;, score=0.905 total time=  27.7s\n",
      "[CV 2/5; 32/100] START bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5494\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5804040257087493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5804040257087493\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9944574626108207, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9944574626108207\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 1/5; 25/100] END bagging_fraction=0.5804040257087493, bagging_freq=2, feature_fraction=0.7428068767931133, lambda_l1=0.44842414298624733, lambda_l2=0.9944574626108207, learning_rate=0.02171289900434781, max_bin=267, max_depth=3, min_data_in_leaf=161, num_leaves=28;, score=0.907 total time=  14.2s\n",
      "[CV 3/5; 26/100] START bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6523\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8492234104941779, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8492234104941779\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8248164495236073, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8248164495236073\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5894113546106644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5894113546106644\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6576128923003434, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6576128923003434\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[CV 3/5; 26/100] END bagging_fraction=0.5894113546106644, bagging_freq=8, feature_fraction=0.8248164495236073, lambda_l1=0.8492234104941779, lambda_l2=0.6576128923003434, learning_rate=0.0589893173168698, max_bin=275, max_depth=5, min_data_in_leaf=138, num_leaves=29;, score=0.909 total time=  19.2s\n",
      "[CV 4/5; 28/100] START bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.356650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5736\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 4/5; 28/100] END bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44;, score=0.910 total time=  28.8s\n",
      "[CV 2/5; 30/100] START bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5768\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 2/5; 30/100] END bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73;, score=0.905 total time=  29.0s\n",
      "[CV 2/5; 31/100] START bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5892\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[CV 2/5; 31/100] END bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57;, score=0.909 total time=  28.3s\n",
      "[CV 1/5; 33/100] START bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5729\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 5/5; 28/100] END bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44;, score=0.909 total time=  29.6s\n",
      "[CV 3/5; 30/100] START bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5769\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 3/5; 30/100] END bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73;, score=0.905 total time=  28.1s\n",
      "[CV 1/5; 32/100] START bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5499\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[CV 1/5; 32/100] END bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73;, score=0.909 total time=  19.7s\n",
      "[CV 5/5; 32/100] START bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5491\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4867421529594551, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4867421529594551\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9894464291375045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9894464291375045\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 2/5; 27/100] END bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71;, score=0.909 total time=  12.4s\n",
      "[CV 3/5; 28/100] START bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5735\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 3/5; 28/100] END bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44;, score=0.910 total time=  29.2s\n",
      "[CV 4/5; 29/100] START bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6682\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[CV 4/5; 29/100] END bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74;, score=0.906 total time=  32.1s\n",
      "[CV 3/5; 31/100] START bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5888\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[CV 3/5; 31/100] END bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57;, score=0.909 total time=  29.2s\n",
      "[CV 3/5; 33/100] START bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6217\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 3/5; 33/100] END bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59;, score=0.909 total time=  28.1s\n",
      "[CV 4/5; 34/100] START bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6219948216895418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6219948216895418\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.906098787718554, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.906098787718554\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 4/5; 27/100] END bagging_fraction=0.6219948216895418, bagging_freq=6, feature_fraction=0.9894464291375045, lambda_l1=0.4867421529594551, lambda_l2=0.906098787718554, learning_rate=0.04626746472349072, max_bin=220, max_depth=3, min_data_in_leaf=47, num_leaves=71;, score=0.909 total time=  12.1s\n",
      "[CV 2/5; 29/100] START bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6673\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[CV 2/5; 29/100] END bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74;, score=0.906 total time=  43.9s\n",
      "[CV 1/5; 31/100] START bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5892\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[CV 1/5; 31/100] END bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57;, score=0.909 total time=  28.3s\n",
      "[CV 4/5; 32/100] START bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5496\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[CV 4/5; 32/100] END bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73;, score=0.909 total time=  19.7s\n",
      "[CV 2/5; 34/100] START bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5307\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[CV 1/5; 29/100] END bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74;, score=0.905 total time=  43.0s\n",
      "[CV 5/5; 30/100] START bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5763\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 5/5; 30/100] END bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73;, score=0.905 total time=  28.3s\n",
      "[CV 3/5; 32/100] START bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5497\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[CV 3/5; 32/100] END bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73;, score=0.909 total time=  22.1s\n",
      "[CV 1/5; 34/100] START bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5312\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 1/5; 34/100] END bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54;, score=0.909 total time=  25.1s\n",
      "[CV 4/5; 35/100] START bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5513\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[CV 2/5; 28/100] START bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5729\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.768554014306309, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.768554014306309\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7860020996045916, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7860020996045916\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320837825359516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320837825359516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04360377175443375, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04360377175443375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 2/5; 28/100] END bagging_fraction=0.9320837825359516, bagging_freq=3, feature_fraction=0.7860020996045916, lambda_l1=0.768554014306309, lambda_l2=0.04360377175443375, learning_rate=0.0994822985257474, max_bin=229, max_depth=7, min_data_in_leaf=25, num_leaves=44;, score=0.909 total time=  30.8s\n",
      "[CV 1/5; 30/100] START bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5771\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8442131407263114, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8442131407263114\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6973457334047362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6973457334047362\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5847463733430462, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5847463733430462\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9300168348108319, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9300168348108319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "[CV 1/5; 30/100] END bagging_fraction=0.5847463733430462, bagging_freq=9, feature_fraction=0.6973457334047362, lambda_l1=0.8442131407263114, lambda_l2=0.9300168348108319, learning_rate=0.01168953243070667, max_bin=231, max_depth=8, min_data_in_leaf=170, num_leaves=73;, score=0.905 total time=  31.9s\n",
      "[CV 5/5; 31/100] START bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5883\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[CV 5/5; 31/100] END bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57;, score=0.909 total time=  29.2s\n",
      "[CV 4/5; 33/100] START bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6222\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 4/5; 33/100] END bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59;, score=0.910 total time=  28.5s\n",
      "[CV 1/5; 35/100] START bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.205673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5516\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6673\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5722924691708383, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5722924691708383\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.776382483417745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.776382483417745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9641592812938626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9641592812938626\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9803315837160457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9803315837160457\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "[CV 5/5; 29/100] END bagging_fraction=0.9641592812938626, bagging_freq=1, feature_fraction=0.776382483417745, lambda_l1=0.5722924691708383, lambda_l2=0.9803315837160457, learning_rate=0.012157894320582181, max_bin=284, max_depth=9, min_data_in_leaf=180, num_leaves=74;, score=0.906 total time=  32.2s\n",
      "[CV 4/5; 31/100] START bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5889\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8486697949246744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8486697949246744\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6612753821193003, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6612753821193003\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6270818245348694, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6270818245348694\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.13662133144202881, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.13662133144202881\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=45, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=45\n",
      "[CV 4/5; 31/100] END bagging_fraction=0.6270818245348694, bagging_freq=2, feature_fraction=0.6612753821193003, lambda_l1=0.8486697949246744, lambda_l2=0.13662133144202881, learning_rate=0.07234654470646128, max_bin=238, max_depth=12, min_data_in_leaf=45, num_leaves=57;, score=0.910 total time=  28.2s\n",
      "[CV 2/5; 33/100] START bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6220\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 2/5; 33/100] END bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59;, score=0.909 total time=  30.4s\n",
      "[CV 3/5; 35/100] START bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.324351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5514\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 3/5; 35/100] END bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26;, score=0.909 total time=  29.6s\n",
      "[CV 1/5; 37/100] START bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5233\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 1/5; 37/100] END bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29;, score=0.909 total time=  26.9s\n",
      "[CV 2/5; 38/100] START bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[CV 5/5; 32/100] END bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73;, score=0.909 total time=  20.0s\n",
      "[CV 3/5; 34/100] START bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5309\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 3/5; 34/100] END bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54;, score=0.909 total time=  24.9s\n",
      "[CV 1/5; 36/100] START bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5431\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 1/5; 36/100] END bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65;, score=0.909 total time=  30.5s\n",
      "[CV 4/5; 37/100] START bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5242\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 4/5; 37/100] END bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29;, score=0.909 total time=  24.3s\n",
      "[CV 2/5; 39/100] START bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5391\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 2/5; 39/100] END bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41;, score=0.909 total time=  27.2s\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7982951789667752, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7982951789667752\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7507581473435998, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7507581473435998\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9335361592900518, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9335361592900518\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6499639307777652, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6499639307777652\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=162, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=162\n",
      "[CV 2/5; 32/100] END bagging_fraction=0.9335361592900518, bagging_freq=7, feature_fraction=0.7507581473435998, lambda_l1=0.7982951789667752, lambda_l2=0.6499639307777652, learning_rate=0.07168685333948183, max_bin=215, max_depth=5, min_data_in_leaf=162, num_leaves=73;, score=0.909 total time=  20.1s\n",
      "[CV 5/5; 33/100] START bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6214\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 5/5; 33/100] END bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59;, score=0.909 total time=  28.5s\n",
      "[CV 2/5; 35/100] START bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5511\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 2/5; 35/100] END bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26;, score=0.909 total time=  29.7s\n",
      "[CV 5/5; 36/100] START bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5422\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 5/5; 36/100] END bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65;, score=0.909 total time=  29.1s\n",
      "[CV 1/5; 39/100] START bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5397\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 2/5; 34/100] END bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54;, score=0.909 total time=  24.9s\n",
      "[CV 5/5; 35/100] START bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5508\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 5/5; 35/100] END bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26;, score=0.909 total time=  27.5s\n",
      "[CV 3/5; 37/100] START bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5227\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 3/5; 37/100] END bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29;, score=0.909 total time=  25.7s\n",
      "[CV 4/5; 38/100] START bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6561\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[CV 4/5; 38/100] END bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69;, score=0.902 total time=  36.2s\n",
      "[CV 2/5; 40/100] START bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5626\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[CV 2/5; 40/100] END bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39;, score=0.908 total time=  28.7s\n",
      "[CV 2/5; 42/100] START bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5309\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 4/5; 34/100] END bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54;, score=0.909 total time=  27.5s\n",
      "[CV 2/5; 36/100] START bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5427\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 2/5; 36/100] END bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65;, score=0.909 total time=  28.0s\n",
      "[CV 5/5; 37/100] START bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5221\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 5/5; 37/100] END bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29;, score=0.909 total time=  25.0s\n",
      "[CV 3/5; 39/100] START bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5396\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 3/5; 39/100] END bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41;, score=0.909 total time=  27.3s\n",
      "[CV 1/5; 41/100] START bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5967\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6228\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1550416167277442, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1550416167277442\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664076333737366, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664076333737366\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7192370615090435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7192370615090435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9818408883105311, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9818408883105311\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 1/5; 33/100] END bagging_fraction=0.7192370615090435, bagging_freq=6, feature_fraction=0.664076333737366, lambda_l1=0.1550416167277442, lambda_l2=0.9818408883105311, learning_rate=0.08469868269658952, max_bin=257, max_depth=8, min_data_in_leaf=68, num_leaves=59;, score=0.909 total time=  30.0s\n",
      "[CV 5/5; 34/100] START bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5302\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3266512417960409, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3266512417960409\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7685412135983277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7685412135983277\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5194173672147115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5194173672147115\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8278690037875887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8278690037875887\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[CV 5/5; 34/100] END bagging_fraction=0.5194173672147115, bagging_freq=6, feature_fraction=0.7685412135983277, lambda_l1=0.3266512417960409, lambda_l2=0.8278690037875887, learning_rate=0.03079657700287548, max_bin=204, max_depth=9, min_data_in_leaf=25, num_leaves=54;, score=0.909 total time=  30.6s\n",
      "[CV 2/5; 37/100] START bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5228\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.09387329008129175, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09387329008129175\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6198936795787016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6198936795787016\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9576068638132402, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9576068638132402\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.18286599710730733, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.18286599710730733\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 2/5; 37/100] END bagging_fraction=0.9576068638132402, bagging_freq=2, feature_fraction=0.6198936795787016, lambda_l1=0.09387329008129175, lambda_l2=0.18286599710730733, learning_rate=0.09378832974727243, max_bin=200, max_depth=9, min_data_in_leaf=88, num_leaves=29;, score=0.909 total time=  27.5s\n",
      "[CV 3/5; 38/100] START bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6557\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[CV 3/5; 38/100] END bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69;, score=0.902 total time=  36.8s\n",
      "[CV 3/5; 40/100] START bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5628\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9758520794625346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9758520794625346\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8630456668613308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8630456668613308\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8497561053835969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497561053835969\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5163003483011953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5163003483011953\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 1/5; 35/100] END bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26;, score=0.909 total time=  28.7s\n",
      "[CV 4/5; 36/100] START bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5427\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 4/5; 36/100] END bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65;, score=0.909 total time=  30.3s\n",
      "[CV 5/5; 38/100] START bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6551\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[CV 5/5; 38/100] END bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69;, score=0.902 total time=  37.0s\n",
      "[CV 5/5; 40/100] START bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5625\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[CV 5/5; 40/100] END bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39;, score=0.908 total time=  26.3s\n",
      "[CV 1/5; 42/100] START bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5249\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[CV 1/5; 42/100] END bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88;, score=0.907 total time=  20.9s\n",
      "[CV 5/5; 42/100] START bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 4/5; 35/100] END bagging_fraction=0.8497561053835969, bagging_freq=8, feature_fraction=0.8630456668613308, lambda_l1=0.9758520794625346, lambda_l2=0.5163003483011953, learning_rate=0.03568086492941837, max_bin=216, max_depth=10, min_data_in_leaf=82, num_leaves=26;, score=0.909 total time=  22.9s\n",
      "[CV 3/5; 36/100] START bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5428\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18433367433137005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18433367433137005\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6254302636733307, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6254302636733307\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.719485710352818, subsample=1.0 will be ignored. Current value: bagging_fraction=0.719485710352818\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08087296661719767, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08087296661719767\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=148, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=148\n",
      "[CV 3/5; 36/100] END bagging_fraction=0.719485710352818, bagging_freq=7, feature_fraction=0.6254302636733307, lambda_l1=0.18433367433137005, lambda_l2=0.08087296661719767, learning_rate=0.04568987511931023, max_bin=211, max_depth=7, min_data_in_leaf=148, num_leaves=65;, score=0.910 total time=  28.3s\n",
      "[CV 1/5; 38/100] START bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6565\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[CV 1/5; 38/100] END bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69;, score=0.902 total time=  37.7s\n",
      "[CV 1/5; 40/100] START bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5636\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[CV 1/5; 40/100] END bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39;, score=0.908 total time=  28.2s\n",
      "[CV 4/5; 41/100] START bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5960\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 4/5; 41/100] END bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74;, score=0.910 total time=  36.1s\n",
      "[CV 2/5; 44/100] START bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6553\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3559726786512616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3559726786512616\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6238654947505787, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6238654947505787\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8650196582809093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8650196582809093\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7578461104643691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7578461104643691\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=66, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=66\n",
      "[CV 2/5; 38/100] END bagging_fraction=0.8650196582809093, bagging_freq=3, feature_fraction=0.6238654947505787, lambda_l1=0.3559726786512616, lambda_l2=0.7578461104643691, learning_rate=0.006367381419826807, max_bin=277, max_depth=7, min_data_in_leaf=66, num_leaves=69;, score=0.902 total time=  38.5s\n",
      "[CV 4/5; 40/100] START bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5630\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8564898411883223, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8564898411883223\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7515681292900438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7515681292900438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.630446811670857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.630446811670857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.658693631618945, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.658693631618945\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[CV 4/5; 40/100] END bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39;, score=0.909 total time=  26.4s\n",
      "[CV 5/5; 41/100] START bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5957\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 5/5; 41/100] END bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74;, score=0.909 total time=  31.3s\n",
      "[CV 4/5; 43/100] START bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6510\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[CV 4/5; 43/100] END bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28;, score=0.909 total time=  28.2s\n",
      "[CV 4/5; 45/100] START bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[CV 4/5; 39/100] START bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5395\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 4/5; 39/100] END bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41;, score=0.909 total time=  28.0s\n",
      "[CV 2/5; 41/100] START bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5960\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 2/5; 41/100] END bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74;, score=0.909 total time=  29.8s\n",
      "[CV 2/5; 43/100] START bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6504\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[CV 2/5; 43/100] END bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28;, score=0.909 total time=  29.7s\n",
      "[CV 2/5; 45/100] START bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5729\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 2/5; 45/100] END bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69;, score=0.908 total time=  15.5s\n",
      "[CV 1/5; 47/100] START bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 1/5; 39/100] END bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41;, score=0.909 total time=  32.4s\n",
      "[CV 5/5; 39/100] START bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5388\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.17320186991001518, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.17320186991001518\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7367358853902828, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7367358853902828\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518289296900119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518289296900119\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.43385164923797304, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.43385164923797304\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[CV 5/5; 39/100] END bagging_fraction=0.8518289296900119, bagging_freq=5, feature_fraction=0.7367358853902828, lambda_l1=0.17320186991001518, lambda_l2=0.43385164923797304, learning_rate=0.042857949767750474, max_bin=209, max_depth=8, min_data_in_leaf=88, num_leaves=41;, score=0.909 total time=  31.8s\n",
      "[CV 3/5; 42/100] START bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5243\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[CV 3/5; 42/100] END bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88;, score=0.907 total time=  26.2s\n",
      "[CV 3/5; 43/100] START bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6506\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[CV 3/5; 43/100] END bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28;, score=0.909 total time=  28.7s\n",
      "[CV 3/5; 45/100] START bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5735\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 3/5; 45/100] END bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69;, score=0.908 total time=  14.3s\n",
      "[CV 5/5; 46/100] START bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 1/5; 41/100] END bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74;, score=0.909 total time=  30.0s\n",
      "[CV 4/5; 42/100] START bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5259\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[CV 4/5; 42/100] END bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88;, score=0.907 total time=  20.9s\n",
      "[CV 1/5; 44/100] START bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5841\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 1/5; 44/100] END bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31;, score=0.909 total time=  18.6s\n",
      "[CV 1/5; 45/100] START bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5738\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 1/5; 45/100] END bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69;, score=0.907 total time=  15.5s\n",
      "[CV 3/5; 46/100] START bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5546\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 3/5; 46/100] END bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85;, score=0.902 total time=  35.5s\n",
      "[CV 4/5; 48/100] START bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5238\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[CV 5/5; 42/100] END bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88;, score=0.907 total time=  22.3s\n",
      "[CV 3/5; 44/100] START bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5839\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 3/5; 44/100] END bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31;, score=0.909 total time=  17.7s\n",
      "[CV 5/5; 45/100] START bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5729\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 5/5; 45/100] END bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69;, score=0.908 total time=  13.5s\n",
      "[CV 3/5; 47/100] START bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5645\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 3/5; 47/100] END bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60;, score=0.909 total time=  25.8s\n",
      "[CV 1/5; 48/100] START bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5312\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[CV 3/5; 40/100] END bagging_fraction=0.630446811670857, bagging_freq=3, feature_fraction=0.7515681292900438, lambda_l1=0.8564898411883223, lambda_l2=0.658693631618945, learning_rate=0.02047877057273582, max_bin=223, max_depth=9, min_data_in_leaf=21, num_leaves=39;, score=0.909 total time=  24.9s\n",
      "[CV 3/5; 41/100] START bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5957\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[CV 3/5; 41/100] END bagging_fraction=0.9701151207124787, bagging_freq=8, feature_fraction=0.8385841711914909, lambda_l1=0.5733670416719333, lambda_l2=0.12850035323391018, learning_rate=0.0820643967899203, max_bin=242, max_depth=7, min_data_in_leaf=31, num_leaves=74;, score=0.910 total time=  36.7s\n",
      "[CV 5/5; 43/100] START bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6501\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[CV 5/5; 43/100] END bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28;, score=0.909 total time=  28.6s\n",
      "[CV 1/5; 46/100] START bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5550\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 1/5; 46/100] END bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85;, score=0.902 total time=  37.4s\n",
      "[CV 5/5; 47/100] START bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5642\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 5/5; 47/100] END bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60;, score=0.909 total time=  31.0s\n",
      "[CV 4/5; 50/100] START bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5246\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19579113478929644, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19579113478929644\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9526753209780319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9526753209780319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.910213421885634, subsample=1.0 will be ignored. Current value: bagging_fraction=0.910213421885634\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.06936130087516545, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06936130087516545\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[CV 2/5; 42/100] END bagging_fraction=0.910213421885634, bagging_freq=8, feature_fraction=0.9526753209780319, lambda_l1=0.19579113478929644, lambda_l2=0.06936130087516545, learning_rate=0.014573910130855534, max_bin=201, max_depth=5, min_data_in_leaf=54, num_leaves=88;, score=0.907 total time=  21.2s\n",
      "[CV 1/5; 43/100] START bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6515\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4960374542934062, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4960374542934062\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6303472641179515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6303472641179515\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8278613176903711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8278613176903711\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6928903586919394, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6928903586919394\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=183, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=183\n",
      "[CV 1/5; 43/100] END bagging_fraction=0.8278613176903711, bagging_freq=2, feature_fraction=0.6303472641179515, lambda_l1=0.4960374542934062, lambda_l2=0.6928903586919394, learning_rate=0.038091977423053744, max_bin=274, max_depth=10, min_data_in_leaf=183, num_leaves=28;, score=0.909 total time=  28.9s\n",
      "[CV 4/5; 44/100] START bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5839\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 4/5; 44/100] END bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31;, score=0.910 total time=  17.1s\n",
      "[CV 2/5; 46/100] START bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5542\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 2/5; 46/100] END bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85;, score=0.902 total time=  37.3s\n",
      "[CV 1/5; 49/100] START bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5447\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5841\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 2/5; 44/100] END bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31;, score=0.909 total time=  17.2s\n",
      "[CV 5/5; 44/100] START bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5831\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7506147516408583, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7506147516408583\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5887197718898614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5887197718898614\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9837902757601238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9837902757601238\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.806834739267264, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.806834739267264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[CV 5/5; 44/100] END bagging_fraction=0.9837902757601238, bagging_freq=4, feature_fraction=0.5887197718898614, lambda_l1=0.7506147516408583, lambda_l2=0.806834739267264, learning_rate=0.09909798849006397, max_bin=235, max_depth=4, min_data_in_leaf=115, num_leaves=31;, score=0.909 total time=  17.5s\n",
      "[CV 4/5; 46/100] START bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.348713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5545\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 4/5; 46/100] END bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85;, score=0.902 total time=  35.3s\n",
      "[CV 5/5; 48/100] START bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5302\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 5/5; 48/100] END bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25;, score=0.909 total time=  18.5s\n",
      "[CV 1/5; 50/100] START bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6348\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5653\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 1/5; 47/100] END bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60;, score=0.909 total time=  30.1s\n",
      "[CV 2/5; 48/100] START bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5307\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 2/5; 48/100] END bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25;, score=0.909 total time=  18.4s\n",
      "[CV 2/5; 49/100] START bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5443\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[CV 2/5; 49/100] END bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16;, score=0.905 total time=  20.8s\n",
      "[CV 5/5; 50/100] START bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6331\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[CV 5/5; 50/100] END bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78;, score=0.909 total time=  33.9s\n",
      "[CV 5/5; 52/100] START bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6450\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[CV 5/5; 52/100] END bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8;, score=0.909 total time=  13.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5736\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6798447799002458, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6798447799002458\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.61929842989517, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.61929842989517\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933558303604861, subsample=1.0 will be ignored. Current value: bagging_fraction=0.933558303604861\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7399087604473745, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7399087604473745\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 4/5; 45/100] END bagging_fraction=0.933558303604861, bagging_freq=5, feature_fraction=0.61929842989517, lambda_l1=0.6798447799002458, lambda_l2=0.7399087604473745, learning_rate=0.027632434478378046, max_bin=229, max_depth=3, min_data_in_leaf=132, num_leaves=69;, score=0.908 total time=  13.9s\n",
      "[CV 2/5; 47/100] START bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5643\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 2/5; 47/100] END bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60;, score=0.909 total time=  25.4s\n",
      "[CV 4/5; 47/100] START bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5647\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6318372121697993, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6318372121697993\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7867189440616431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7867189440616431\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8383849522121516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8383849522121516\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44844552197831977, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44844552197831977\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=142, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=142\n",
      "[CV 4/5; 47/100] END bagging_fraction=0.8383849522121516, bagging_freq=4, feature_fraction=0.7867189440616431, lambda_l1=0.6318372121697993, lambda_l2=0.44844552197831977, learning_rate=0.03285502331131613, max_bin=224, max_depth=6, min_data_in_leaf=142, num_leaves=60;, score=0.909 total time=  32.0s\n",
      "[CV 3/5; 50/100] START bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6334\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[CV 3/5; 50/100] END bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78;, score=0.909 total time=  38.1s\n",
      "[CV 1/5; 52/100] START bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6465\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[CV 1/5; 52/100] END bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8;, score=0.909 total time=  15.0s\n",
      "[CV 2/5; 53/100] START bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5542\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8955232284962005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8955232284962005\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6600248005153059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6600248005153059\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7482805953415388, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7482805953415388\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3892016787341631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3892016787341631\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 5/5; 46/100] END bagging_fraction=0.7482805953415388, bagging_freq=9, feature_fraction=0.6600248005153059, lambda_l1=0.8955232284962005, lambda_l2=0.3892016787341631, learning_rate=0.0060295768906283445, max_bin=218, max_depth=11, min_data_in_leaf=47, num_leaves=85;, score=0.902 total time=  33.2s\n",
      "[CV 3/5; 48/100] START bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5309\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 3/5; 48/100] END bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25;, score=0.910 total time=  18.1s\n",
      "[CV 4/5; 49/100] START bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5444\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[CV 4/5; 49/100] END bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16;, score=0.905 total time=  24.8s\n",
      "[CV 3/5; 51/100] START bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5888\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[CV 3/5; 51/100] END bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26;, score=0.909 total time=  26.0s\n",
      "[CV 4/5; 52/100] START bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6459\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[CV 4/5; 52/100] END bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8;, score=0.909 total time=  14.0s\n",
      "[CV 5/5; 53/100] START bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5309\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 4/5; 48/100] END bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25;, score=0.909 total time=  18.3s\n",
      "[CV 5/5; 49/100] START bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5439\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[CV 5/5; 49/100] END bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16;, score=0.905 total time=  20.6s\n",
      "[CV 1/5; 51/100] START bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5892\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[CV 1/5; 51/100] END bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26;, score=0.909 total time=  26.0s\n",
      "[CV 3/5; 52/100] START bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6453\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[CV 3/5; 52/100] END bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8;, score=0.909 total time=  14.2s\n",
      "[CV 3/5; 53/100] START bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5327\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.9309291056872926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9309291056872926\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.556732301008724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.556732301008724\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8957895218629243, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8957895218629243\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9742482085344102, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9742482085344102\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 1/5; 48/100] END bagging_fraction=0.8957895218629243, bagging_freq=9, feature_fraction=0.556732301008724, lambda_l1=0.9309291056872926, lambda_l2=0.9742482085344102, learning_rate=0.09961346799288127, max_bin=204, max_depth=5, min_data_in_leaf=94, num_leaves=25;, score=0.909 total time=  20.1s\n",
      "[CV 3/5; 49/100] START bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5446\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6182180633162611, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6182180633162611\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8807553158587361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8807553158587361\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9843259719870674, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9843259719870674\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.10112267612279024, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.10112267612279024\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[CV 3/5; 49/100] END bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16;, score=0.905 total time=  24.8s\n",
      "[CV 2/5; 51/100] START bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5892\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[CV 2/5; 51/100] END bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26;, score=0.909 total time=  28.6s\n",
      "[CV 1/5; 53/100] START bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5328\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[CV 1/5; 53/100] END bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52;, score=0.909 total time=  36.9s\n",
      "[CV 4/5; 54/100] START bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5545\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 4/5; 54/100] END bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54;, score=0.909 total time=  25.1s\n",
      "[CV 2/5; 56/100] START bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6339\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[CV 4/5; 50/100] END bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78;, score=0.909 total time=  38.5s\n",
      "[CV 2/5; 52/100] START bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6458\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4052020867814171, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4052020867814171\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268410869365694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268410869365694\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5555987411530756, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5555987411530756\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.5714730252050106, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5714730252050106\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=186, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=186\n",
      "[CV 2/5; 52/100] END bagging_fraction=0.5555987411530756, bagging_freq=3, feature_fraction=0.6268410869365694, lambda_l1=0.4052020867814171, lambda_l2=0.5714730252050106, learning_rate=0.0753925167865135, max_bin=271, max_depth=11, min_data_in_leaf=186, num_leaves=8;, score=0.909 total time=  14.8s\n",
      "[CV 4/5; 53/100] START bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.193679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5326\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[CV 4/5; 53/100] END bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52;, score=0.909 total time=  36.3s\n",
      "[CV 5/5; 55/100] START bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5422\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[CV 5/5; 55/100] END bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79;, score=0.903 total time=  29.6s\n",
      "[CV 4/5; 57/100] START bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5718\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] min_data_in_leaf is set=189, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=189\n",
      "[CV 1/5; 49/100] END bagging_fraction=0.9843259719870674, bagging_freq=9, feature_fraction=0.8807553158587361, lambda_l1=0.6182180633162611, lambda_l2=0.10112267612279024, learning_rate=0.012990146580924757, max_bin=212, max_depth=10, min_data_in_leaf=189, num_leaves=16;, score=0.905 total time=  21.1s\n",
      "[CV 2/5; 50/100] START bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6339\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[CV 2/5; 50/100] END bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78;, score=0.909 total time=  40.1s\n",
      "[CV 5/5; 51/100] START bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5883\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[CV 5/5; 51/100] END bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26;, score=0.909 total time=  27.8s\n",
      "[CV 3/5; 54/100] START bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5546\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 3/5; 54/100] END bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54;, score=0.909 total time=  29.1s\n",
      "[CV 1/5; 56/100] START bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6228\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 1/5; 56/100] END bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48;, score=0.909 total time=  14.8s\n",
      "[CV 3/5; 56/100] START bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5319\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[CV 5/5; 53/100] END bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52;, score=0.909 total time=  33.0s\n",
      "[CV 4/5; 55/100] START bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5427\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[CV 4/5; 55/100] END bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79;, score=0.903 total time=  30.2s\n",
      "[CV 3/5; 57/100] START bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5716\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 3/5; 57/100] END bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89;, score=0.909 total time=  16.8s\n",
      "[CV 1/5; 59/100] START bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6072\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.08483771408519192, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.08483771408519192\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5406743903209499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5406743903209499\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9109300296451781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9109300296451781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9866395785011755, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9866395785011755\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=164, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=164\n",
      "[CV 1/5; 50/100] END bagging_fraction=0.9109300296451781, bagging_freq=2, feature_fraction=0.5406743903209499, lambda_l1=0.08483771408519192, lambda_l2=0.9866395785011755, learning_rate=0.040555725596831425, max_bin=264, max_depth=8, min_data_in_leaf=164, num_leaves=78;, score=0.909 total time=  40.5s\n",
      "[CV 4/5; 51/100] START bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5889\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7081811955831998, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7081811955831998\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6751091233501567, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6751091233501567\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9930005319114354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9930005319114354\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.48166698796411533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.48166698796411533\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[CV 4/5; 51/100] END bagging_fraction=0.9930005319114354, bagging_freq=4, feature_fraction=0.6751091233501567, lambda_l1=0.7081811955831998, lambda_l2=0.48166698796411533, learning_rate=0.04090882106161417, max_bin=238, max_depth=12, min_data_in_leaf=109, num_leaves=26;, score=0.909 total time=  28.3s\n",
      "[CV 2/5; 54/100] START bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5542\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 2/5; 54/100] END bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54;, score=0.909 total time=  27.0s\n",
      "[CV 3/5; 55/100] START bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5428\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[CV 3/5; 55/100] END bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79;, score=0.903 total time=  30.1s\n",
      "[CV 2/5; 57/100] START bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5712\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6220\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 2/5; 56/100] END bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48;, score=0.909 total time=  17.6s\n",
      "[CV 5/5; 57/100] START bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.206625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5713\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 5/5; 57/100] END bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89;, score=0.909 total time=  16.0s\n",
      "[CV 3/5; 59/100] START bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6063\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[CV 3/5; 59/100] END bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8;, score=0.909 total time=  10.0s\n",
      "[CV 5/5; 59/100] START bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[CV 5/5; 59/100] END bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8;, score=0.909 total time=  10.1s\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5325\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.28571208628186073, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.28571208628186073\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[CV 2/5; 53/100] END bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52;, score=0.909 total time=  33.5s\n",
      "[CV 2/5; 55/100] START bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5427\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[CV 2/5; 55/100] END bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79;, score=0.903 total time=  29.9s\n",
      "[CV 1/5; 57/100] START bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5720\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 1/5; 57/100] END bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89;, score=0.909 total time=  17.1s\n",
      "[CV 3/5; 58/100] START bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5309\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[CV 1/5; 54/100] START bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5550\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 1/5; 54/100] END bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54;, score=0.908 total time=  26.3s\n",
      "[CV 5/5; 54/100] START bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5542\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.924643330223596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.924643330223596\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5217003916490863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5217003916490863\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9849394133538195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9849394133538195\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9191157152786594, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9191157152786594\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=165, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=165\n",
      "[CV 5/5; 54/100] END bagging_fraction=0.9849394133538195, bagging_freq=5, feature_fraction=0.5217003916490863, lambda_l1=0.924643330223596, lambda_l2=0.9191157152786594, learning_rate=0.02903406595897791, max_bin=218, max_depth=6, min_data_in_leaf=165, num_leaves=54;, score=0.909 total time=  27.3s\n",
      "[CV 4/5; 56/100] START bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6222\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 4/5; 56/100] END bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48;, score=0.909 total time=  17.2s\n",
      "[CV 2/5; 58/100] START bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5307\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[CV 2/5; 58/100] END bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72;, score=0.909 total time=  32.7s\n",
      "[CV 5/5; 60/100] START bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6124\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[CV 1/5; 59/100] END bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8;, score=0.909 total time=  10.8s\n",
      "[CV 4/5; 59/100] START bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6068\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[CV 4/5; 59/100] END bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8;, score=0.909 total time=  11.5s\n",
      "[CV 3/5; 60/100] START bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6132\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[CV 3/5; 60/100] END bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44;, score=0.909 total time=  21.4s\n",
      "[CV 1/5; 62/100] START bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6089\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[CV 1/5; 62/100] END bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12;, score=0.908 total time=  11.0s\n",
      "[CV 2/5; 63/100] START bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5460\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 2/5; 63/100] END bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58;, score=0.909 total time=  13.4s\n",
      "[CV 4/5; 64/100] START bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6874352897618521, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6874352897618521\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8405197135484546, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8405197135484546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8685991281894603, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8685991281894603\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[CV 3/5; 53/100] END bagging_fraction=0.8405197135484546, bagging_freq=9, feature_fraction=0.6874352897618521, lambda_l1=0.28571208628186073, lambda_l2=0.8685991281894603, learning_rate=0.026241604659348, max_bin=205, max_depth=12, min_data_in_leaf=63, num_leaves=52;, score=0.909 total time=  33.0s\n",
      "[CV 1/5; 55/100] START bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5431\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5843143119231002, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5843143119231002\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8137790400420317, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8137790400420317\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8146993190676313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8146993190676313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.901158010490989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.901158010490989\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=35\n",
      "[CV 1/5; 55/100] END bagging_fraction=0.8146993190676313, bagging_freq=1, feature_fraction=0.8137790400420317, lambda_l1=0.5843143119231002, lambda_l2=0.901158010490989, learning_rate=0.009317406132438498, max_bin=211, max_depth=12, min_data_in_leaf=35, num_leaves=79;, score=0.903 total time=  29.7s\n",
      "[CV 5/5; 56/100] START bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6214\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 5/5; 56/100] END bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48;, score=0.909 total time=  17.7s\n",
      "[CV 4/5; 58/100] START bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5309\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[CV 4/5; 58/100] END bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72;, score=0.910 total time=  29.5s\n",
      "[CV 2/5; 61/100] START bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6537\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 2/5; 61/100] END bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39;, score=0.909 total time=  25.3s\n",
      "[CV 5/5; 62/100] START bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 2/5; 57/100] END bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89;, score=0.909 total time=  17.2s\n",
      "[CV 5/5; 58/100] START bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5302\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[CV 5/5; 58/100] END bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72;, score=0.909 total time=  29.6s\n",
      "[CV 3/5; 61/100] START bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6540\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 3/5; 61/100] END bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39;, score=0.910 total time=  23.9s\n",
      "[CV 1/5; 63/100] START bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5464\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 1/5; 63/100] END bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58;, score=0.909 total time=  14.9s\n",
      "[CV 5/5; 64/100] START bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6297\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6217\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2773811829811327, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2773811829811327\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8100662989007683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8100662989007683\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.697260899304065, subsample=1.0 will be ignored. Current value: bagging_fraction=0.697260899304065\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1881211597237613, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1881211597237613\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=47, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=47\n",
      "[CV 3/5; 56/100] END bagging_fraction=0.697260899304065, bagging_freq=8, feature_fraction=0.8100662989007683, lambda_l1=0.2773811829811327, lambda_l2=0.1881211597237613, learning_rate=0.0490513484692983, max_bin=257, max_depth=4, min_data_in_leaf=47, num_leaves=48;, score=0.909 total time=  17.1s\n",
      "[CV 1/5; 58/100] START bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5312\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4182430362906189, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4182430362906189\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8067075979678949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8067075979678949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7083195288692397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7083195288692397\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9327284833540133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9327284833540133\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[CV 1/5; 58/100] END bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72;, score=0.909 total time=  32.5s\n",
      "[CV 2/5; 60/100] START bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6134\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[CV 2/5; 60/100] END bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44;, score=0.909 total time=  21.4s\n",
      "[CV 5/5; 61/100] START bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6533\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 5/5; 61/100] END bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39;, score=0.909 total time=  25.1s\n",
      "[CV 3/5; 64/100] START bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6301\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[CV 3/5; 64/100] END bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45;, score=0.909 total time=  19.7s\n",
      "[CV 1/5; 66/100] START bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2797635071689457, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2797635071689457\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5161579755548252, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5161579755548252\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6299749314271313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6299749314271313\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.41120672087218624, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41120672087218624\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=153, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=153\n",
      "[CV 4/5; 57/100] END bagging_fraction=0.6299749314271313, bagging_freq=6, feature_fraction=0.5161579755548252, lambda_l1=0.2797635071689457, lambda_l2=0.41120672087218624, learning_rate=0.06226427879221336, max_bin=228, max_depth=5, min_data_in_leaf=153, num_leaves=89;, score=0.909 total time=  17.4s\n",
      "[CV 2/5; 59/100] START bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6063\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7495782999760721, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7495782999760721\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6883694849689703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6883694849689703\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7401850409331774, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7401850409331774\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3929894489788591, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3929894489788591\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[CV 2/5; 59/100] END bagging_fraction=0.7401850409331774, bagging_freq=1, feature_fraction=0.6883694849689703, lambda_l1=0.7495782999760721, lambda_l2=0.3929894489788591, learning_rate=0.08377060096711887, max_bin=248, max_depth=3, min_data_in_leaf=117, num_leaves=8;, score=0.909 total time=  12.4s\n",
      "[CV 1/5; 60/100] START bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6140\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[CV 1/5; 60/100] END bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44;, score=0.909 total time=  21.4s\n",
      "[CV 4/5; 61/100] START bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6544\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 4/5; 61/100] END bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39;, score=0.910 total time=  25.1s\n",
      "[CV 5/5; 63/100] START bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5457\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5; 60/100] START bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6137\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[CV 4/5; 60/100] END bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44;, score=0.910 total time=  22.2s\n",
      "[CV 2/5; 62/100] START bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6081\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[CV 2/5; 62/100] END bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12;, score=0.909 total time=  11.0s\n",
      "[CV 3/5; 63/100] START bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5463\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 3/5; 63/100] END bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58;, score=0.909 total time=  13.5s\n",
      "[CV 1/5; 65/100] START bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5516\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 1/5; 65/100] END bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91;, score=0.909 total time=  38.4s\n",
      "[CV 4/5; 66/100] START bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[CV 3/5; 58/100] END bagging_fraction=0.7083195288692397, bagging_freq=5, feature_fraction=0.8067075979678949, lambda_l1=0.4182430362906189, lambda_l2=0.9327284833540133, learning_rate=0.0872760695025388, max_bin=204, max_depth=9, min_data_in_leaf=52, num_leaves=72;, score=0.910 total time=  29.3s\n",
      "[CV 1/5; 61/100] START bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6547\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4505441353100935, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4505441353100935\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6236740508715988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6236740508715988\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.878540601021228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.878540601021228\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12915941515149498, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12915941515149498\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[CV 1/5; 61/100] END bagging_fraction=0.878540601021228, bagging_freq=3, feature_fraction=0.6236740508715988, lambda_l1=0.4505441353100935, lambda_l2=0.12915941515149498, learning_rate=0.09563484758957863, max_bin=276, max_depth=6, min_data_in_leaf=132, num_leaves=39;, score=0.909 total time=  25.2s\n",
      "[CV 4/5; 62/100] START bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6087\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[CV 4/5; 62/100] END bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12;, score=0.909 total time=  10.1s\n",
      "[CV 1/5; 64/100] START bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6314\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[CV 1/5; 64/100] END bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45;, score=0.909 total time=  19.7s\n",
      "[CV 4/5; 65/100] START bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5513\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 4/5; 65/100] END bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91;, score=0.910 total time=  38.2s\n",
      "[CV 5/5; 67/100] START bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8129010091300776, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8129010091300776\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9323611881275267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9323611881275267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5068359824134987, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5068359824134987\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9997176732861306, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9997176732861306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=72, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=72\n",
      "[CV 5/5; 60/100] END bagging_fraction=0.5068359824134987, bagging_freq=7, feature_fraction=0.9323611881275267, lambda_l1=0.8129010091300776, lambda_l2=0.9997176732861306, learning_rate=0.09968049952202102, max_bin=252, max_depth=9, min_data_in_leaf=72, num_leaves=44;, score=0.909 total time=  22.1s\n",
      "[CV 3/5; 62/100] START bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6083\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[CV 3/5; 62/100] END bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12;, score=0.909 total time=  10.0s\n",
      "[CV 4/5; 63/100] START bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5462\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 4/5; 63/100] END bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58;, score=0.909 total time=  14.0s\n",
      "[CV 2/5; 65/100] START bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5511\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 2/5; 65/100] END bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91;, score=0.909 total time=  45.6s\n",
      "[CV 4/5; 67/100] START bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6871\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6306\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[CV 4/5; 64/100] END bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45;, score=0.909 total time=  20.7s\n",
      "[CV 3/5; 66/100] START bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6506\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[CV 3/5; 66/100] END bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30;, score=0.909 total time=  22.7s\n",
      "[CV 1/5; 67/100] START bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6859\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 1/5; 67/100] END bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39;, score=0.909 total time=  23.5s\n",
      "[CV 4/5; 68/100] START bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5578\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 4/5; 68/100] END bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12;, score=0.908 total time=  14.3s\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6515\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[CV 1/5; 66/100] END bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30;, score=0.909 total time=  22.5s\n",
      "[CV 5/5; 66/100] START bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.401251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6501\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[CV 5/5; 66/100] END bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30;, score=0.909 total time=  23.9s\n",
      "[CV 3/5; 68/100] START bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5578\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 3/5; 68/100] END bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12;, score=0.908 total time=  14.3s\n",
      "[CV 1/5; 70/100] START bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6580\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[CV 1/5; 70/100] END bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35;, score=0.908 total time=  28.4s\n",
      "[CV 4/5; 71/100] START bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6077\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49816518664589293, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49816518664589293\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5110923710151508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5110923710151508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5234482338956099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5234482338956099\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4762106967890142, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4762106967890142\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[CV 5/5; 62/100] END bagging_fraction=0.5234482338956099, bagging_freq=8, feature_fraction=0.5110923710151508, lambda_l1=0.49816518664589293, lambda_l2=0.4762106967890142, learning_rate=0.08398029155593763, max_bin=249, max_depth=3, min_data_in_leaf=73, num_leaves=12;, score=0.909 total time=  11.0s\n",
      "[CV 2/5; 64/100] START bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6307\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5899176868546331, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5899176868546331\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[CV 2/5; 64/100] END bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45;, score=0.909 total time=  20.0s\n",
      "[CV 5/5; 65/100] START bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5508\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 5/5; 65/100] END bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91;, score=0.909 total time=  42.0s\n",
      "[CV 1/5; 68/100] START bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5583\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 1/5; 68/100] END bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12;, score=0.907 total time=  14.9s\n",
      "[CV 4/5; 69/100] START bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6033\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[CV 4/5; 69/100] END bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70;, score=0.906 total time=  35.9s\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.48004596701847513, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.48004596701847513\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.794977948568818, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.794977948568818\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5442041116828233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5442041116828233\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4205357786650751, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4205357786650751\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=94\n",
      "[CV 5/5; 63/100] END bagging_fraction=0.5442041116828233, bagging_freq=9, feature_fraction=0.794977948568818, lambda_l1=0.48004596701847513, lambda_l2=0.4205357786650751, learning_rate=0.07954348507470942, max_bin=213, max_depth=4, min_data_in_leaf=94, num_leaves=58;, score=0.909 total time=  13.1s\n",
      "[CV 3/5; 65/100] START bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5514\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6976418714934634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6976418714934634\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7618011380075056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7618011380075056\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7254552237080523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7254552237080523\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7964717756738773, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7964717756738773\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 3/5; 65/100] END bagging_fraction=0.7254552237080523, bagging_freq=7, feature_fraction=0.7618011380075056, lambda_l1=0.6976418714934634, lambda_l2=0.7964717756738773, learning_rate=0.04863794675488638, max_bin=216, max_depth=12, min_data_in_leaf=36, num_leaves=91;, score=0.910 total time=  41.1s\n",
      "[CV 2/5; 67/100] START bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6855\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 2/5; 67/100] END bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39;, score=0.909 total time=  22.2s\n",
      "[CV 5/5; 68/100] START bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5574\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 5/5; 68/100] END bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12;, score=0.908 total time=  14.4s\n",
      "[CV 3/5; 70/100] START bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6572\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6752937794032985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6752937794032985\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9515755293383131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9515755293383131\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3922440450997323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3922440450997323\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=71\n",
      "[CV 5/5; 64/100] END bagging_fraction=0.9515755293383131, bagging_freq=2, feature_fraction=0.6752937794032985, lambda_l1=0.5899176868546331, lambda_l2=0.3922440450997323, learning_rate=0.04656011759225426, max_bin=262, max_depth=4, min_data_in_leaf=71, num_leaves=45;, score=0.909 total time=  20.4s\n",
      "[CV 2/5; 66/100] START bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6504\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[CV 2/5; 66/100] END bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30;, score=0.909 total time=  25.7s\n",
      "[CV 3/5; 67/100] START bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.281824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6860\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 3/5; 67/100] END bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39;, score=0.909 total time=  23.5s\n",
      "[CV 2/5; 69/100] START bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6028\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[CV 2/5; 69/100] END bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70;, score=0.906 total time=  38.0s\n",
      "[CV 2/5; 71/100] START bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5262\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6851\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 5/5; 67/100] END bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39;, score=0.909 total time=  19.4s\n",
      "[CV 3/5; 69/100] START bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6026\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[CV 3/5; 69/100] END bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70;, score=0.906 total time=  36.1s\n",
      "[CV 1/5; 71/100] START bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5266\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[CV 1/5; 71/100] END bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25;, score=0.907 total time=  23.7s\n",
      "[CV 2/5; 72/100] START bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5892\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 2/5; 72/100] END bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51;, score=0.909 total time=  35.7s\n",
      "[CV 2/5; 75/100] START bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6510\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3474134082254142, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3474134082254142\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8104028422110456, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8104028422110456\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7295678781191306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7295678781191306\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20913079107851418, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20913079107851418\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[CV 4/5; 66/100] END bagging_fraction=0.7295678781191306, bagging_freq=2, feature_fraction=0.8104028422110456, lambda_l1=0.3474134082254142, lambda_l2=0.20913079107851418, learning_rate=0.06006674377481515, max_bin=274, max_depth=5, min_data_in_leaf=130, num_leaves=30;, score=0.909 total time=  23.3s\n",
      "[CV 2/5; 68/100] START bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5575\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.16893506307216455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.16893506307216455\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204343081332394, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204343081332394\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5861599356008149, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5861599356008149\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2785903390319586, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2785903390319586\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 2/5; 68/100] END bagging_fraction=0.5861599356008149, bagging_freq=3, feature_fraction=0.5204343081332394, lambda_l1=0.16893506307216455, lambda_l2=0.2785903390319586, learning_rate=0.021815996006290948, max_bin=220, max_depth=8, min_data_in_leaf=20, num_leaves=12;, score=0.908 total time=  15.4s\n",
      "[CV 5/5; 69/100] START bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6023\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[CV 5/5; 69/100] END bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70;, score=0.906 total time=  33.3s\n",
      "[CV 1/5; 72/100] START bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5892\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 1/5; 72/100] END bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51;, score=0.908 total time=  31.3s\n",
      "[CV 4/5; 73/100] START bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5276\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[CV 4/5; 71/100] END bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25;, score=0.908 total time=  27.1s\n",
      "[CV 1/5; 73/100] START bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6924\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[CV 1/5; 73/100] END bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15;, score=0.909 total time=  19.1s\n",
      "[CV 5/5; 73/100] START bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6927\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[CV 5/5; 73/100] END bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15;, score=0.909 total time=  19.0s\n",
      "[CV 3/5; 75/100] START bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6826\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 3/5; 75/100] END bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16;, score=0.909 total time=  17.1s\n",
      "[CV 1/5; 76/100] START bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6666\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[CV 2/5; 70/100] START bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6571\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[CV 2/5; 70/100] END bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35;, score=0.908 total time=  25.9s\n",
      "[CV 4/5; 70/100] START bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6578\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[CV 4/5; 70/100] END bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35;, score=0.909 total time=  28.5s\n",
      "[CV 3/5; 73/100] START bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6931\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[CV 3/5; 73/100] END bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15;, score=0.909 total time=  18.8s\n",
      "[CV 1/5; 74/100] START bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6244\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[CV 1/5; 74/100] END bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77;, score=0.909 total time=  38.0s\n",
      "[CV 2/5; 76/100] START bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6656\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.18006727234929853, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.18006727234929853\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8488337868484659, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8488337868484659\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5640229194788862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5640229194788862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.696501466227079, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.696501466227079\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 4/5; 67/100] END bagging_fraction=0.5640229194788862, bagging_freq=6, feature_fraction=0.8488337868484659, lambda_l1=0.18006727234929853, lambda_l2=0.696501466227079, learning_rate=0.04410781536396517, max_bin=295, max_depth=5, min_data_in_leaf=174, num_leaves=39;, score=0.909 total time=  21.5s\n",
      "[CV 1/5; 69/100] START bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6036\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3300995133101554, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3300995133101554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643741991498219, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643741991498219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7303893840163629, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7303893840163629\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.321582764680029, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.321582764680029\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[CV 1/5; 69/100] END bagging_fraction=0.7303893840163629, bagging_freq=2, feature_fraction=0.5643741991498219, lambda_l1=0.3300995133101554, lambda_l2=0.321582764680029, learning_rate=0.01376760569173088, max_bin=246, max_depth=11, min_data_in_leaf=75, num_leaves=70;, score=0.906 total time=  38.4s\n",
      "[CV 5/5; 70/100] START bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.279961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6571\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8062012797930613, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8062012797930613\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[CV 5/5; 70/100] END bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35;, score=0.908 total time=  28.3s\n",
      "[CV 2/5; 73/100] START bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6929\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[CV 2/5; 73/100] END bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15;, score=0.909 total time=  19.2s\n",
      "[CV 2/5; 74/100] START bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6237\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6384388240736019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6384388240736019\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6886429826210363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6886429826210363\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7482596903836584, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7482596903836584\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[CV 3/5; 70/100] END bagging_fraction=0.6886429826210363, bagging_freq=1, feature_fraction=0.6384388240736019, lambda_l1=0.8062012797930613, lambda_l2=0.7482596903836584, learning_rate=0.022529496838855845, max_bin=278, max_depth=10, min_data_in_leaf=33, num_leaves=35;, score=0.909 total time=  25.9s\n",
      "[CV 5/5; 71/100] START bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5256\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[CV 5/5; 71/100] END bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25;, score=0.908 total time=  23.4s\n",
      "[CV 5/5; 72/100] START bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5883\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 5/5; 72/100] END bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51;, score=0.909 total time=  33.5s\n",
      "[CV 1/5; 75/100] START bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6827\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 1/5; 75/100] END bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16;, score=0.909 total time=  16.4s\n",
      "[CV 4/5; 75/100] START bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6833\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 4/5; 75/100] END bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16;, score=0.909 total time=  15.4s\n",
      "[CV 4/5; 76/100] START bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV 3/5; 71/100] START bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5274\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06234136251124145, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06234136251124145\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[CV 3/5; 71/100] END bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25;, score=0.908 total time=  23.6s\n",
      "[CV 4/5; 72/100] START bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5889\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 4/5; 72/100] END bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51;, score=0.909 total time=  31.9s\n",
      "[CV 5/5; 74/100] START bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6231\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[CV 5/5; 74/100] END bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77;, score=0.909 total time=  38.4s\n",
      "[CV 2/5; 77/100] START bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5661\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[CV 2/5; 77/100] END bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51;, score=0.909 total time=  22.0s\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8602580306957324, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8602580306957324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6844568197848862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6844568197848862\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1477390919062267, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1477390919062267\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[CV 2/5; 71/100] END bagging_fraction=0.6844568197848862, bagging_freq=8, feature_fraction=0.8602580306957324, lambda_l1=0.06234136251124145, lambda_l2=0.1477390919062267, learning_rate=0.017646108215810436, max_bin=202, max_depth=9, min_data_in_leaf=126, num_leaves=25;, score=0.908 total time=  23.6s\n",
      "[CV 3/5; 72/100] START bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5888\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1048691665324809, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1048691665324809\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6985819162520412, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6985819162520412\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7237061834117273, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7237061834117273\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7374052060938191, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7374052060938191\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[CV 3/5; 72/100] END bagging_fraction=0.7237061834117273, bagging_freq=3, feature_fraction=0.6985819162520412, lambda_l1=0.1048691665324809, lambda_l2=0.7374052060938191, learning_rate=0.022316968455216017, max_bin=238, max_depth=11, min_data_in_leaf=36, num_leaves=51;, score=0.909 total time=  31.4s\n",
      "[CV 4/5; 74/100] START bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6240\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[CV 4/5; 74/100] END bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77;, score=0.910 total time=  37.6s\n",
      "[CV 1/5; 77/100] START bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5670\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[CV 1/5; 77/100] END bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51;, score=0.909 total time=  22.4s\n",
      "[CV 4/5; 78/100] START bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5242\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6824\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 2/5; 75/100] END bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16;, score=0.909 total time=  18.3s\n",
      "[CV 5/5; 75/100] START bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6818\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22533279771252823, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22533279771252823\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5037671815670374, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5037671815670374\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8028874096784435, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8028874096784435\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.36535681967405764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.36535681967405764\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 5/5; 75/100] END bagging_fraction=0.8028874096784435, bagging_freq=1, feature_fraction=0.5037671815670374, lambda_l1=0.22533279771252823, lambda_l2=0.36535681967405764, learning_rate=0.0513419310768301, max_bin=293, max_depth=10, min_data_in_leaf=74, num_leaves=16;, score=0.909 total time=  18.3s\n",
      "[CV 3/5; 77/100] START bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5664\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[CV 3/5; 77/100] END bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51;, score=0.909 total time=  21.0s\n",
      "[CV 1/5; 79/100] START bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5312\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[CV 1/5; 79/100] END bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51;, score=0.909 total time=  26.4s\n",
      "[CV 1/5; 80/100] START bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[CV 2/5; 76/100] END bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13;, score=0.909 total time=  16.7s\n",
      "[CV 5/5; 77/100] START bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5659\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[CV 5/5; 77/100] END bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51;, score=0.909 total time=  24.4s\n",
      "[CV 3/5; 79/100] START bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5309\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[CV 3/5; 79/100] END bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51;, score=0.909 total time=  26.5s\n",
      "[CV 1/5; 81/100] START bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6515\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[CV 1/5; 81/100] END bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83;, score=0.909 total time=  30.0s\n",
      "[CV 4/5; 82/100] START bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Total Bins 6940\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35510430327930476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35510430327930476\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.736984970715278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.736984970715278\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6166082057749006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6166082057749006\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.648822840291576, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.648822840291576\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=53\n",
      "[CV 4/5; 73/100] END bagging_fraction=0.6166082057749006, bagging_freq=4, feature_fraction=0.736984970715278, lambda_l1=0.35510430327930476, lambda_l2=0.648822840291576, learning_rate=0.05056029966665377, max_bin=299, max_depth=8, min_data_in_leaf=53, num_leaves=15;, score=0.909 total time=  19.0s\n",
      "[CV 3/5; 74/100] START bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6234\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3374468338719181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3374468338719181\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[CV 3/5; 74/100] END bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77;, score=0.910 total time=  38.2s\n",
      "[CV 5/5; 76/100] START bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6657\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[CV 5/5; 76/100] END bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13;, score=0.909 total time=  16.0s\n",
      "[CV 3/5; 78/100] START bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5227\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 3/5; 78/100] END bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71;, score=0.910 total time=  34.6s\n",
      "[CV 2/5; 80/100] START bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.262381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6504\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[CV 2/5; 80/100] END bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42;, score=0.909 total time=  27.0s\n",
      "[CV 5/5; 81/100] START bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[CV 1/5; 76/100] END bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13;, score=0.909 total time=  16.1s\n",
      "[CV 4/5; 77/100] START bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5665\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8208614731533526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8208614731533526\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8614480483108878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8614480483108878\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9875335814725712, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9875335814725712\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7184572708365536, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7184572708365536\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=192, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=192\n",
      "[CV 4/5; 77/100] END bagging_fraction=0.9875335814725712, bagging_freq=8, feature_fraction=0.8614480483108878, lambda_l1=0.8208614731533526, lambda_l2=0.7184572708365536, learning_rate=0.0558284895442665, max_bin=225, max_depth=5, min_data_in_leaf=192, num_leaves=51;, score=0.909 total time=  21.8s\n",
      "[CV 2/5; 79/100] START bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5307\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[CV 2/5; 79/100] END bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51;, score=0.909 total time=  26.5s\n",
      "[CV 3/5; 80/100] START bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6506\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[CV 3/5; 80/100] END bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42;, score=0.910 total time=  27.8s\n",
      "[CV 1/5; 82/100] START bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5464\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6893863129756447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6893863129756447\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7932677173303795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7932677173303795\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8996473971450123, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8996473971450123\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=78\n",
      "[CV 2/5; 74/100] END bagging_fraction=0.7932677173303795, bagging_freq=3, feature_fraction=0.6893863129756447, lambda_l1=0.3374468338719181, lambda_l2=0.8996473971450123, learning_rate=0.06271774611788943, max_bin=258, max_depth=10, min_data_in_leaf=78, num_leaves=77;, score=0.909 total time=  41.3s\n",
      "[CV 3/5; 76/100] START bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6655\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[CV 3/5; 76/100] END bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13;, score=0.910 total time=  15.4s\n",
      "[CV 1/5; 78/100] START bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5233\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 1/5; 78/100] END bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71;, score=0.909 total time=  30.4s\n",
      "[CV 4/5; 79/100] START bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5309\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[CV 4/5; 79/100] END bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51;, score=0.909 total time=  29.4s\n",
      "[CV 2/5; 81/100] START bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6504\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[CV 2/5; 81/100] END bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83;, score=0.909 total time=  30.5s\n",
      "[CV 3/5; 83/100] START bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[CV 5/5; 78/100] START bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.313044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5221\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 5/5; 78/100] END bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71;, score=0.909 total time=  35.2s\n",
      "[CV 5/5; 80/100] START bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6501\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[CV 5/5; 80/100] END bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42;, score=0.909 total time=  25.5s\n",
      "[CV 3/5; 82/100] START bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5463\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[CV 3/5; 82/100] END bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12;, score=0.908 total time=  14.5s\n",
      "[CV 1/5; 83/100] START bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5464\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[CV 1/5; 83/100] END bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45;, score=0.909 total time=  20.6s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 4/5; 78/100] END bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71;, score=0.910 total time=  34.9s\n",
      "[CV 4/5; 80/100] START bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6510\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[CV 4/5; 80/100] END bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42;, score=0.910 total time=  26.7s\n",
      "[CV 2/5; 82/100] START bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5460\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[CV 2/5; 82/100] END bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12;, score=0.907 total time=  14.7s\n",
      "[CV 2/5; 83/100] START bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5460\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[CV 2/5; 83/100] END bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45;, score=0.909 total time=  19.7s\n",
      "[CV 4/5; 84/100] START bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6104\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6665\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6969614623664215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6969614623664215\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5091212407141805, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5091212407141805\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5401168728308211, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5401168728308211\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9972555347683943, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9972555347683943\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[CV 4/5; 76/100] END bagging_fraction=0.5401168728308211, bagging_freq=3, feature_fraction=0.5091212407141805, lambda_l1=0.6969614623664215, lambda_l2=0.9972555347683943, learning_rate=0.09017797498253198, max_bin=283, max_depth=6, min_data_in_leaf=98, num_leaves=13;, score=0.909 total time=  15.3s\n",
      "[CV 2/5; 78/100] START bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5228\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.19950692733831976, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.19950692733831976\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8554762394183741, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8554762394183741\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6025387800747295, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6025387800747295\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7362471836984882, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7362471836984882\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=161, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=161\n",
      "[CV 2/5; 78/100] END bagging_fraction=0.6025387800747295, bagging_freq=6, feature_fraction=0.8554762394183741, lambda_l1=0.19950692733831976, lambda_l2=0.7362471836984882, learning_rate=0.05533480220220608, max_bin=200, max_depth=12, min_data_in_leaf=161, num_leaves=71;, score=0.909 total time=  36.4s\n",
      "[CV 5/5; 79/100] START bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5302\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5938830975578988, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5938830975578988\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6603210960797572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6603210960797572\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5436450897948377, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5436450897948377\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3692304815098961, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3692304815098961\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=157, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=157\n",
      "[CV 5/5; 79/100] END bagging_fraction=0.5436450897948377, bagging_freq=4, feature_fraction=0.6603210960797572, lambda_l1=0.5938830975578988, lambda_l2=0.3692304815098961, learning_rate=0.04815545736081447, max_bin=204, max_depth=9, min_data_in_leaf=157, num_leaves=51;, score=0.909 total time=  29.1s\n",
      "[CV 3/5; 81/100] START bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6506\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[CV 3/5; 81/100] END bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83;, score=0.910 total time=  30.9s\n",
      "[CV 2/5; 84/100] START bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6098\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[CV 2/5; 84/100] END bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77;, score=0.909 total time=  35.1s\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6501\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[CV 5/5; 81/100] END bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83;, score=0.909 total time=  30.4s\n",
      "[CV 5/5; 83/100] START bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5457\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[CV 5/5; 83/100] END bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45;, score=0.909 total time=  18.2s\n",
      "[CV 3/5; 85/100] START bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5786\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 3/5; 85/100] END bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43;, score=0.910 total time=  25.8s\n",
      "[CV 5/5; 86/100] START bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6689\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[CV 5/5; 86/100] END bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12;, score=0.902 total time=  18.7s\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6515\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.25464065476376385, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.25464065476376385\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9049372229273174, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049372229273174\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6008652311651179, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6008652311651179\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6815027222239293, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6815027222239293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=57\n",
      "[CV 1/5; 80/100] END bagging_fraction=0.6008652311651179, bagging_freq=8, feature_fraction=0.9049372229273174, lambda_l1=0.25464065476376385, lambda_l2=0.6815027222239293, learning_rate=0.07722164668952022, max_bin=274, max_depth=10, min_data_in_leaf=57, num_leaves=42;, score=0.909 total time=  27.0s\n",
      "[CV 4/5; 81/100] START bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6510\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7459745255487181, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7459745255487181\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9253637300082274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9253637300082274\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6744341332714976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6744341332714976\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4085182929359169, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4085182929359169\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=181, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=181\n",
      "[CV 4/5; 81/100] END bagging_fraction=0.6744341332714976, bagging_freq=1, feature_fraction=0.9253637300082274, lambda_l1=0.7459745255487181, lambda_l2=0.4085182929359169, learning_rate=0.0936291163744957, max_bin=274, max_depth=11, min_data_in_leaf=181, num_leaves=83;, score=0.910 total time=  30.6s\n",
      "[CV 1/5; 84/100] START bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6106\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[CV 1/5; 84/100] END bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77;, score=0.909 total time=  34.6s\n",
      "[CV 4/5; 85/100] START bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5785\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 4/5; 85/100] END bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43;, score=0.910 total time=  25.4s\n",
      "[CV 5/5; 87/100] START bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6109\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5462\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[CV 4/5; 82/100] END bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12;, score=0.907 total time=  14.3s\n",
      "[CV 4/5; 83/100] START bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5462\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[CV 4/5; 83/100] END bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45;, score=0.909 total time=  18.3s\n",
      "[CV 2/5; 85/100] START bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5786\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 2/5; 85/100] END bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43;, score=0.909 total time=  30.8s\n",
      "[CV 1/5; 87/100] START bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6123\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[CV 1/5; 87/100] END bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36;, score=0.909 total time=  26.5s\n",
      "[CV 4/5; 88/100] START bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[CV 1/5; 82/100] END bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12;, score=0.907 total time=  14.9s\n",
      "[CV 5/5; 82/100] START bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5457\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5744731131799119, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5744731131799119\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8705603246450295, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8705603246450295\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6896142924819002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6896142924819002\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.8418287767582721, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8418287767582721\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=184, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=184\n",
      "[CV 5/5; 82/100] END bagging_fraction=0.6896142924819002, bagging_freq=1, feature_fraction=0.8705603246450295, lambda_l1=0.5744731131799119, lambda_l2=0.8418287767582721, learning_rate=0.0182783757794975, max_bin=213, max_depth=5, min_data_in_leaf=184, num_leaves=12;, score=0.907 total time=  19.1s\n",
      "[CV 3/5; 84/100] START bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6099\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[CV 3/5; 84/100] END bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77;, score=0.910 total time=  30.7s\n",
      "[CV 2/5; 86/100] START bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6689\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[CV 2/5; 86/100] END bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12;, score=0.902 total time=  19.6s\n",
      "[CV 2/5; 87/100] START bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6115\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[CV 2/5; 87/100] END bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36;, score=0.909 total time=  28.5s\n",
      "[CV 3/5; 89/100] START bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5463\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5883066757950567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5883066757950567\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6580982911482474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6580982911482474\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6178458730324998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6178458730324998\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6829820634171324, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6829820634171324\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=144\n",
      "[CV 3/5; 83/100] END bagging_fraction=0.6178458730324998, bagging_freq=4, feature_fraction=0.6580982911482474, lambda_l1=0.5883066757950567, lambda_l2=0.6829820634171324, learning_rate=0.04793511712340873, max_bin=213, max_depth=5, min_data_in_leaf=144, num_leaves=45;, score=0.909 total time=  18.9s\n",
      "[CV 1/5; 85/100] START bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5787\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 1/5; 85/100] END bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43;, score=0.909 total time=  30.7s\n",
      "[CV 4/5; 86/100] START bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6698\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[CV 4/5; 86/100] END bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12;, score=0.902 total time=  18.9s\n",
      "[CV 1/5; 88/100] START bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5841\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[CV 1/5; 88/100] END bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34;, score=0.909 total time=  32.7s\n",
      "[CV 4/5; 89/100] START bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[CV 4/5; 84/100] END bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77;, score=0.910 total time=  28.3s\n",
      "[CV 1/5; 86/100] START bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6699\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[CV 1/5; 86/100] END bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12;, score=0.902 total time=  20.4s\n",
      "[CV 4/5; 87/100] START bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6120\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[CV 4/5; 87/100] END bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36;, score=0.909 total time=  22.6s\n",
      "[CV 5/5; 88/100] START bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.350280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5831\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[CV 5/5; 88/100] END bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34;, score=0.909 total time=  29.3s\n",
      "[CV 3/5; 90/100] START bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6386\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 3/5; 90/100] END bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44;, score=0.909 total time=  21.8s\n",
      "[CV 1/5; 92/100] START bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[CV 5/5; 84/100] START bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6093\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8995745732745685, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8995745732745685\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6161063475734087, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6161063475734087\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698905309573165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698905309573165\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3838912213732114, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3838912213732114\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=198, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=198\n",
      "[CV 5/5; 84/100] END bagging_fraction=0.7698905309573165, bagging_freq=1, feature_fraction=0.6161063475734087, lambda_l1=0.8995745732745685, lambda_l2=0.3838912213732114, learning_rate=0.056637521805828914, max_bin=250, max_depth=9, min_data_in_leaf=198, num_leaves=77;, score=0.909 total time=  28.9s\n",
      "[CV 3/5; 86/100] START bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6690\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.24848913981446574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.24848913981446574\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6558610338977742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6558610338977742\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9988463079619216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9988463079619216\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.743946292572677, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.743946292572677\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[CV 3/5; 86/100] END bagging_fraction=0.9988463079619216, bagging_freq=9, feature_fraction=0.6558610338977742, lambda_l1=0.24848913981446574, lambda_l2=0.743946292572677, learning_rate=0.00818558129989904, max_bin=285, max_depth=4, min_data_in_leaf=29, num_leaves=12;, score=0.902 total time=  19.2s\n",
      "[CV 3/5; 87/100] START bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.202983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6115\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5364813514845947, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5364813514845947\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[CV 3/5; 87/100] END bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36;, score=0.909 total time=  26.5s\n",
      "[CV 1/5; 89/100] START bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5431\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[CV 1/5; 89/100] END bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69;, score=0.909 total time=  33.7s\n",
      "[CV 2/5; 91/100] START bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5909\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 88/100] START bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5841\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[CV 2/5; 88/100] END bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34;, score=0.909 total time=  32.2s\n",
      "[CV 5/5; 89/100] START bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5422\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[CV 5/5; 89/100] END bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69;, score=0.909 total time=  29.8s\n",
      "[CV 5/5; 91/100] START bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5904\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[CV 5/5; 91/100] END bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30;, score=0.909 total time=  20.1s\n",
      "[CV 2/5; 93/100] START bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5824\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5839\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[CV 4/5; 88/100] END bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34;, score=0.909 total time=  27.5s\n",
      "[CV 2/5; 90/100] START bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6391\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 2/5; 90/100] END bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44;, score=0.909 total time=  21.8s\n",
      "[CV 3/5; 91/100] START bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.361660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5908\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[CV 3/5; 91/100] END bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30;, score=0.910 total time=  25.2s\n",
      "[CV 3/5; 93/100] START bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5823\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5514580153720725, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5514580153720725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9383828183808747, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9383828183808747\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3788215655569157, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3788215655569157\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[CV 5/5; 87/100] END bagging_fraction=0.9383828183808747, bagging_freq=1, feature_fraction=0.5514580153720725, lambda_l1=0.5364813514845947, lambda_l2=0.3788215655569157, learning_rate=0.04841502083175944, max_bin=251, max_depth=10, min_data_in_leaf=34, num_leaves=36;, score=0.909 total time=  26.6s\n",
      "[CV 2/5; 89/100] START bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5427\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[CV 2/5; 89/100] END bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69;, score=0.909 total time=  28.4s\n",
      "[CV 4/5; 90/100] START bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6391\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 4/5; 90/100] END bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44;, score=0.909 total time=  22.3s\n",
      "[CV 2/5; 92/100] START bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6307\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 2/5; 92/100] END bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47;, score=0.909 total time=  24.4s\n",
      "[CV 5/5; 93/100] START bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5814\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 85/100] START bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5780\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6937176074033384, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6937176074033384\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8652745684046508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8652745684046508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699160618067375, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699160618067375\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.16673075999338316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16673075999338316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=174, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=174\n",
      "[CV 5/5; 85/100] END bagging_fraction=0.9699160618067375, bagging_freq=7, feature_fraction=0.8652745684046508, lambda_l1=0.6937176074033384, lambda_l2=0.16673075999338316, learning_rate=0.08846976596047663, max_bin=232, max_depth=6, min_data_in_leaf=174, num_leaves=43;, score=0.909 total time=  30.1s\n",
      "[CV 3/5; 88/100] START bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5839\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7200365365460744, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7200365365460744\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8611262841965331, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8611262841965331\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7699303185564184, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7699303185564184\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6411476328852973, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6411476328852973\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=62, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=62\n",
      "[CV 3/5; 88/100] END bagging_fraction=0.7699303185564184, bagging_freq=4, feature_fraction=0.8611262841965331, lambda_l1=0.7200365365460744, lambda_l2=0.6411476328852973, learning_rate=0.07092510222437451, max_bin=235, max_depth=12, min_data_in_leaf=62, num_leaves=34;, score=0.910 total time=  30.8s\n",
      "[CV 1/5; 90/100] START bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6396\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 1/5; 90/100] END bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44;, score=0.909 total time=  21.9s\n",
      "[CV 5/5; 90/100] START bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6381\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8721239089441515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8721239089441515\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5090550919104202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5090550919104202\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5503973015079433, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5503973015079433\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.9321182824836124, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.9321182824836124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[CV 5/5; 90/100] END bagging_fraction=0.5503973015079433, bagging_freq=1, feature_fraction=0.5090550919104202, lambda_l1=0.8721239089441515, lambda_l2=0.9321182824836124, learning_rate=0.05868765244097485, max_bin=267, max_depth=7, min_data_in_leaf=139, num_leaves=44;, score=0.909 total time=  19.5s\n",
      "[CV 3/5; 92/100] START bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.210010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5428\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[CV 3/5; 89/100] END bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69;, score=0.909 total time=  31.2s\n",
      "[CV 1/5; 91/100] START bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5912\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[CV 1/5; 91/100] END bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30;, score=0.909 total time=  26.3s\n",
      "[CV 1/5; 93/100] START bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5821\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[CV 1/5; 93/100] END bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82;, score=0.909 total time=  24.8s\n",
      "[CV 4/5; 94/100] START bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6104\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(8366) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Info] Number of positive: 116035, number of negative: 1070417\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5965\n",
      "[LightGBM] [Info] Number of data points in the train set: 1186452, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "En iyi parametreler: {'bagging_fraction': 0.9701151207124787, 'bagging_freq': 8, 'feature_fraction': 0.8385841711914909, 'lambda_l1': 0.5733670416719333, 'lambda_l2': 0.12850035323391018, 'learning_rate': 0.0820643967899203, 'max_bin': 242, 'max_depth': 7, 'min_data_in_leaf': 31, 'num_leaves': 74}\n",
      "En iyi skor: 0.9094240647307581\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6314\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 1/5; 92/100] END bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47;, score=0.909 total time=  24.2s\n",
      "[CV 4/5; 93/100] START bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5822\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[CV 4/5; 93/100] END bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82;, score=0.910 total time=  28.7s\n",
      "[CV 2/5; 95/100] START bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5661\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 2/5; 95/100] END bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78;, score=0.909 total time=  16.2s\n",
      "[CV 4/5; 96/100] START bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6068\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 4/5; 96/100] END bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20;, score=0.910 total time=  22.4s\n",
      "[CV 2/5; 98/100] START bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.193926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5876\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 2/5; 98/100] END bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11;, score=0.909 total time=  17.1s\n",
      "[CV 3/5; 99/100] START bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6875\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 3/5; 99/100] END bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39;, score=0.909 total time=  25.2s\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[CV 2/5; 93/100] END bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82;, score=0.909 total time=  29.1s\n",
      "[CV 5/5; 94/100] START bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6093\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[CV 5/5; 94/100] END bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32;, score=0.909 total time=  29.3s\n",
      "[CV 5/5; 96/100] START bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6060\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 5/5; 96/100] END bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20;, score=0.909 total time=  21.9s\n",
      "[CV 3/5; 98/100] START bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5872\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 3/5; 98/100] END bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11;, score=0.909 total time=  16.5s\n",
      "[CV 4/5; 99/100] START bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6888\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 4/5; 99/100] END bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39;, score=0.910 total time=  28.0s\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6301\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 3/5; 92/100] END bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47;, score=0.910 total time=  27.6s\n",
      "[CV 2/5; 94/100] START bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6098\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[CV 2/5; 94/100] END bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32;, score=0.909 total time=  30.6s\n",
      "[CV 1/5; 96/100] START bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6072\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 1/5; 96/100] END bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20;, score=0.909 total time=  23.0s\n",
      "[CV 4/5; 97/100] START bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5995\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[CV 4/5; 97/100] END bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25;, score=0.909 total time=  22.9s\n",
      "[CV 5/5; 99/100] START bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.206192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6875\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 5/5; 99/100] END bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39;, score=0.909 total time=  28.2s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5427\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4008514167636399, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4008514167636399\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7916958973830602, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7916958973830602\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6728479967519597, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6728479967519597\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.4620058036441327, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4620058036441327\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "[CV 4/5; 89/100] END bagging_fraction=0.6728479967519597, bagging_freq=8, feature_fraction=0.7916958973830602, lambda_l1=0.4008514167636399, lambda_l2=0.4620058036441327, learning_rate=0.09499191726312246, max_bin=211, max_depth=12, min_data_in_leaf=160, num_leaves=69;, score=0.910 total time=  28.2s\n",
      "[CV 4/5; 91/100] START bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5908\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[CV 4/5; 91/100] END bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30;, score=0.910 total time=  19.8s\n",
      "[CV 5/5; 92/100] START bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6297\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 5/5; 92/100] END bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47;, score=0.909 total time=  24.8s\n",
      "[CV 3/5; 94/100] START bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6099\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[CV 3/5; 94/100] END bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32;, score=0.909 total time=  29.8s\n",
      "[CV 2/5; 96/100] START bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6063\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 2/5; 96/100] END bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20;, score=0.909 total time=  23.2s\n",
      "[CV 5/5; 97/100] START bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5990\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[CV 5/5; 97/100] END bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25;, score=0.908 total time=  22.5s\n",
      "[CV 1/5; 100/100] START bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6465\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[CV 1/5; 100/100] END bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58;, score=0.909 total time=  27.2s\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2055798083756375, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2055798083756375\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[CV 5/5; 93/100] END bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82;, score=0.909 total time=  26.2s\n",
      "[CV 3/5; 95/100] START bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.255300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5664\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 3/5; 95/100] END bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78;, score=0.909 total time=  15.8s\n",
      "[CV 1/5; 97/100] START bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5999\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[CV 1/5; 97/100] END bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25;, score=0.908 total time=  24.6s\n",
      "[CV 4/5; 98/100] START bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5873\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 4/5; 98/100] END bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11;, score=0.909 total time=  16.4s\n",
      "[CV 2/5; 100/100] START bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6458\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[CV 2/5; 100/100] END bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58;, score=0.909 total time=  26.6s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8721248259216444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8721248259216444\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8039471968971293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8039471968971293\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7877898887144924, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7877898887144924\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[CV 3/5; 93/100] END bagging_fraction=0.8039471968971293, bagging_freq=9, feature_fraction=0.8721248259216444, lambda_l1=0.2055798083756375, lambda_l2=0.7877898887144924, learning_rate=0.0623535455143632, max_bin=234, max_depth=6, min_data_in_leaf=44, num_leaves=82;, score=0.909 total time=  28.6s\n",
      "[CV 1/5; 95/100] START bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5670\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 1/5; 95/100] END bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78;, score=0.908 total time=  13.9s\n",
      "[CV 4/5; 95/100] START bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5665\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 4/5; 95/100] END bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78;, score=0.909 total time=  15.7s\n",
      "[CV 2/5; 97/100] START bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5993\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[CV 2/5; 97/100] END bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25;, score=0.908 total time=  24.7s\n",
      "[CV 5/5; 98/100] START bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 5/5; 98/100] END bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11;, score=0.909 total time=  16.0s\n",
      "[CV 3/5; 100/100] START bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6453\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[CV 3/5; 100/100] END bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58;, score=0.910 total time=  27.1s\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.6067150463828559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6067150463828559\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7881441800834066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7881441800834066\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8536193171566993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8536193171566993\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.424130671302386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.424130671302386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[CV 2/5; 91/100] END bagging_fraction=0.8536193171566993, bagging_freq=1, feature_fraction=0.7881441800834066, lambda_l1=0.6067150463828559, lambda_l2=0.424130671302386, learning_rate=0.07496220238434867, max_bin=239, max_depth=11, min_data_in_leaf=30, num_leaves=30;, score=0.909 total time=  20.8s\n",
      "[CV 4/5; 92/100] START bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6306\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8388980864459341, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8388980864459341\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9924205994811672, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9924205994811672\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5614053909228669, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5614053909228669\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12466268120326685, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12466268120326685\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 4/5; 92/100] END bagging_fraction=0.5614053909228669, bagging_freq=5, feature_fraction=0.9924205994811672, lambda_l1=0.8388980864459341, lambda_l2=0.12466268120326685, learning_rate=0.09247997884865038, max_bin=262, max_depth=8, min_data_in_leaf=74, num_leaves=47;, score=0.910 total time=  24.7s\n",
      "[CV 1/5; 94/100] START bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6106\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7375012481097484, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7375012481097484\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6734766009481139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6734766009481139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9614801158992424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9614801158992424\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[CV 1/5; 94/100] END bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32;, score=0.908 total time=  27.4s\n",
      "[CV 5/5; 95/100] START bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5659\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.3114731711327574, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3114731711327574\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5169395801232974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5169395801232974\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7491838863697399, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7491838863697399\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.7804957075460877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7804957075460877\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[CV 5/5; 95/100] END bagging_fraction=0.7491838863697399, bagging_freq=6, feature_fraction=0.5169395801232974, lambda_l1=0.3114731711327574, lambda_l2=0.7804957075460877, learning_rate=0.03137078139979472, max_bin=225, max_depth=4, min_data_in_leaf=68, num_leaves=78;, score=0.909 total time=  16.0s\n",
      "[CV 3/5; 97/100] START bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5991\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.8980542894407137, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8980542894407137\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6049527965477929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6049527965477929\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.698659057728799, subsample=1.0 will be ignored. Current value: bagging_fraction=0.698659057728799\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.20513964048200717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.20513964048200717\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[CV 3/5; 97/100] END bagging_fraction=0.698659057728799, bagging_freq=3, feature_fraction=0.6049527965477929, lambda_l1=0.8980542894407137, lambda_l2=0.20513964048200717, learning_rate=0.023115333463048327, max_bin=244, max_depth=10, min_data_in_leaf=38, num_leaves=25;, score=0.909 total time=  21.8s\n",
      "[CV 1/5; 99/100] START bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6875\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 1/5; 99/100] END bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39;, score=0.909 total time=  28.8s\n",
      "[CV 4/5; 100/100] START bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6459\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[CV 4/5; 100/100] END bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58;, score=0.909 total time=  17.4s\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.45221794088980716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.45221794088980716\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=171, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=171\n",
      "[CV 4/5; 94/100] END bagging_fraction=0.9614801158992424, bagging_freq=7, feature_fraction=0.6734766009481139, lambda_l1=0.7375012481097484, lambda_l2=0.45221794088980716, learning_rate=0.02633745817929829, max_bin=250, max_depth=8, min_data_in_leaf=171, num_leaves=32;, score=0.909 total time=  29.6s\n",
      "[CV 3/5; 96/100] START bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6063\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.41462412372702373, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41462412372702373\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5743313638765565, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5743313638765565\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9877706578413781, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9877706578413781\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08534966807864386, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08534966807864386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[CV 3/5; 96/100] END bagging_fraction=0.9877706578413781, bagging_freq=9, feature_fraction=0.5743313638765565, lambda_l1=0.41462412372702373, lambda_l2=0.08534966807864386, learning_rate=0.099703053925365, max_bin=248, max_depth=9, min_data_in_leaf=20, num_leaves=20;, score=0.909 total time=  21.8s\n",
      "[CV 1/5; 98/100] START bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5874\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.52439026932758, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.52439026932758\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7266444173740139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7266444173740139\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6470223396104857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6470223396104857\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44076274693822814, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44076274693822814\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n",
      "[CV 1/5; 98/100] END bagging_fraction=0.6470223396104857, bagging_freq=3, feature_fraction=0.7266444173740139, lambda_l1=0.52439026932758, lambda_l2=0.44076274693822814, learning_rate=0.04307249078315597, max_bin=237, max_depth=5, min_data_in_leaf=82, num_leaves=11;, score=0.909 total time=  17.2s\n",
      "[CV 2/5; 99/100] START bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6875\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.37330931627975295, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.37330931627975295\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9730577310668164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9730577310668164\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5909640652476356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5909640652476356\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.27074467314355377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.27074467314355377\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[CV 2/5; 99/100] END bagging_fraction=0.5909640652476356, bagging_freq=7, feature_fraction=0.9730577310668164, lambda_l1=0.37330931627975295, lambda_l2=0.27074467314355377, learning_rate=0.0661799566077065, max_bin=296, max_depth=9, min_data_in_leaf=74, num_leaves=39;, score=0.909 total time=  25.1s\n",
      "[CV 5/5; 100/100] START bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6450\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.22197216193294944, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.22197216193294944\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5135479962517417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135479962517417\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8579861144236988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8579861144236988\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2310747965880714, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2310747965880714\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=86, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=86\n",
      "[CV 5/5; 100/100] END bagging_fraction=0.8579861144236988, bagging_freq=8, feature_fraction=0.5135479962517417, lambda_l1=0.22197216193294944, lambda_l2=0.2310747965880714, learning_rate=0.06882981064187921, max_bin=271, max_depth=8, min_data_in_leaf=86, num_leaves=58;, score=0.909 total time=  14.0s\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Parametre aralıklarını belirleme\n",
    "param_grid = {\n",
    "    'num_leaves': randint(8, 92),  # Yaprak sayısı\n",
    "    'max_depth': randint(3, 13),   # Maksimum derinlik\n",
    "    'learning_rate': uniform(0.005, 0.095),  # Öğrenme oranı\n",
    "    'min_data_in_leaf': randint(20, 200),  # Her yaprakta olması gereken minimum veri sayısı\n",
    "    'bagging_fraction': uniform(0.5, 0.5),  # Bagging oranı\n",
    "    'bagging_freq': randint(1, 10),         # Bagging sıklığı\n",
    "    'feature_fraction': uniform(0.5, 0.5),  # Özellik oranı\n",
    "    'lambda_l1': uniform(0, 1),             # L1 düzenlileştirme\n",
    "    'lambda_l2': uniform(0, 1),             # L2 düzenlileştirme\n",
    "    'max_bin': randint(200, 300)            # Maksimum bin sayısı\n",
    "}\n",
    "\n",
    "# LightGBM modelini tanımlama\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary')\n",
    "\n",
    "# RandomizedSearchCV kullanarak modeli eğitmek\n",
    "random_cfl = RandomizedSearchCV(\n",
    "    lgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,  # Kaç farklı parametre kombinasyonunu deneyeceğini belirtir\n",
    "    scoring='accuracy',  # Değerlendirme ölçütü\n",
    "    cv=5,  # Çapraz doğrulama kat sayısı\n",
    "    verbose=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Modeli eğitmek\n",
    "random_cfl.fit(X_train, y_train)\n",
    "\n",
    "# En iyi parametreleri ve skoru göster\n",
    "print(\"En iyi parametreler:\", random_cfl.best_params_)\n",
    "print(\"En iyi skor:\", random_cfl.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa45a04d-d667-40df-9239-8bee18d55ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 56, 'max_depth': 10, 'learning_rate': 0.05}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best parameters \n",
    "random_cfl.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e69f037f-3b68-4917-928c-48efe2267daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 330 candidates, totalling 1650 fits\n",
      "[CV 3/5; 2/330] START learning_rate=0.01, max_depth=8, num_leaves=52............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 2/330] END learning_rate=0.01, max_depth=8, num_leaves=52;, score=0.904 total time=  26.2s\n",
      "[CV 3/5; 3/330] START learning_rate=0.01, max_depth=8, num_leaves=54............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 3/330] END learning_rate=0.01, max_depth=8, num_leaves=54;, score=0.904 total time=  25.5s\n",
      "[CV 3/5; 4/330] START learning_rate=0.01, max_depth=8, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 4/330] END learning_rate=0.01, max_depth=8, num_leaves=56;, score=0.904 total time=  40.4s\n",
      "[CV 4/5; 6/330] START learning_rate=0.01, max_depth=8, num_leaves=60............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 6/330] END learning_rate=0.01, max_depth=8, num_leaves=60;, score=0.904 total time=  42.6s\n",
      "[CV 2/5; 8/330] START learning_rate=0.01, max_depth=8, num_leaves=64............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 8/330] END learning_rate=0.01, max_depth=8, num_leaves=64;, score=0.904 total time=  28.2s\n",
      "[CV 1/5; 10/330] START learning_rate=0.01, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 10/330] END learning_rate=0.01, max_depth=8, num_leaves=68;, score=0.904 total time=  28.2s\n",
      "[CV 1/5; 11/330] START learning_rate=0.01, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.312855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 11/330] END learning_rate=0.01, max_depth=8, num_leaves=70;, score=0.904 total time=  45.2s\n",
      "[CV 3/5; 13/330] START learning_rate=0.01, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 13/330] END learning_rate=0.01, max_depth=9, num_leaves=52;, score=0.904 total time=  25.6s\n",
      "[CV 2/5; 14/330] START learning_rate=0.01, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 14/330] END learning_rate=0.01, max_depth=9, num_leaves=54;, score=0.904 total time=  25.5s\n",
      "[CV 4/5; 15/330] START learning_rate=0.01, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 15/330] END learning_rate=0.01, max_depth=9, num_leaves=56;, score=0.904 total time=  24.6s\n",
      "[CV 5/5; 16/330] START learning_rate=0.01, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 16/330] END learning_rate=0.01, max_depth=9, num_leaves=58;, score=0.904 total time=  25.5s\n",
      "[CV 2/5; 18/330] START learning_rate=0.01, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 18/330] END learning_rate=0.01, max_depth=9, num_leaves=62;, score=0.904 total time=  27.0s\n",
      "[CV 3/5; 19/330] START learning_rate=0.01, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 19/330] END learning_rate=0.01, max_depth=9, num_leaves=64;, score=0.904 total time=  44.4s\n",
      "[CV 2/5; 21/330] START learning_rate=0.01, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 21/330] END learning_rate=0.01, max_depth=9, num_leaves=68;, score=0.904 total time=  28.6s\n",
      "[CV 1/5; 23/330] START learning_rate=0.01, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 23/330] END learning_rate=0.01, max_depth=10, num_leaves=50;, score=0.904 total time=  40.0s\n",
      "[CV 2/5; 24/330] START learning_rate=0.01, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 24/330] END learning_rate=0.01, max_depth=10, num_leaves=52;, score=0.904 total time=  26.3s\n",
      "[CV 4/5; 25/330] START learning_rate=0.01, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 25/330] END learning_rate=0.01, max_depth=10, num_leaves=54;, score=0.904 total time=  25.4s\n",
      "[CV 2/5; 27/330] START learning_rate=0.01, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 27/330] END learning_rate=0.01, max_depth=10, num_leaves=58;, score=0.904 total time=  27.1s\n",
      "[CV 5/5; 28/330] START learning_rate=0.01, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.303283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 28/330] END learning_rate=0.01, max_depth=10, num_leaves=60;, score=0.904 total time=  42.5s\n",
      "[CV 3/5; 30/330] START learning_rate=0.01, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 30/330] END learning_rate=0.01, max_depth=10, num_leaves=64;, score=0.904 total time=  40.9s\n",
      "[CV 4/5; 32/330] START learning_rate=0.01, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 32/330] END learning_rate=0.01, max_depth=10, num_leaves=68;, score=0.904 total time=  28.8s\n",
      "[CV 1/5; 34/330] START learning_rate=0.01, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 34/330] END learning_rate=0.01, max_depth=11, num_leaves=50;, score=0.904 total time=  26.0s\n",
      "[CV 2/5; 35/330] START learning_rate=0.01, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[CV 1/5; 1/330] START learning_rate=0.01, max_depth=8, num_leaves=50............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 1/330] END learning_rate=0.01, max_depth=8, num_leaves=50;, score=0.904 total time=  24.7s\n",
      "[CV 4/5; 2/330] START learning_rate=0.01, max_depth=8, num_leaves=52............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 2/330] END learning_rate=0.01, max_depth=8, num_leaves=52;, score=0.904 total time=  38.6s\n",
      "[CV 4/5; 4/330] START learning_rate=0.01, max_depth=8, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 4/330] END learning_rate=0.01, max_depth=8, num_leaves=56;, score=0.904 total time=  25.4s\n",
      "[CV 5/5; 5/330] START learning_rate=0.01, max_depth=8, num_leaves=58............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 5/330] END learning_rate=0.01, max_depth=8, num_leaves=58;, score=0.904 total time=  26.2s\n",
      "[CV 3/5; 7/330] START learning_rate=0.01, max_depth=8, num_leaves=62............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 7/330] END learning_rate=0.01, max_depth=8, num_leaves=62;, score=0.904 total time=  28.9s\n",
      "[CV 5/5; 8/330] START learning_rate=0.01, max_depth=8, num_leaves=64............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 8/330] END learning_rate=0.01, max_depth=8, num_leaves=64;, score=0.904 total time=  27.0s\n",
      "[CV 2/5; 10/330] START learning_rate=0.01, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 10/330] END learning_rate=0.01, max_depth=8, num_leaves=68;, score=0.904 total time=  28.0s\n",
      "[CV 2/5; 11/330] START learning_rate=0.01, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 11/330] END learning_rate=0.01, max_depth=8, num_leaves=70;, score=0.904 total time=  27.9s\n",
      "[CV 3/5; 12/330] START learning_rate=0.01, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 12/330] END learning_rate=0.01, max_depth=9, num_leaves=50;, score=0.904 total time=  39.0s\n",
      "[CV 3/5; 14/330] START learning_rate=0.01, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 14/330] END learning_rate=0.01, max_depth=9, num_leaves=54;, score=0.904 total time=  25.9s\n",
      "[CV 5/5; 15/330] START learning_rate=0.01, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 15/330] END learning_rate=0.01, max_depth=9, num_leaves=56;, score=0.904 total time=  25.9s\n",
      "[CV 2/5; 17/330] START learning_rate=0.01, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 17/330] END learning_rate=0.01, max_depth=9, num_leaves=60;, score=0.904 total time=  43.1s\n",
      "[CV 2/5; 19/330] START learning_rate=0.01, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 19/330] END learning_rate=0.01, max_depth=9, num_leaves=64;, score=0.904 total time=  28.4s\n",
      "[CV 3/5; 20/330] START learning_rate=0.01, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 20/330] END learning_rate=0.01, max_depth=9, num_leaves=66;, score=0.904 total time=  43.4s\n",
      "[CV 3/5; 22/330] START learning_rate=0.01, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 22/330] END learning_rate=0.01, max_depth=9, num_leaves=70;, score=0.904 total time=  28.6s\n",
      "[CV 5/5; 23/330] START learning_rate=0.01, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 23/330] END learning_rate=0.01, max_depth=10, num_leaves=50;, score=0.904 total time=  26.1s\n",
      "[CV 2/5; 25/330] START learning_rate=0.01, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 25/330] END learning_rate=0.01, max_depth=10, num_leaves=54;, score=0.904 total time=  25.9s\n",
      "[CV 3/5; 26/330] START learning_rate=0.01, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 26/330] END learning_rate=0.01, max_depth=10, num_leaves=56;, score=0.904 total time=  40.3s\n",
      "[CV 3/5; 28/330] START learning_rate=0.01, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 28/330] END learning_rate=0.01, max_depth=10, num_leaves=60;, score=0.904 total time=  43.7s\n",
      "[CV 1/5; 30/330] START learning_rate=0.01, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 30/330] END learning_rate=0.01, max_depth=10, num_leaves=64;, score=0.904 total time=  26.4s\n",
      "[CV 4/5; 31/330] START learning_rate=0.01, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 31/330] END learning_rate=0.01, max_depth=10, num_leaves=66;, score=0.904 total time=  44.1s\n",
      "[CV 5/5; 33/330] START learning_rate=0.01, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.132415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 33/330] END learning_rate=0.01, max_depth=10, num_leaves=70;, score=0.904 total time=  45.4s\n",
      "[CV 4/5; 35/330] START learning_rate=0.01, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[CV 2/5; 2/330] START learning_rate=0.01, max_depth=8, num_leaves=52............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 2/330] END learning_rate=0.01, max_depth=8, num_leaves=52;, score=0.904 total time=  40.6s\n",
      "[CV 5/5; 3/330] START learning_rate=0.01, max_depth=8, num_leaves=54............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 3/330] END learning_rate=0.01, max_depth=8, num_leaves=54;, score=0.904 total time=  24.6s\n",
      "[CV 2/5; 5/330] START learning_rate=0.01, max_depth=8, num_leaves=58............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 5/330] END learning_rate=0.01, max_depth=8, num_leaves=58;, score=0.904 total time=  26.9s\n",
      "[CV 5/5; 6/330] START learning_rate=0.01, max_depth=8, num_leaves=60............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 6/330] END learning_rate=0.01, max_depth=8, num_leaves=60;, score=0.904 total time=  42.7s\n",
      "[CV 4/5; 8/330] START learning_rate=0.01, max_depth=8, num_leaves=64............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 8/330] END learning_rate=0.01, max_depth=8, num_leaves=64;, score=0.904 total time=  27.8s\n",
      "[CV 5/5; 9/330] START learning_rate=0.01, max_depth=8, num_leaves=66............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 9/330] END learning_rate=0.01, max_depth=8, num_leaves=66;, score=0.904 total time=  43.9s\n",
      "[CV 1/5; 12/330] START learning_rate=0.01, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 12/330] END learning_rate=0.01, max_depth=9, num_leaves=50;, score=0.904 total time=  24.3s\n",
      "[CV 4/5; 12/330] START learning_rate=0.01, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 12/330] END learning_rate=0.01, max_depth=9, num_leaves=50;, score=0.904 total time=  39.4s\n",
      "[CV 4/5; 14/330] START learning_rate=0.01, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 14/330] END learning_rate=0.01, max_depth=9, num_leaves=54;, score=0.904 total time=  25.6s\n",
      "[CV 1/5; 16/330] START learning_rate=0.01, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 16/330] END learning_rate=0.01, max_depth=9, num_leaves=58;, score=0.904 total time=  25.7s\n",
      "[CV 3/5; 17/330] START learning_rate=0.01, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 17/330] END learning_rate=0.01, max_depth=9, num_leaves=60;, score=0.904 total time=  27.3s\n",
      "[CV 4/5; 18/330] START learning_rate=0.01, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 18/330] END learning_rate=0.01, max_depth=9, num_leaves=62;, score=0.904 total time=  43.0s\n",
      "[CV 4/5; 20/330] START learning_rate=0.01, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.210099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 20/330] END learning_rate=0.01, max_depth=9, num_leaves=66;, score=0.904 total time=  42.1s\n",
      "[CV 4/5; 22/330] START learning_rate=0.01, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 22/330] END learning_rate=0.01, max_depth=9, num_leaves=70;, score=0.904 total time=  45.0s\n",
      "[CV 4/5; 24/330] START learning_rate=0.01, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270886 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 24/330] END learning_rate=0.01, max_depth=10, num_leaves=52;, score=0.904 total time=  38.2s\n",
      "[CV 5/5; 26/330] START learning_rate=0.01, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 26/330] END learning_rate=0.01, max_depth=10, num_leaves=56;, score=0.904 total time=  25.8s\n",
      "[CV 5/5; 27/330] START learning_rate=0.01, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 27/330] END learning_rate=0.01, max_depth=10, num_leaves=58;, score=0.904 total time=  42.3s\n",
      "[CV 4/5; 29/330] START learning_rate=0.01, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 29/330] END learning_rate=0.01, max_depth=10, num_leaves=62;, score=0.904 total time=  27.1s\n",
      "[CV 2/5; 31/330] START learning_rate=0.01, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 31/330] END learning_rate=0.01, max_depth=10, num_leaves=66;, score=0.904 total time=  28.1s\n",
      "[CV 5/5; 32/330] START learning_rate=0.01, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 32/330] END learning_rate=0.01, max_depth=10, num_leaves=68;, score=0.904 total time=  43.4s\n",
      "[CV 4/5; 34/330] START learning_rate=0.01, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 34/330] END learning_rate=0.01, max_depth=11, num_leaves=50;, score=0.904 total time=  38.9s\n",
      "[CV 4/5; 36/330] START learning_rate=0.01, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 36/330] END learning_rate=0.01, max_depth=11, num_leaves=54;, score=0.904 total time=  40.0s\n",
      "[CV 2/5; 38/330] START learning_rate=0.01, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 5/5; 1/330] START learning_rate=0.01, max_depth=8, num_leaves=50............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 1/330] END learning_rate=0.01, max_depth=8, num_leaves=50;, score=0.904 total time=  25.1s\n",
      "[CV 5/5; 2/330] START learning_rate=0.01, max_depth=8, num_leaves=52............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 2/330] END learning_rate=0.01, max_depth=8, num_leaves=52;, score=0.904 total time=  38.5s\n",
      "[CV 5/5; 4/330] START learning_rate=0.01, max_depth=8, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 4/330] END learning_rate=0.01, max_depth=8, num_leaves=56;, score=0.904 total time=  26.2s\n",
      "[CV 1/5; 6/330] START learning_rate=0.01, max_depth=8, num_leaves=60............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 6/330] END learning_rate=0.01, max_depth=8, num_leaves=60;, score=0.904 total time=  43.8s\n",
      "[CV 1/5; 8/330] START learning_rate=0.01, max_depth=8, num_leaves=64............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 8/330] END learning_rate=0.01, max_depth=8, num_leaves=64;, score=0.904 total time=  27.9s\n",
      "[CV 3/5; 9/330] START learning_rate=0.01, max_depth=8, num_leaves=66............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.330995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 9/330] END learning_rate=0.01, max_depth=8, num_leaves=66;, score=0.904 total time=  42.6s\n",
      "[CV 3/5; 11/330] START learning_rate=0.01, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 11/330] END learning_rate=0.01, max_depth=8, num_leaves=70;, score=0.904 total time=  28.3s\n",
      "[CV 1/5; 13/330] START learning_rate=0.01, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.320448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 13/330] END learning_rate=0.01, max_depth=9, num_leaves=52;, score=0.904 total time=  40.4s\n",
      "[CV 1/5; 15/330] START learning_rate=0.01, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 15/330] END learning_rate=0.01, max_depth=9, num_leaves=56;, score=0.904 total time=  39.8s\n",
      "[CV 1/5; 17/330] START learning_rate=0.01, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 17/330] END learning_rate=0.01, max_depth=9, num_leaves=60;, score=0.904 total time=  26.2s\n",
      "[CV 3/5; 18/330] START learning_rate=0.01, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 18/330] END learning_rate=0.01, max_depth=9, num_leaves=62;, score=0.904 total time=  42.0s\n",
      "[CV 1/5; 20/330] START learning_rate=0.01, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.260463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 20/330] END learning_rate=0.01, max_depth=9, num_leaves=66;, score=0.904 total time=  28.1s\n",
      "[CV 3/5; 21/330] START learning_rate=0.01, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 21/330] END learning_rate=0.01, max_depth=9, num_leaves=68;, score=0.904 total time=  28.4s\n",
      "[CV 5/5; 22/330] START learning_rate=0.01, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 22/330] END learning_rate=0.01, max_depth=9, num_leaves=70;, score=0.904 total time=  44.4s\n",
      "[CV 5/5; 24/330] START learning_rate=0.01, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 24/330] END learning_rate=0.01, max_depth=10, num_leaves=52;, score=0.904 total time=  25.4s\n",
      "[CV 1/5; 26/330] START learning_rate=0.01, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 26/330] END learning_rate=0.01, max_depth=10, num_leaves=56;, score=0.904 total time=  40.3s\n",
      "[CV 2/5; 28/330] START learning_rate=0.01, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 28/330] END learning_rate=0.01, max_depth=10, num_leaves=60;, score=0.904 total time=  28.7s\n",
      "[CV 3/5; 29/330] START learning_rate=0.01, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 29/330] END learning_rate=0.01, max_depth=10, num_leaves=62;, score=0.904 total time=  27.6s\n",
      "[CV 1/5; 31/330] START learning_rate=0.01, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 31/330] END learning_rate=0.01, max_depth=10, num_leaves=66;, score=0.904 total time=  26.7s\n",
      "[CV 3/5; 32/330] START learning_rate=0.01, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 32/330] END learning_rate=0.01, max_depth=10, num_leaves=68;, score=0.904 total time=  28.4s\n",
      "[CV 4/5; 33/330] START learning_rate=0.01, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 33/330] END learning_rate=0.01, max_depth=10, num_leaves=70;, score=0.904 total time=  45.3s\n",
      "[CV 3/5; 35/330] START learning_rate=0.01, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.203603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 35/330] END learning_rate=0.01, max_depth=11, num_leaves=52;, score=0.904 total time=  40.3s\n",
      "[CV 3/5; 37/330] START learning_rate=0.01, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 37/330] END learning_rate=0.01, max_depth=11, num_leaves=56;, score=0.904 total time=  40.4s\n",
      "[CV 2/5; 1/330] START learning_rate=0.01, max_depth=8, num_leaves=50............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 1/330] END learning_rate=0.01, max_depth=8, num_leaves=50;, score=0.904 total time=  40.6s\n",
      "[CV 1/5; 4/330] START learning_rate=0.01, max_depth=8, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 4/330] END learning_rate=0.01, max_depth=8, num_leaves=56;, score=0.904 total time=  25.4s\n",
      "[CV 3/5; 5/330] START learning_rate=0.01, max_depth=8, num_leaves=58............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 5/330] END learning_rate=0.01, max_depth=8, num_leaves=58;, score=0.904 total time=  40.1s\n",
      "[CV 1/5; 7/330] START learning_rate=0.01, max_depth=8, num_leaves=62............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 7/330] END learning_rate=0.01, max_depth=8, num_leaves=62;, score=0.904 total time=  28.7s\n",
      "[CV 3/5; 8/330] START learning_rate=0.01, max_depth=8, num_leaves=64............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 8/330] END learning_rate=0.01, max_depth=8, num_leaves=64;, score=0.904 total time=  41.7s\n",
      "[CV 3/5; 10/330] START learning_rate=0.01, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 10/330] END learning_rate=0.01, max_depth=8, num_leaves=68;, score=0.904 total time=  28.6s\n",
      "[CV 4/5; 11/330] START learning_rate=0.01, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 11/330] END learning_rate=0.01, max_depth=8, num_leaves=70;, score=0.904 total time=  44.0s\n",
      "[CV 5/5; 13/330] START learning_rate=0.01, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 13/330] END learning_rate=0.01, max_depth=9, num_leaves=52;, score=0.904 total time=  38.2s\n",
      "[CV 3/5; 15/330] START learning_rate=0.01, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 15/330] END learning_rate=0.01, max_depth=9, num_leaves=56;, score=0.904 total time=  38.6s\n",
      "[CV 1/5; 18/330] START learning_rate=0.01, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 18/330] END learning_rate=0.01, max_depth=9, num_leaves=62;, score=0.904 total time=  27.2s\n",
      "[CV 1/5; 19/330] START learning_rate=0.01, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 19/330] END learning_rate=0.01, max_depth=9, num_leaves=64;, score=0.904 total time=  28.3s\n",
      "[CV 2/5; 20/330] START learning_rate=0.01, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 20/330] END learning_rate=0.01, max_depth=9, num_leaves=66;, score=0.904 total time=  45.5s\n",
      "[CV 2/5; 22/330] START learning_rate=0.01, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 22/330] END learning_rate=0.01, max_depth=9, num_leaves=70;, score=0.904 total time=  28.9s\n",
      "[CV 4/5; 23/330] START learning_rate=0.01, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 23/330] END learning_rate=0.01, max_depth=10, num_leaves=50;, score=0.904 total time=  26.6s\n",
      "[CV 1/5; 25/330] START learning_rate=0.01, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 25/330] END learning_rate=0.01, max_depth=10, num_leaves=54;, score=0.904 total time=  25.4s\n",
      "[CV 2/5; 26/330] START learning_rate=0.01, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 26/330] END learning_rate=0.01, max_depth=10, num_leaves=56;, score=0.904 total time=  25.9s\n",
      "[CV 4/5; 27/330] START learning_rate=0.01, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 27/330] END learning_rate=0.01, max_depth=10, num_leaves=58;, score=0.904 total time=  41.8s\n",
      "[CV 2/5; 29/330] START learning_rate=0.01, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 29/330] END learning_rate=0.01, max_depth=10, num_leaves=62;, score=0.904 total time=  27.6s\n",
      "[CV 5/5; 30/330] START learning_rate=0.01, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 30/330] END learning_rate=0.01, max_depth=10, num_leaves=64;, score=0.904 total time=  26.0s\n",
      "[CV 2/5; 32/330] START learning_rate=0.01, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 32/330] END learning_rate=0.01, max_depth=10, num_leaves=68;, score=0.904 total time=  46.7s\n",
      "[CV 2/5; 34/330] START learning_rate=0.01, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 34/330] END learning_rate=0.01, max_depth=11, num_leaves=50;, score=0.904 total time=  40.6s\n",
      "[CV 1/5; 36/330] START learning_rate=0.01, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 36/330] END learning_rate=0.01, max_depth=11, num_leaves=54;, score=0.904 total time=  26.5s\n",
      "[CV 2/5; 37/330] START learning_rate=0.01, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[CV 3/5; 1/330] START learning_rate=0.01, max_depth=8, num_leaves=50............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 1/330] END learning_rate=0.01, max_depth=8, num_leaves=50;, score=0.904 total time=  25.3s\n",
      "[CV 1/5; 3/330] START learning_rate=0.01, max_depth=8, num_leaves=54............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.257260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 3/330] END learning_rate=0.01, max_depth=8, num_leaves=54;, score=0.904 total time=  25.7s\n",
      "[CV 2/5; 4/330] START learning_rate=0.01, max_depth=8, num_leaves=56............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 4/330] END learning_rate=0.01, max_depth=8, num_leaves=56;, score=0.904 total time=  41.1s\n",
      "[CV 3/5; 6/330] START learning_rate=0.01, max_depth=8, num_leaves=60............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 6/330] END learning_rate=0.01, max_depth=8, num_leaves=60;, score=0.904 total time=  27.6s\n",
      "[CV 5/5; 7/330] START learning_rate=0.01, max_depth=8, num_leaves=62............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 7/330] END learning_rate=0.01, max_depth=8, num_leaves=62;, score=0.904 total time=  43.0s\n",
      "[CV 4/5; 9/330] START learning_rate=0.01, max_depth=8, num_leaves=66............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 9/330] END learning_rate=0.01, max_depth=8, num_leaves=66;, score=0.904 total time=  42.9s\n",
      "[CV 5/5; 11/330] START learning_rate=0.01, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 11/330] END learning_rate=0.01, max_depth=8, num_leaves=70;, score=0.904 total time=  43.8s\n",
      "[CV 4/5; 13/330] START learning_rate=0.01, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 13/330] END learning_rate=0.01, max_depth=9, num_leaves=52;, score=0.904 total time=  25.1s\n",
      "[CV 2/5; 15/330] START learning_rate=0.01, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 15/330] END learning_rate=0.01, max_depth=9, num_leaves=56;, score=0.904 total time=  25.6s\n",
      "[CV 2/5; 16/330] START learning_rate=0.01, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 16/330] END learning_rate=0.01, max_depth=9, num_leaves=58;, score=0.904 total time=  26.1s\n",
      "[CV 4/5; 17/330] START learning_rate=0.01, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 17/330] END learning_rate=0.01, max_depth=9, num_leaves=60;, score=0.904 total time=  41.7s\n",
      "[CV 4/5; 19/330] START learning_rate=0.01, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 19/330] END learning_rate=0.01, max_depth=9, num_leaves=64;, score=0.904 total time=  42.6s\n",
      "[CV 4/5; 21/330] START learning_rate=0.01, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 21/330] END learning_rate=0.01, max_depth=9, num_leaves=68;, score=0.904 total time=  43.4s\n",
      "[CV 3/5; 23/330] START learning_rate=0.01, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 23/330] END learning_rate=0.01, max_depth=10, num_leaves=50;, score=0.904 total time=  38.9s\n",
      "[CV 3/5; 25/330] START learning_rate=0.01, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 25/330] END learning_rate=0.01, max_depth=10, num_leaves=54;, score=0.904 total time=  25.6s\n",
      "[CV 1/5; 27/330] START learning_rate=0.01, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 27/330] END learning_rate=0.01, max_depth=10, num_leaves=58;, score=0.904 total time=  26.1s\n",
      "[CV 1/5; 28/330] START learning_rate=0.01, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 28/330] END learning_rate=0.01, max_depth=10, num_leaves=60;, score=0.904 total time=  44.4s\n",
      "[CV 5/5; 29/330] START learning_rate=0.01, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 29/330] END learning_rate=0.01, max_depth=10, num_leaves=62;, score=0.904 total time=  26.9s\n",
      "[CV 3/5; 31/330] START learning_rate=0.01, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 31/330] END learning_rate=0.01, max_depth=10, num_leaves=66;, score=0.904 total time=  43.3s\n",
      "[CV 3/5; 33/330] START learning_rate=0.01, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 33/330] END learning_rate=0.01, max_depth=10, num_leaves=70;, score=0.904 total time=  28.9s\n",
      "[CV 1/5; 35/330] START learning_rate=0.01, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 35/330] END learning_rate=0.01, max_depth=11, num_leaves=52;, score=0.904 total time=  26.3s\n",
      "[CV 5/5; 35/330] START learning_rate=0.01, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 35/330] END learning_rate=0.01, max_depth=11, num_leaves=52;, score=0.904 total time=  39.7s\n",
      "[CV 5/5; 37/330] START learning_rate=0.01, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 37/330] END learning_rate=0.01, max_depth=11, num_leaves=56;, score=0.904 total time=  41.1s\n",
      "[CV 1/5; 2/330] START learning_rate=0.01, max_depth=8, num_leaves=52............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 2/330] END learning_rate=0.01, max_depth=8, num_leaves=52;, score=0.904 total time=  26.1s\n",
      "[CV 2/5; 3/330] START learning_rate=0.01, max_depth=8, num_leaves=54............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 3/330] END learning_rate=0.01, max_depth=8, num_leaves=54;, score=0.904 total time=  40.9s\n",
      "[CV 4/5; 5/330] START learning_rate=0.01, max_depth=8, num_leaves=58............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 5/330] END learning_rate=0.01, max_depth=8, num_leaves=58;, score=0.904 total time=  40.5s\n",
      "[CV 2/5; 7/330] START learning_rate=0.01, max_depth=8, num_leaves=62............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 7/330] END learning_rate=0.01, max_depth=8, num_leaves=62;, score=0.904 total time=  44.0s\n",
      "[CV 1/5; 9/330] START learning_rate=0.01, max_depth=8, num_leaves=66............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 9/330] END learning_rate=0.01, max_depth=8, num_leaves=66;, score=0.904 total time=  28.0s\n",
      "[CV 4/5; 10/330] START learning_rate=0.01, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 10/330] END learning_rate=0.01, max_depth=8, num_leaves=68;, score=0.904 total time=  28.6s\n",
      "[CV 2/5; 12/330] START learning_rate=0.01, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 12/330] END learning_rate=0.01, max_depth=9, num_leaves=50;, score=0.904 total time=  24.9s\n",
      "[CV 2/5; 13/330] START learning_rate=0.01, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 13/330] END learning_rate=0.01, max_depth=9, num_leaves=52;, score=0.904 total time=  25.8s\n",
      "[CV 1/5; 14/330] START learning_rate=0.01, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 14/330] END learning_rate=0.01, max_depth=9, num_leaves=54;, score=0.904 total time=  40.9s\n",
      "[CV 3/5; 16/330] START learning_rate=0.01, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 16/330] END learning_rate=0.01, max_depth=9, num_leaves=58;, score=0.904 total time=  26.2s\n",
      "[CV 5/5; 17/330] START learning_rate=0.01, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 17/330] END learning_rate=0.01, max_depth=9, num_leaves=60;, score=0.904 total time=  41.6s\n",
      "[CV 5/5; 19/330] START learning_rate=0.01, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 19/330] END learning_rate=0.01, max_depth=9, num_leaves=64;, score=0.904 total time=  27.7s\n",
      "[CV 1/5; 21/330] START learning_rate=0.01, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 21/330] END learning_rate=0.01, max_depth=9, num_leaves=68;, score=0.904 total time=  28.5s\n",
      "[CV 1/5; 22/330] START learning_rate=0.01, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 22/330] END learning_rate=0.01, max_depth=9, num_leaves=70;, score=0.904 total time=  28.7s\n",
      "[CV 2/5; 23/330] START learning_rate=0.01, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 23/330] END learning_rate=0.01, max_depth=10, num_leaves=50;, score=0.904 total time=  25.5s\n",
      "[CV 3/5; 24/330] START learning_rate=0.01, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.209374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 24/330] END learning_rate=0.01, max_depth=10, num_leaves=52;, score=0.904 total time=  38.3s\n",
      "[CV 4/5; 26/330] START learning_rate=0.01, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 26/330] END learning_rate=0.01, max_depth=10, num_leaves=56;, score=0.904 total time=  39.7s\n",
      "[CV 4/5; 28/330] START learning_rate=0.01, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 28/330] END learning_rate=0.01, max_depth=10, num_leaves=60;, score=0.904 total time=  42.1s\n",
      "[CV 2/5; 30/330] START learning_rate=0.01, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 30/330] END learning_rate=0.01, max_depth=10, num_leaves=64;, score=0.904 total time=  26.7s\n",
      "[CV 5/5; 31/330] START learning_rate=0.01, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 31/330] END learning_rate=0.01, max_depth=10, num_leaves=66;, score=0.904 total time=  28.4s\n",
      "[CV 1/5; 33/330] START learning_rate=0.01, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 33/330] END learning_rate=0.01, max_depth=10, num_leaves=70;, score=0.904 total time=  29.6s\n",
      "[CV 3/5; 34/330] START learning_rate=0.01, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304666 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 34/330] END learning_rate=0.01, max_depth=11, num_leaves=50;, score=0.904 total time=  39.3s\n",
      "[CV 3/5; 36/330] START learning_rate=0.01, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[CV 4/5; 1/330] START learning_rate=0.01, max_depth=8, num_leaves=50............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 1/330] END learning_rate=0.01, max_depth=8, num_leaves=50;, score=0.904 total time=  38.4s\n",
      "[CV 4/5; 3/330] START learning_rate=0.01, max_depth=8, num_leaves=54............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 3/330] END learning_rate=0.01, max_depth=8, num_leaves=54;, score=0.904 total time=  25.4s\n",
      "[CV 1/5; 5/330] START learning_rate=0.01, max_depth=8, num_leaves=58............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 5/330] END learning_rate=0.01, max_depth=8, num_leaves=58;, score=0.904 total time=  26.5s\n",
      "[CV 2/5; 6/330] START learning_rate=0.01, max_depth=8, num_leaves=60............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 6/330] END learning_rate=0.01, max_depth=8, num_leaves=60;, score=0.904 total time=  27.7s\n",
      "[CV 4/5; 7/330] START learning_rate=0.01, max_depth=8, num_leaves=62............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 7/330] END learning_rate=0.01, max_depth=8, num_leaves=62;, score=0.904 total time=  42.7s\n",
      "[CV 2/5; 9/330] START learning_rate=0.01, max_depth=8, num_leaves=66............\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 9/330] END learning_rate=0.01, max_depth=8, num_leaves=66;, score=0.904 total time=  28.3s\n",
      "[CV 5/5; 10/330] START learning_rate=0.01, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 10/330] END learning_rate=0.01, max_depth=8, num_leaves=68;, score=0.904 total time=  43.6s\n",
      "[CV 5/5; 12/330] START learning_rate=0.01, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.247931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 12/330] END learning_rate=0.01, max_depth=9, num_leaves=50;, score=0.904 total time=  37.9s\n",
      "[CV 5/5; 14/330] START learning_rate=0.01, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 14/330] END learning_rate=0.01, max_depth=9, num_leaves=54;, score=0.904 total time=  39.0s\n",
      "[CV 4/5; 16/330] START learning_rate=0.01, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 16/330] END learning_rate=0.01, max_depth=9, num_leaves=58;, score=0.904 total time=  39.5s\n",
      "[CV 5/5; 18/330] START learning_rate=0.01, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.236246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 18/330] END learning_rate=0.01, max_depth=9, num_leaves=62;, score=0.904 total time=  42.9s\n",
      "[CV 5/5; 20/330] START learning_rate=0.01, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 20/330] END learning_rate=0.01, max_depth=9, num_leaves=66;, score=0.904 total time=  28.3s\n",
      "[CV 5/5; 21/330] START learning_rate=0.01, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.333413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 21/330] END learning_rate=0.01, max_depth=9, num_leaves=68;, score=0.904 total time=  43.8s\n",
      "[CV 1/5; 24/330] START learning_rate=0.01, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 24/330] END learning_rate=0.01, max_depth=10, num_leaves=52;, score=0.904 total time=  40.2s\n",
      "[CV 5/5; 25/330] START learning_rate=0.01, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 25/330] END learning_rate=0.01, max_depth=10, num_leaves=54;, score=0.904 total time=  25.9s\n",
      "[CV 3/5; 27/330] START learning_rate=0.01, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 27/330] END learning_rate=0.01, max_depth=10, num_leaves=58;, score=0.904 total time=  27.1s\n",
      "[CV 1/5; 29/330] START learning_rate=0.01, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 29/330] END learning_rate=0.01, max_depth=10, num_leaves=62;, score=0.904 total time=  44.8s\n",
      "[CV 4/5; 30/330] START learning_rate=0.01, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 30/330] END learning_rate=0.01, max_depth=10, num_leaves=64;, score=0.904 total time=  25.8s\n",
      "[CV 1/5; 32/330] START learning_rate=0.01, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 32/330] END learning_rate=0.01, max_depth=10, num_leaves=68;, score=0.904 total time=  29.1s\n",
      "[CV 2/5; 33/330] START learning_rate=0.01, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 33/330] END learning_rate=0.01, max_depth=10, num_leaves=70;, score=0.904 total time=  29.1s\n",
      "[CV 5/5; 34/330] START learning_rate=0.01, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 34/330] END learning_rate=0.01, max_depth=11, num_leaves=50;, score=0.904 total time=  38.4s\n",
      "[CV 5/5; 36/330] START learning_rate=0.01, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176666 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 36/330] END learning_rate=0.01, max_depth=11, num_leaves=54;, score=0.904 total time=  39.4s\n",
      "[CV 3/5; 38/330] START learning_rate=0.01, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 38/330] END learning_rate=0.01, max_depth=11, num_leaves=58;, score=0.904 total time=  40.4s\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 35/330] END learning_rate=0.01, max_depth=11, num_leaves=52;, score=0.904 total time=  25.2s\n",
      "[CV 1/5; 37/330] START learning_rate=0.01, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 37/330] END learning_rate=0.01, max_depth=11, num_leaves=56;, score=0.904 total time=  26.7s\n",
      "[CV 4/5; 38/330] START learning_rate=0.01, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 38/330] END learning_rate=0.01, max_depth=11, num_leaves=58;, score=0.904 total time=  27.2s\n",
      "[CV 5/5; 39/330] START learning_rate=0.01, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 39/330] END learning_rate=0.01, max_depth=11, num_leaves=60;, score=0.904 total time=  41.5s\n",
      "[CV 5/5; 41/330] START learning_rate=0.01, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.236241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 41/330] END learning_rate=0.01, max_depth=11, num_leaves=64;, score=0.904 total time=  44.3s\n",
      "[CV 3/5; 43/330] START learning_rate=0.01, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 43/330] END learning_rate=0.01, max_depth=11, num_leaves=68;, score=0.904 total time=  28.7s\n",
      "[CV 4/5; 44/330] START learning_rate=0.01, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 44/330] END learning_rate=0.01, max_depth=11, num_leaves=70;, score=0.904 total time=  45.7s\n",
      "[CV 4/5; 46/330] START learning_rate=0.01, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 46/330] END learning_rate=0.01, max_depth=12, num_leaves=52;, score=0.904 total time=  38.7s\n",
      "[CV 1/5; 49/330] START learning_rate=0.01, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 49/330] END learning_rate=0.01, max_depth=12, num_leaves=58;, score=0.904 total time=  25.5s\n",
      "[CV 2/5; 50/330] START learning_rate=0.01, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 50/330] END learning_rate=0.01, max_depth=12, num_leaves=60;, score=0.904 total time=  25.9s\n",
      "[CV 4/5; 51/330] START learning_rate=0.01, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 51/330] END learning_rate=0.01, max_depth=12, num_leaves=62;, score=0.904 total time=  26.9s\n",
      "[CV 4/5; 52/330] START learning_rate=0.01, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 52/330] END learning_rate=0.01, max_depth=12, num_leaves=64;, score=0.904 total time=  40.4s\n",
      "[CV 1/5; 55/330] START learning_rate=0.01, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 55/330] END learning_rate=0.01, max_depth=12, num_leaves=70;, score=0.904 total time=  30.3s\n",
      "[CV 1/5; 56/330] START learning_rate=0.03, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 56/330] END learning_rate=0.03, max_depth=8, num_leaves=50;, score=0.909 total time=  40.1s\n",
      "[CV 1/5; 58/330] START learning_rate=0.03, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 58/330] END learning_rate=0.03, max_depth=8, num_leaves=54;, score=0.909 total time=  25.6s\n",
      "[CV 2/5; 59/330] START learning_rate=0.03, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 59/330] END learning_rate=0.03, max_depth=8, num_leaves=56;, score=0.909 total time=  25.4s\n",
      "[CV 5/5; 60/330] START learning_rate=0.03, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 60/330] END learning_rate=0.03, max_depth=8, num_leaves=58;, score=0.909 total time=  40.0s\n",
      "[CV 5/5; 62/330] START learning_rate=0.03, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 62/330] END learning_rate=0.03, max_depth=8, num_leaves=62;, score=0.909 total time=  25.9s\n",
      "[CV 2/5; 64/330] START learning_rate=0.03, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 64/330] END learning_rate=0.03, max_depth=8, num_leaves=66;, score=0.909 total time=  27.6s\n",
      "[CV 4/5; 65/330] START learning_rate=0.03, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 65/330] END learning_rate=0.03, max_depth=8, num_leaves=68;, score=0.909 total time=  27.0s\n",
      "[CV 1/5; 67/330] START learning_rate=0.03, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 67/330] END learning_rate=0.03, max_depth=9, num_leaves=50;, score=0.909 total time=  38.6s\n",
      "[CV 5/5; 68/330] START learning_rate=0.03, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 68/330] END learning_rate=0.03, max_depth=9, num_leaves=52;, score=0.909 total time=  26.5s\n",
      "[CV 2/5; 70/330] START learning_rate=0.03, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 70/330] END learning_rate=0.03, max_depth=9, num_leaves=56;, score=0.909 total time=  26.6s\n",
      "[CV 5/5; 71/330] START learning_rate=0.03, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 35/330] END learning_rate=0.01, max_depth=11, num_leaves=52;, score=0.904 total time=  26.1s\n",
      "[CV 2/5; 36/330] START learning_rate=0.01, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 36/330] END learning_rate=0.01, max_depth=11, num_leaves=54;, score=0.904 total time=  40.7s\n",
      "[CV 1/5; 38/330] START learning_rate=0.01, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 38/330] END learning_rate=0.01, max_depth=11, num_leaves=58;, score=0.904 total time=  27.1s\n",
      "[CV 2/5; 39/330] START learning_rate=0.01, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 39/330] END learning_rate=0.01, max_depth=11, num_leaves=60;, score=0.904 total time=  28.3s\n",
      "[CV 5/5; 40/330] START learning_rate=0.01, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 40/330] END learning_rate=0.01, max_depth=11, num_leaves=62;, score=0.904 total time=  43.0s\n",
      "[CV 5/5; 42/330] START learning_rate=0.01, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 42/330] END learning_rate=0.01, max_depth=11, num_leaves=66;, score=0.904 total time=  29.3s\n",
      "[CV 2/5; 44/330] START learning_rate=0.01, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 44/330] END learning_rate=0.01, max_depth=11, num_leaves=70;, score=0.904 total time=  29.4s\n",
      "[CV 1/5; 45/330] START learning_rate=0.01, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 45/330] END learning_rate=0.01, max_depth=12, num_leaves=50;, score=0.904 total time=  40.2s\n",
      "[CV 1/5; 47/330] START learning_rate=0.01, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 47/330] END learning_rate=0.01, max_depth=12, num_leaves=54;, score=0.904 total time=  25.9s\n",
      "[CV 2/5; 48/330] START learning_rate=0.01, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 48/330] END learning_rate=0.01, max_depth=12, num_leaves=56;, score=0.904 total time=  40.3s\n",
      "[CV 4/5; 50/330] START learning_rate=0.01, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 50/330] END learning_rate=0.01, max_depth=12, num_leaves=60;, score=0.904 total time=  41.4s\n",
      "[CV 3/5; 52/330] START learning_rate=0.01, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 52/330] END learning_rate=0.01, max_depth=12, num_leaves=64;, score=0.904 total time=  27.0s\n",
      "[CV 1/5; 54/330] START learning_rate=0.01, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 54/330] END learning_rate=0.01, max_depth=12, num_leaves=68;, score=0.904 total time=  27.3s\n",
      "[CV 2/5; 55/330] START learning_rate=0.01, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 55/330] END learning_rate=0.01, max_depth=12, num_leaves=70;, score=0.904 total time=  29.7s\n",
      "[CV 5/5; 56/330] START learning_rate=0.03, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 56/330] END learning_rate=0.03, max_depth=8, num_leaves=50;, score=0.909 total time=  38.5s\n",
      "[CV 4/5; 58/330] START learning_rate=0.03, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 58/330] END learning_rate=0.03, max_depth=8, num_leaves=54;, score=0.909 total time=  39.2s\n",
      "[CV 4/5; 60/330] START learning_rate=0.03, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 60/330] END learning_rate=0.03, max_depth=8, num_leaves=58;, score=0.909 total time=  40.3s\n",
      "[CV 3/5; 62/330] START learning_rate=0.03, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 62/330] END learning_rate=0.03, max_depth=8, num_leaves=62;, score=0.909 total time=  26.2s\n",
      "[CV 1/5; 64/330] START learning_rate=0.03, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 64/330] END learning_rate=0.03, max_depth=8, num_leaves=66;, score=0.909 total time=  26.7s\n",
      "[CV 3/5; 65/330] START learning_rate=0.03, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.369402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 65/330] END learning_rate=0.03, max_depth=8, num_leaves=68;, score=0.909 total time=  40.5s\n",
      "[CV 3/5; 67/330] START learning_rate=0.03, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 67/330] END learning_rate=0.03, max_depth=9, num_leaves=50;, score=0.909 total time=  26.5s\n",
      "[CV 4/5; 68/330] START learning_rate=0.03, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 68/330] END learning_rate=0.03, max_depth=9, num_leaves=52;, score=0.909 total time=  40.0s\n",
      "[CV 4/5; 70/330] START learning_rate=0.03, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 70/330] END learning_rate=0.03, max_depth=9, num_leaves=56;, score=0.909 total time=  25.8s\n",
      "[CV 2/5; 72/330] START learning_rate=0.03, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[CV 2/5; 38/330] END learning_rate=0.01, max_depth=11, num_leaves=58;, score=0.904 total time=  27.0s\n",
      "[CV 3/5; 39/330] START learning_rate=0.01, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 39/330] END learning_rate=0.01, max_depth=11, num_leaves=60;, score=0.904 total time=  27.5s\n",
      "[CV 1/5; 41/330] START learning_rate=0.01, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 41/330] END learning_rate=0.01, max_depth=11, num_leaves=64;, score=0.904 total time=  28.7s\n",
      "[CV 2/5; 42/330] START learning_rate=0.01, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.259473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 42/330] END learning_rate=0.01, max_depth=11, num_leaves=66;, score=0.904 total time=  29.1s\n",
      "[CV 1/5; 43/330] START learning_rate=0.01, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 43/330] END learning_rate=0.01, max_depth=11, num_leaves=68;, score=0.904 total time=  46.3s\n",
      "[CV 2/5; 45/330] START learning_rate=0.01, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 45/330] END learning_rate=0.01, max_depth=12, num_leaves=50;, score=0.904 total time=  25.8s\n",
      "[CV 3/5; 46/330] START learning_rate=0.01, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 46/330] END learning_rate=0.01, max_depth=12, num_leaves=52;, score=0.904 total time=  38.8s\n",
      "[CV 4/5; 48/330] START learning_rate=0.01, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 48/330] END learning_rate=0.01, max_depth=12, num_leaves=56;, score=0.904 total time=  25.6s\n",
      "[CV 5/5; 49/330] START learning_rate=0.01, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 49/330] END learning_rate=0.01, max_depth=12, num_leaves=58;, score=0.904 total time=  26.7s\n",
      "[CV 2/5; 51/330] START learning_rate=0.01, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.176558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 51/330] END learning_rate=0.01, max_depth=12, num_leaves=62;, score=0.904 total time=  27.8s\n",
      "[CV 2/5; 52/330] START learning_rate=0.01, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 52/330] END learning_rate=0.01, max_depth=12, num_leaves=64;, score=0.904 total time=  26.8s\n",
      "[CV 5/5; 53/330] START learning_rate=0.01, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.561061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 53/330] END learning_rate=0.01, max_depth=12, num_leaves=66;, score=0.904 total time=  42.8s\n",
      "[CV 5/5; 55/330] START learning_rate=0.01, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 55/330] END learning_rate=0.01, max_depth=12, num_leaves=70;, score=0.904 total time=  45.1s\n",
      "[CV 2/5; 58/330] START learning_rate=0.03, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 58/330] END learning_rate=0.03, max_depth=8, num_leaves=54;, score=0.909 total time=  25.8s\n",
      "[CV 3/5; 59/330] START learning_rate=0.03, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 59/330] END learning_rate=0.03, max_depth=8, num_leaves=56;, score=0.909 total time=  25.3s\n",
      "[CV 1/5; 61/330] START learning_rate=0.03, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 61/330] END learning_rate=0.03, max_depth=8, num_leaves=60;, score=0.909 total time=  26.3s\n",
      "[CV 1/5; 62/330] START learning_rate=0.03, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 62/330] END learning_rate=0.03, max_depth=8, num_leaves=62;, score=0.909 total time=  26.1s\n",
      "[CV 3/5; 63/330] START learning_rate=0.03, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 63/330] END learning_rate=0.03, max_depth=8, num_leaves=64;, score=0.909 total time=  26.5s\n",
      "[CV 1/5; 65/330] START learning_rate=0.03, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 65/330] END learning_rate=0.03, max_depth=8, num_leaves=68;, score=0.909 total time=  27.9s\n",
      "[CV 1/5; 66/330] START learning_rate=0.03, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 66/330] END learning_rate=0.03, max_depth=8, num_leaves=70;, score=0.909 total time=  27.1s\n",
      "[CV 4/5; 67/330] START learning_rate=0.03, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 67/330] END learning_rate=0.03, max_depth=9, num_leaves=50;, score=0.909 total time=  25.7s\n",
      "[CV 1/5; 69/330] START learning_rate=0.03, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 69/330] END learning_rate=0.03, max_depth=9, num_leaves=54;, score=0.909 total time=  41.4s\n",
      "[CV 1/5; 71/330] START learning_rate=0.03, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 71/330] END learning_rate=0.03, max_depth=9, num_leaves=58;, score=0.909 total time=  26.0s\n",
      "[CV 4/5; 72/330] START learning_rate=0.03, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[CV 1/5; 39/330] START learning_rate=0.01, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 39/330] END learning_rate=0.01, max_depth=11, num_leaves=60;, score=0.904 total time=  27.2s\n",
      "[CV 4/5; 40/330] START learning_rate=0.01, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 40/330] END learning_rate=0.01, max_depth=11, num_leaves=62;, score=0.904 total time=  43.3s\n",
      "[CV 4/5; 42/330] START learning_rate=0.01, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 42/330] END learning_rate=0.01, max_depth=11, num_leaves=66;, score=0.904 total time=  29.6s\n",
      "[CV 5/5; 43/330] START learning_rate=0.01, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 43/330] END learning_rate=0.01, max_depth=11, num_leaves=68;, score=0.904 total time=  43.1s\n",
      "[CV 1/5; 46/330] START learning_rate=0.01, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 46/330] END learning_rate=0.01, max_depth=12, num_leaves=52;, score=0.904 total time=  25.6s\n",
      "[CV 5/5; 46/330] START learning_rate=0.01, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 46/330] END learning_rate=0.01, max_depth=12, num_leaves=52;, score=0.904 total time=  25.1s\n",
      "[CV 1/5; 48/330] START learning_rate=0.01, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 48/330] END learning_rate=0.01, max_depth=12, num_leaves=56;, score=0.904 total time=  26.5s\n",
      "[CV 3/5; 49/330] START learning_rate=0.01, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 49/330] END learning_rate=0.01, max_depth=12, num_leaves=58;, score=0.904 total time=  25.9s\n",
      "[CV 1/5; 51/330] START learning_rate=0.01, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 51/330] END learning_rate=0.01, max_depth=12, num_leaves=62;, score=0.904 total time=  42.7s\n",
      "[CV 1/5; 53/330] START learning_rate=0.01, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 53/330] END learning_rate=0.01, max_depth=12, num_leaves=66;, score=0.904 total time=  27.0s\n",
      "[CV 3/5; 54/330] START learning_rate=0.01, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 54/330] END learning_rate=0.01, max_depth=12, num_leaves=68;, score=0.904 total time=  44.6s\n",
      "[CV 2/5; 56/330] START learning_rate=0.03, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 56/330] END learning_rate=0.03, max_depth=8, num_leaves=50;, score=0.909 total time=  24.2s\n",
      "[CV 3/5; 57/330] START learning_rate=0.03, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 57/330] END learning_rate=0.03, max_depth=8, num_leaves=52;, score=0.909 total time=  39.3s\n",
      "[CV 4/5; 59/330] START learning_rate=0.03, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 59/330] END learning_rate=0.03, max_depth=8, num_leaves=56;, score=0.909 total time=  25.6s\n",
      "[CV 2/5; 61/330] START learning_rate=0.03, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 61/330] END learning_rate=0.03, max_depth=8, num_leaves=60;, score=0.909 total time=  26.7s\n",
      "[CV 2/5; 62/330] START learning_rate=0.03, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 62/330] END learning_rate=0.03, max_depth=8, num_leaves=62;, score=0.909 total time=  26.7s\n",
      "[CV 5/5; 63/330] START learning_rate=0.03, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 63/330] END learning_rate=0.03, max_depth=8, num_leaves=64;, score=0.909 total time=  40.7s\n",
      "[CV 5/5; 65/330] START learning_rate=0.03, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 65/330] END learning_rate=0.03, max_depth=8, num_leaves=68;, score=0.909 total time=  27.7s\n",
      "[CV 2/5; 67/330] START learning_rate=0.03, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 67/330] END learning_rate=0.03, max_depth=9, num_leaves=50;, score=0.909 total time=  39.7s\n",
      "[CV 3/5; 69/330] START learning_rate=0.03, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 69/330] END learning_rate=0.03, max_depth=9, num_leaves=54;, score=0.909 total time=  39.3s\n",
      "[CV 2/5; 71/330] START learning_rate=0.03, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.245762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 71/330] END learning_rate=0.03, max_depth=9, num_leaves=58;, score=0.909 total time=  27.0s\n",
      "[CV 5/5; 72/330] START learning_rate=0.03, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 72/330] END learning_rate=0.03, max_depth=9, num_leaves=60;, score=0.909 total time=  25.9s\n",
      "[CV 2/5; 74/330] START learning_rate=0.03, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[CV 1/5; 40/330] START learning_rate=0.01, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 40/330] END learning_rate=0.01, max_depth=11, num_leaves=62;, score=0.904 total time=  28.1s\n",
      "[CV 2/5; 41/330] START learning_rate=0.01, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 41/330] END learning_rate=0.01, max_depth=11, num_leaves=64;, score=0.904 total time=  28.4s\n",
      "[CV 3/5; 42/330] START learning_rate=0.01, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 42/330] END learning_rate=0.01, max_depth=11, num_leaves=66;, score=0.904 total time=  45.3s\n",
      "[CV 3/5; 44/330] START learning_rate=0.01, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 44/330] END learning_rate=0.01, max_depth=11, num_leaves=70;, score=0.904 total time=  29.8s\n",
      "[CV 5/5; 45/330] START learning_rate=0.01, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 45/330] END learning_rate=0.01, max_depth=12, num_leaves=50;, score=0.904 total time=  38.3s\n",
      "[CV 4/5; 47/330] START learning_rate=0.01, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 47/330] END learning_rate=0.01, max_depth=12, num_leaves=54;, score=0.904 total time=  25.9s\n",
      "[CV 5/5; 48/330] START learning_rate=0.01, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 48/330] END learning_rate=0.01, max_depth=12, num_leaves=56;, score=0.904 total time=  24.8s\n",
      "[CV 1/5; 50/330] START learning_rate=0.01, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 50/330] END learning_rate=0.01, max_depth=12, num_leaves=60;, score=0.904 total time=  26.4s\n",
      "[CV 3/5; 51/330] START learning_rate=0.01, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 51/330] END learning_rate=0.01, max_depth=12, num_leaves=62;, score=0.904 total time=  41.1s\n",
      "[CV 4/5; 53/330] START learning_rate=0.01, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 53/330] END learning_rate=0.01, max_depth=12, num_leaves=66;, score=0.904 total time=  43.8s\n",
      "[CV 4/5; 55/330] START learning_rate=0.01, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 55/330] END learning_rate=0.01, max_depth=12, num_leaves=70;, score=0.904 total time=  28.3s\n",
      "[CV 1/5; 57/330] START learning_rate=0.03, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 57/330] END learning_rate=0.03, max_depth=8, num_leaves=52;, score=0.909 total time=  26.7s\n",
      "[CV 3/5; 58/330] START learning_rate=0.03, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 58/330] END learning_rate=0.03, max_depth=8, num_leaves=54;, score=0.909 total time=  39.2s\n",
      "[CV 2/5; 60/330] START learning_rate=0.03, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 60/330] END learning_rate=0.03, max_depth=8, num_leaves=58;, score=0.909 total time=  27.6s\n",
      "[CV 4/5; 61/330] START learning_rate=0.03, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.368527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 61/330] END learning_rate=0.03, max_depth=8, num_leaves=60;, score=0.909 total time=  38.7s\n",
      "[CV 4/5; 63/330] START learning_rate=0.03, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 63/330] END learning_rate=0.03, max_depth=8, num_leaves=64;, score=0.909 total time=  25.9s\n",
      "[CV 2/5; 65/330] START learning_rate=0.03, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 65/330] END learning_rate=0.03, max_depth=8, num_leaves=68;, score=0.909 total time=  29.2s\n",
      "[CV 2/5; 66/330] START learning_rate=0.03, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 66/330] END learning_rate=0.03, max_depth=8, num_leaves=70;, score=0.909 total time=  26.6s\n",
      "[CV 5/5; 67/330] START learning_rate=0.03, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 67/330] END learning_rate=0.03, max_depth=9, num_leaves=50;, score=0.909 total time=  39.0s\n",
      "[CV 5/5; 69/330] START learning_rate=0.03, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 69/330] END learning_rate=0.03, max_depth=9, num_leaves=54;, score=0.909 total time=  25.4s\n",
      "[CV 5/5; 70/330] START learning_rate=0.03, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 70/330] END learning_rate=0.03, max_depth=9, num_leaves=56;, score=0.909 total time=  25.3s\n",
      "[CV 3/5; 72/330] START learning_rate=0.03, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 72/330] END learning_rate=0.03, max_depth=9, num_leaves=60;, score=0.909 total time=  26.0s\n",
      "[CV 1/5; 74/330] START learning_rate=0.03, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 37/330] END learning_rate=0.01, max_depth=11, num_leaves=56;, score=0.904 total time=  26.9s\n",
      "[CV 5/5; 38/330] START learning_rate=0.01, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 38/330] END learning_rate=0.01, max_depth=11, num_leaves=58;, score=0.904 total time=  40.4s\n",
      "[CV 3/5; 40/330] START learning_rate=0.01, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 40/330] END learning_rate=0.01, max_depth=11, num_leaves=62;, score=0.904 total time=  29.0s\n",
      "[CV 1/5; 42/330] START learning_rate=0.01, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 42/330] END learning_rate=0.01, max_depth=11, num_leaves=66;, score=0.904 total time=  46.6s\n",
      "[CV 1/5; 44/330] START learning_rate=0.01, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 44/330] END learning_rate=0.01, max_depth=11, num_leaves=70;, score=0.904 total time=  27.5s\n",
      "[CV 5/5; 44/330] START learning_rate=0.01, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 44/330] END learning_rate=0.01, max_depth=11, num_leaves=70;, score=0.904 total time=  29.3s\n",
      "[CV 2/5; 46/330] START learning_rate=0.01, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 46/330] END learning_rate=0.01, max_depth=12, num_leaves=52;, score=0.904 total time=  26.0s\n",
      "[CV 5/5; 47/330] START learning_rate=0.01, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 47/330] END learning_rate=0.01, max_depth=12, num_leaves=54;, score=0.904 total time=  40.0s\n",
      "[CV 4/5; 49/330] START learning_rate=0.01, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 49/330] END learning_rate=0.01, max_depth=12, num_leaves=58;, score=0.904 total time=  41.0s\n",
      "[CV 5/5; 51/330] START learning_rate=0.01, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 51/330] END learning_rate=0.01, max_depth=12, num_leaves=62;, score=0.904 total time=  26.2s\n",
      "[CV 2/5; 53/330] START learning_rate=0.01, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 53/330] END learning_rate=0.01, max_depth=12, num_leaves=66;, score=0.904 total time=  26.7s\n",
      "[CV 4/5; 54/330] START learning_rate=0.01, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.320974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 54/330] END learning_rate=0.01, max_depth=12, num_leaves=68;, score=0.904 total time=  43.5s\n",
      "[CV 3/5; 56/330] START learning_rate=0.03, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 56/330] END learning_rate=0.03, max_depth=8, num_leaves=50;, score=0.909 total time=  25.3s\n",
      "[CV 4/5; 57/330] START learning_rate=0.03, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 57/330] END learning_rate=0.03, max_depth=8, num_leaves=52;, score=0.909 total time=  39.6s\n",
      "[CV 5/5; 59/330] START learning_rate=0.03, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 59/330] END learning_rate=0.03, max_depth=8, num_leaves=56;, score=0.909 total time=  40.0s\n",
      "[CV 5/5; 61/330] START learning_rate=0.03, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 61/330] END learning_rate=0.03, max_depth=8, num_leaves=60;, score=0.909 total time=  26.8s\n",
      "[CV 2/5; 63/330] START learning_rate=0.03, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 63/330] END learning_rate=0.03, max_depth=8, num_leaves=64;, score=0.909 total time=  27.3s\n",
      "[CV 4/5; 64/330] START learning_rate=0.03, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 64/330] END learning_rate=0.03, max_depth=8, num_leaves=66;, score=0.909 total time=  42.1s\n",
      "[CV 4/5; 66/330] START learning_rate=0.03, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 66/330] END learning_rate=0.03, max_depth=8, num_leaves=70;, score=0.909 total time=  27.9s\n",
      "[CV 2/5; 68/330] START learning_rate=0.03, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 68/330] END learning_rate=0.03, max_depth=9, num_leaves=52;, score=0.909 total time=  26.3s\n",
      "[CV 4/5; 69/330] START learning_rate=0.03, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 69/330] END learning_rate=0.03, max_depth=9, num_leaves=54;, score=0.909 total time=  39.7s\n",
      "[CV 3/5; 71/330] START learning_rate=0.03, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 71/330] END learning_rate=0.03, max_depth=9, num_leaves=58;, score=0.909 total time=  26.4s\n",
      "[CV 1/5; 73/330] START learning_rate=0.03, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 73/330] END learning_rate=0.03, max_depth=9, num_leaves=62;, score=0.909 total time=  26.5s\n",
      "[CV 3/5; 74/330] START learning_rate=0.03, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 36/330] END learning_rate=0.01, max_depth=11, num_leaves=54;, score=0.904 total time=  26.2s\n",
      "[CV 4/5; 37/330] START learning_rate=0.01, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 37/330] END learning_rate=0.01, max_depth=11, num_leaves=56;, score=0.904 total time=  41.2s\n",
      "[CV 4/5; 39/330] START learning_rate=0.01, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 39/330] END learning_rate=0.01, max_depth=11, num_leaves=60;, score=0.904 total time=  42.3s\n",
      "[CV 3/5; 41/330] START learning_rate=0.01, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 41/330] END learning_rate=0.01, max_depth=11, num_leaves=64;, score=0.904 total time=  45.4s\n",
      "[CV 2/5; 43/330] START learning_rate=0.01, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 43/330] END learning_rate=0.01, max_depth=11, num_leaves=68;, score=0.904 total time=  46.2s\n",
      "[CV 4/5; 45/330] START learning_rate=0.01, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 45/330] END learning_rate=0.01, max_depth=12, num_leaves=50;, score=0.904 total time=  38.7s\n",
      "[CV 3/5; 47/330] START learning_rate=0.01, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 47/330] END learning_rate=0.01, max_depth=12, num_leaves=54;, score=0.904 total time=  39.3s\n",
      "[CV 2/5; 49/330] START learning_rate=0.01, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 49/330] END learning_rate=0.01, max_depth=12, num_leaves=58;, score=0.904 total time=  27.2s\n",
      "[CV 5/5; 50/330] START learning_rate=0.01, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 50/330] END learning_rate=0.01, max_depth=12, num_leaves=60;, score=0.904 total time=  41.1s\n",
      "[CV 5/5; 52/330] START learning_rate=0.01, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 52/330] END learning_rate=0.01, max_depth=12, num_leaves=64;, score=0.904 total time=  25.9s\n",
      "[CV 2/5; 54/330] START learning_rate=0.01, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 54/330] END learning_rate=0.01, max_depth=12, num_leaves=68;, score=0.904 total time=  28.0s\n",
      "[CV 3/5; 55/330] START learning_rate=0.01, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 55/330] END learning_rate=0.01, max_depth=12, num_leaves=70;, score=0.904 total time=  44.4s\n",
      "[CV 2/5; 57/330] START learning_rate=0.03, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 57/330] END learning_rate=0.03, max_depth=8, num_leaves=52;, score=0.909 total time=  26.4s\n",
      "[CV 5/5; 58/330] START learning_rate=0.03, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 58/330] END learning_rate=0.03, max_depth=8, num_leaves=54;, score=0.909 total time=  25.7s\n",
      "[CV 1/5; 60/330] START learning_rate=0.03, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 60/330] END learning_rate=0.03, max_depth=8, num_leaves=58;, score=0.909 total time=  27.6s\n",
      "[CV 3/5; 61/330] START learning_rate=0.03, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 61/330] END learning_rate=0.03, max_depth=8, num_leaves=60;, score=0.909 total time=  26.5s\n",
      "[CV 1/5; 63/330] START learning_rate=0.03, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 63/330] END learning_rate=0.03, max_depth=8, num_leaves=64;, score=0.909 total time=  26.8s\n",
      "[CV 3/5; 64/330] START learning_rate=0.03, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 64/330] END learning_rate=0.03, max_depth=8, num_leaves=66;, score=0.909 total time=  42.3s\n",
      "[CV 3/5; 66/330] START learning_rate=0.03, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 66/330] END learning_rate=0.03, max_depth=8, num_leaves=70;, score=0.909 total time=  26.6s\n",
      "[CV 1/5; 68/330] START learning_rate=0.03, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 68/330] END learning_rate=0.03, max_depth=9, num_leaves=52;, score=0.909 total time=  25.9s\n",
      "[CV 2/5; 69/330] START learning_rate=0.03, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 69/330] END learning_rate=0.03, max_depth=9, num_leaves=54;, score=0.909 total time=  27.1s\n",
      "[CV 3/5; 70/330] START learning_rate=0.03, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 70/330] END learning_rate=0.03, max_depth=9, num_leaves=56;, score=0.909 total time=  26.7s\n",
      "[CV 1/5; 72/330] START learning_rate=0.03, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 72/330] END learning_rate=0.03, max_depth=9, num_leaves=60;, score=0.909 total time=  27.7s\n",
      "[CV 4/5; 73/330] START learning_rate=0.03, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[CV 2/5; 40/330] START learning_rate=0.01, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 40/330] END learning_rate=0.01, max_depth=11, num_leaves=62;, score=0.904 total time=  28.5s\n",
      "[CV 4/5; 41/330] START learning_rate=0.01, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 41/330] END learning_rate=0.01, max_depth=11, num_leaves=64;, score=0.904 total time=  44.7s\n",
      "[CV 4/5; 43/330] START learning_rate=0.01, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 43/330] END learning_rate=0.01, max_depth=11, num_leaves=68;, score=0.904 total time=  43.7s\n",
      "[CV 3/5; 45/330] START learning_rate=0.01, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 45/330] END learning_rate=0.01, max_depth=12, num_leaves=50;, score=0.904 total time=  38.8s\n",
      "[CV 2/5; 47/330] START learning_rate=0.01, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 47/330] END learning_rate=0.01, max_depth=12, num_leaves=54;, score=0.904 total time=  25.1s\n",
      "[CV 3/5; 48/330] START learning_rate=0.01, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 48/330] END learning_rate=0.01, max_depth=12, num_leaves=56;, score=0.904 total time=  39.5s\n",
      "[CV 3/5; 50/330] START learning_rate=0.01, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 50/330] END learning_rate=0.01, max_depth=12, num_leaves=60;, score=0.904 total time=  26.2s\n",
      "[CV 1/5; 52/330] START learning_rate=0.01, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 52/330] END learning_rate=0.01, max_depth=12, num_leaves=64;, score=0.904 total time=  27.2s\n",
      "[CV 3/5; 53/330] START learning_rate=0.01, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 53/330] END learning_rate=0.01, max_depth=12, num_leaves=66;, score=0.904 total time=  27.8s\n",
      "[CV 5/5; 54/330] START learning_rate=0.01, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 54/330] END learning_rate=0.01, max_depth=12, num_leaves=68;, score=0.904 total time=  44.6s\n",
      "[CV 4/5; 56/330] START learning_rate=0.03, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 56/330] END learning_rate=0.03, max_depth=8, num_leaves=50;, score=0.909 total time=  25.4s\n",
      "[CV 5/5; 57/330] START learning_rate=0.03, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 57/330] END learning_rate=0.03, max_depth=8, num_leaves=52;, score=0.909 total time=  25.9s\n",
      "[CV 1/5; 59/330] START learning_rate=0.03, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 59/330] END learning_rate=0.03, max_depth=8, num_leaves=56;, score=0.909 total time=  26.3s\n",
      "[CV 3/5; 60/330] START learning_rate=0.03, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 60/330] END learning_rate=0.03, max_depth=8, num_leaves=58;, score=0.909 total time=  40.4s\n",
      "[CV 4/5; 62/330] START learning_rate=0.03, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 62/330] END learning_rate=0.03, max_depth=8, num_leaves=62;, score=0.909 total time=  39.2s\n",
      "[CV 5/5; 64/330] START learning_rate=0.03, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 64/330] END learning_rate=0.03, max_depth=8, num_leaves=66;, score=0.909 total time=  42.1s\n",
      "[CV 5/5; 66/330] START learning_rate=0.03, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 66/330] END learning_rate=0.03, max_depth=8, num_leaves=70;, score=0.909 total time=  26.7s\n",
      "[CV 3/5; 68/330] START learning_rate=0.03, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 68/330] END learning_rate=0.03, max_depth=9, num_leaves=52;, score=0.909 total time=  39.0s\n",
      "[CV 1/5; 70/330] START learning_rate=0.03, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 70/330] END learning_rate=0.03, max_depth=9, num_leaves=56;, score=0.909 total time=  26.1s\n",
      "[CV 4/5; 71/330] START learning_rate=0.03, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 71/330] END learning_rate=0.03, max_depth=9, num_leaves=58;, score=0.909 total time=  26.7s\n",
      "[CV 2/5; 73/330] START learning_rate=0.03, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 73/330] END learning_rate=0.03, max_depth=9, num_leaves=62;, score=0.909 total time=  26.4s\n",
      "[CV 4/5; 74/330] START learning_rate=0.03, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 74/330] END learning_rate=0.03, max_depth=9, num_leaves=64;, score=0.909 total time=  41.5s\n",
      "[CV 4/5; 76/330] START learning_rate=0.03, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 71/330] END learning_rate=0.03, max_depth=9, num_leaves=58;, score=0.909 total time=  27.3s\n",
      "[CV 3/5; 73/330] START learning_rate=0.03, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 73/330] END learning_rate=0.03, max_depth=9, num_leaves=62;, score=0.909 total time=  27.7s\n",
      "[CV 5/5; 74/330] START learning_rate=0.03, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 74/330] END learning_rate=0.03, max_depth=9, num_leaves=64;, score=0.909 total time=  26.9s\n",
      "[CV 2/5; 76/330] START learning_rate=0.03, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 76/330] END learning_rate=0.03, max_depth=9, num_leaves=68;, score=0.909 total time=  28.0s\n",
      "[CV 3/5; 77/330] START learning_rate=0.03, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.340486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 77/330] END learning_rate=0.03, max_depth=9, num_leaves=70;, score=0.909 total time=  44.2s\n",
      "[CV 1/5; 79/330] START learning_rate=0.03, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 79/330] END learning_rate=0.03, max_depth=10, num_leaves=52;, score=0.909 total time=  26.1s\n",
      "[CV 4/5; 80/330] START learning_rate=0.03, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 80/330] END learning_rate=0.03, max_depth=10, num_leaves=54;, score=0.909 total time=  26.2s\n",
      "[CV 1/5; 82/330] START learning_rate=0.03, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 82/330] END learning_rate=0.03, max_depth=10, num_leaves=58;, score=0.909 total time=  25.8s\n",
      "[CV 3/5; 83/330] START learning_rate=0.03, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 83/330] END learning_rate=0.03, max_depth=10, num_leaves=60;, score=0.909 total time=  42.3s\n",
      "[CV 4/5; 85/330] START learning_rate=0.03, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 85/330] END learning_rate=0.03, max_depth=10, num_leaves=64;, score=0.909 total time=  28.6s\n",
      "[CV 2/5; 86/330] START learning_rate=0.03, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.179139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 86/330] END learning_rate=0.03, max_depth=10, num_leaves=66;, score=0.909 total time=  28.6s\n",
      "[CV 4/5; 87/330] START learning_rate=0.03, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 87/330] END learning_rate=0.03, max_depth=10, num_leaves=68;, score=0.909 total time=  44.4s\n",
      "[CV 4/5; 89/330] START learning_rate=0.03, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 89/330] END learning_rate=0.03, max_depth=11, num_leaves=50;, score=0.909 total time=  40.1s\n",
      "[CV 4/5; 91/330] START learning_rate=0.03, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 91/330] END learning_rate=0.03, max_depth=11, num_leaves=54;, score=0.909 total time=  26.9s\n",
      "[CV 2/5; 93/330] START learning_rate=0.03, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 93/330] END learning_rate=0.03, max_depth=11, num_leaves=58;, score=0.909 total time=  28.6s\n",
      "[CV 5/5; 93/330] START learning_rate=0.03, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 93/330] END learning_rate=0.03, max_depth=11, num_leaves=58;, score=0.909 total time=  27.2s\n",
      "[CV 1/5; 95/330] START learning_rate=0.03, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 95/330] END learning_rate=0.03, max_depth=11, num_leaves=62;, score=0.909 total time=  27.6s\n",
      "[CV 3/5; 96/330] START learning_rate=0.03, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 96/330] END learning_rate=0.03, max_depth=11, num_leaves=64;, score=0.909 total time=  43.8s\n",
      "[CV 4/5; 98/330] START learning_rate=0.03, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 98/330] END learning_rate=0.03, max_depth=11, num_leaves=68;, score=0.909 total time=  45.8s\n",
      "[CV 2/5; 100/330] START learning_rate=0.03, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 100/330] END learning_rate=0.03, max_depth=12, num_leaves=50;, score=0.909 total time=  41.9s\n",
      "[CV 5/5; 101/330] START learning_rate=0.03, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.195820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 101/330] END learning_rate=0.03, max_depth=12, num_leaves=52;, score=0.909 total time=  39.2s\n",
      "[CV 1/5; 104/330] START learning_rate=0.03, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 104/330] END learning_rate=0.03, max_depth=12, num_leaves=58;, score=0.909 total time=  27.7s\n",
      "[CV 2/5; 105/330] START learning_rate=0.03, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 72/330] END learning_rate=0.03, max_depth=9, num_leaves=60;, score=0.909 total time=  26.8s\n",
      "[CV 5/5; 73/330] START learning_rate=0.03, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 73/330] END learning_rate=0.03, max_depth=9, num_leaves=62;, score=0.909 total time=  41.7s\n",
      "[CV 1/5; 76/330] START learning_rate=0.03, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 76/330] END learning_rate=0.03, max_depth=9, num_leaves=68;, score=0.909 total time=  28.7s\n",
      "[CV 2/5; 77/330] START learning_rate=0.03, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.205726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 77/330] END learning_rate=0.03, max_depth=9, num_leaves=70;, score=0.909 total time=  46.8s\n",
      "[CV 2/5; 79/330] START learning_rate=0.03, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 79/330] END learning_rate=0.03, max_depth=10, num_leaves=52;, score=0.909 total time=  25.7s\n",
      "[CV 5/5; 80/330] START learning_rate=0.03, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 80/330] END learning_rate=0.03, max_depth=10, num_leaves=54;, score=0.909 total time=  26.4s\n",
      "[CV 2/5; 82/330] START learning_rate=0.03, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 82/330] END learning_rate=0.03, max_depth=10, num_leaves=58;, score=0.909 total time=  26.5s\n",
      "[CV 4/5; 83/330] START learning_rate=0.03, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 83/330] END learning_rate=0.03, max_depth=10, num_leaves=60;, score=0.909 total time=  26.9s\n",
      "[CV 3/5; 84/330] START learning_rate=0.03, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 84/330] END learning_rate=0.03, max_depth=10, num_leaves=62;, score=0.909 total time=  28.3s\n",
      "[CV 1/5; 86/330] START learning_rate=0.03, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.196462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 86/330] END learning_rate=0.03, max_depth=10, num_leaves=66;, score=0.909 total time=  45.3s\n",
      "[CV 5/5; 87/330] START learning_rate=0.03, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 87/330] END learning_rate=0.03, max_depth=10, num_leaves=68;, score=0.909 total time=  44.1s\n",
      "[CV 5/5; 89/330] START learning_rate=0.03, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.298040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 89/330] END learning_rate=0.03, max_depth=11, num_leaves=50;, score=0.909 total time=  26.3s\n",
      "[CV 1/5; 91/330] START learning_rate=0.03, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 91/330] END learning_rate=0.03, max_depth=11, num_leaves=54;, score=0.909 total time=  27.2s\n",
      "[CV 1/5; 92/330] START learning_rate=0.03, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 92/330] END learning_rate=0.03, max_depth=11, num_leaves=56;, score=0.909 total time=  28.7s\n",
      "[CV 4/5; 93/330] START learning_rate=0.03, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144530 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 93/330] END learning_rate=0.03, max_depth=11, num_leaves=58;, score=0.909 total time=  41.9s\n",
      "[CV 3/5; 95/330] START learning_rate=0.03, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 95/330] END learning_rate=0.03, max_depth=11, num_leaves=62;, score=0.909 total time=  28.1s\n",
      "[CV 1/5; 97/330] START learning_rate=0.03, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 97/330] END learning_rate=0.03, max_depth=11, num_leaves=66;, score=0.909 total time=  28.1s\n",
      "[CV 2/5; 98/330] START learning_rate=0.03, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 98/330] END learning_rate=0.03, max_depth=11, num_leaves=68;, score=0.909 total time=  48.6s\n",
      "[CV 5/5; 99/330] START learning_rate=0.03, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 99/330] END learning_rate=0.03, max_depth=11, num_leaves=70;, score=0.909 total time=  47.0s\n",
      "[CV 4/5; 101/330] START learning_rate=0.03, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 101/330] END learning_rate=0.03, max_depth=12, num_leaves=52;, score=0.909 total time=  25.5s\n",
      "[CV 5/5; 102/330] START learning_rate=0.03, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 102/330] END learning_rate=0.03, max_depth=12, num_leaves=54;, score=0.909 total time=  25.9s\n",
      "[CV 2/5; 104/330] START learning_rate=0.03, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 104/330] END learning_rate=0.03, max_depth=12, num_leaves=58;, score=0.909 total time=  41.9s\n",
      "[CV 4/5; 106/330] START learning_rate=0.03, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 74/330] END learning_rate=0.03, max_depth=9, num_leaves=64;, score=0.909 total time=  27.4s\n",
      "[CV 3/5; 75/330] START learning_rate=0.03, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 75/330] END learning_rate=0.03, max_depth=9, num_leaves=66;, score=0.909 total time=  28.5s\n",
      "[CV 5/5; 76/330] START learning_rate=0.03, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 76/330] END learning_rate=0.03, max_depth=9, num_leaves=68;, score=0.909 total time=  28.8s\n",
      "[CV 2/5; 78/330] START learning_rate=0.03, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.195870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 78/330] END learning_rate=0.03, max_depth=10, num_leaves=50;, score=0.909 total time=  40.5s\n",
      "[CV 5/5; 79/330] START learning_rate=0.03, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 79/330] END learning_rate=0.03, max_depth=10, num_leaves=52;, score=0.909 total time=  39.8s\n",
      "[CV 3/5; 82/330] START learning_rate=0.03, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 82/330] END learning_rate=0.03, max_depth=10, num_leaves=58;, score=0.909 total time=  27.9s\n",
      "[CV 5/5; 83/330] START learning_rate=0.03, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 83/330] END learning_rate=0.03, max_depth=10, num_leaves=60;, score=0.909 total time=  26.8s\n",
      "[CV 4/5; 84/330] START learning_rate=0.03, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 84/330] END learning_rate=0.03, max_depth=10, num_leaves=62;, score=0.909 total time=  43.5s\n",
      "[CV 3/5; 86/330] START learning_rate=0.03, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.407822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 86/330] END learning_rate=0.03, max_depth=10, num_leaves=66;, score=0.909 total time=  42.8s\n",
      "[CV 3/5; 88/330] START learning_rate=0.03, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.168952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 88/330] END learning_rate=0.03, max_depth=10, num_leaves=70;, score=0.909 total time=  30.4s\n",
      "[CV 1/5; 90/330] START learning_rate=0.03, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 90/330] END learning_rate=0.03, max_depth=11, num_leaves=52;, score=0.909 total time=  26.8s\n",
      "[CV 3/5; 91/330] START learning_rate=0.03, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 91/330] END learning_rate=0.03, max_depth=11, num_leaves=54;, score=0.909 total time=  26.6s\n",
      "[CV 3/5; 92/330] START learning_rate=0.03, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 92/330] END learning_rate=0.03, max_depth=11, num_leaves=56;, score=0.909 total time=  42.7s\n",
      "[CV 1/5; 94/330] START learning_rate=0.03, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 94/330] END learning_rate=0.03, max_depth=11, num_leaves=60;, score=0.909 total time=  27.1s\n",
      "[CV 4/5; 95/330] START learning_rate=0.03, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 95/330] END learning_rate=0.03, max_depth=11, num_leaves=62;, score=0.909 total time=  28.1s\n",
      "[CV 2/5; 97/330] START learning_rate=0.03, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 97/330] END learning_rate=0.03, max_depth=11, num_leaves=66;, score=0.909 total time=  45.9s\n",
      "[CV 5/5; 98/330] START learning_rate=0.03, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 98/330] END learning_rate=0.03, max_depth=11, num_leaves=68;, score=0.909 total time=  45.8s\n",
      "[CV 4/5; 100/330] START learning_rate=0.03, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 100/330] END learning_rate=0.03, max_depth=12, num_leaves=50;, score=0.909 total time=  40.1s\n",
      "[CV 2/5; 102/330] START learning_rate=0.03, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 102/330] END learning_rate=0.03, max_depth=12, num_leaves=54;, score=0.909 total time=  26.2s\n",
      "[CV 5/5; 103/330] START learning_rate=0.03, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 103/330] END learning_rate=0.03, max_depth=12, num_leaves=56;, score=0.909 total time=  40.3s\n",
      "[CV 4/5; 105/330] START learning_rate=0.03, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.196777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 105/330] END learning_rate=0.03, max_depth=12, num_leaves=60;, score=0.909 total time=  42.2s\n",
      "[CV 5/5; 107/330] START learning_rate=0.03, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 107/330] END learning_rate=0.03, max_depth=12, num_leaves=64;, score=0.909 total time=  43.9s\n",
      "[CV 5/5; 109/330] START learning_rate=0.03, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 109/330] END learning_rate=0.03, max_depth=12, num_leaves=68;, score=0.909 total time=  30.0s\n",
      "[CV 2/5; 111/330] START learning_rate=0.05, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.359486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 73/330] END learning_rate=0.03, max_depth=9, num_leaves=62;, score=0.909 total time=  41.8s\n",
      "[CV 4/5; 75/330] START learning_rate=0.03, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.325253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 75/330] END learning_rate=0.03, max_depth=9, num_leaves=66;, score=0.909 total time=  43.2s\n",
      "[CV 1/5; 78/330] START learning_rate=0.03, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 78/330] END learning_rate=0.03, max_depth=10, num_leaves=50;, score=0.909 total time=  27.0s\n",
      "[CV 5/5; 78/330] START learning_rate=0.03, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 78/330] END learning_rate=0.03, max_depth=10, num_leaves=50;, score=0.909 total time=  25.6s\n",
      "[CV 1/5; 80/330] START learning_rate=0.03, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 80/330] END learning_rate=0.03, max_depth=10, num_leaves=54;, score=0.909 total time=  26.6s\n",
      "[CV 3/5; 81/330] START learning_rate=0.03, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 81/330] END learning_rate=0.03, max_depth=10, num_leaves=56;, score=0.909 total time=  26.8s\n",
      "[CV 5/5; 82/330] START learning_rate=0.03, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 82/330] END learning_rate=0.03, max_depth=10, num_leaves=58;, score=0.909 total time=  41.1s\n",
      "[CV 1/5; 85/330] START learning_rate=0.03, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 85/330] END learning_rate=0.03, max_depth=10, num_leaves=64;, score=0.909 total time=  45.3s\n",
      "[CV 5/5; 86/330] START learning_rate=0.03, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 86/330] END learning_rate=0.03, max_depth=10, num_leaves=66;, score=0.909 total time=  44.4s\n",
      "[CV 4/5; 88/330] START learning_rate=0.03, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 88/330] END learning_rate=0.03, max_depth=10, num_leaves=70;, score=0.909 total time=  44.0s\n",
      "[CV 4/5; 90/330] START learning_rate=0.03, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 90/330] END learning_rate=0.03, max_depth=11, num_leaves=52;, score=0.909 total time=  40.1s\n",
      "[CV 5/5; 92/330] START learning_rate=0.03, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 92/330] END learning_rate=0.03, max_depth=11, num_leaves=56;, score=0.909 total time=  42.7s\n",
      "[CV 4/5; 94/330] START learning_rate=0.03, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 94/330] END learning_rate=0.03, max_depth=11, num_leaves=60;, score=0.909 total time=  27.2s\n",
      "[CV 5/5; 95/330] START learning_rate=0.03, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 95/330] END learning_rate=0.03, max_depth=11, num_leaves=62;, score=0.909 total time=  41.8s\n",
      "[CV 4/5; 97/330] START learning_rate=0.03, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.311903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 97/330] END learning_rate=0.03, max_depth=11, num_leaves=66;, score=0.909 total time=  46.4s\n",
      "[CV 2/5; 99/330] START learning_rate=0.03, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 99/330] END learning_rate=0.03, max_depth=11, num_leaves=70;, score=0.909 total time=  46.9s\n",
      "[CV 2/5; 101/330] START learning_rate=0.03, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 101/330] END learning_rate=0.03, max_depth=12, num_leaves=52;, score=0.909 total time=  26.2s\n",
      "[CV 4/5; 102/330] START learning_rate=0.03, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 102/330] END learning_rate=0.03, max_depth=12, num_leaves=54;, score=0.909 total time=  40.7s\n",
      "[CV 4/5; 104/330] START learning_rate=0.03, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 104/330] END learning_rate=0.03, max_depth=12, num_leaves=58;, score=0.909 total time=  26.9s\n",
      "[CV 1/5; 106/330] START learning_rate=0.03, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 106/330] END learning_rate=0.03, max_depth=12, num_leaves=62;, score=0.909 total time=  26.7s\n",
      "[CV 5/5; 106/330] START learning_rate=0.03, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 106/330] END learning_rate=0.03, max_depth=12, num_leaves=62;, score=0.909 total time=  27.2s\n",
      "[CV 3/5; 108/330] START learning_rate=0.03, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.206928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 108/330] END learning_rate=0.03, max_depth=12, num_leaves=66;, score=0.909 total time=  43.5s\n",
      "[CV 2/5; 110/330] START learning_rate=0.03, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 110/330] END learning_rate=0.03, max_depth=12, num_leaves=70;, score=0.909 total time=  27.3s\n",
      "[CV 5/5; 111/330] START learning_rate=0.05, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 74/330] END learning_rate=0.03, max_depth=9, num_leaves=64;, score=0.909 total time=  27.1s\n",
      "[CV 5/5; 75/330] START learning_rate=0.03, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 75/330] END learning_rate=0.03, max_depth=9, num_leaves=66;, score=0.909 total time=  28.6s\n",
      "[CV 1/5; 77/330] START learning_rate=0.03, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 77/330] END learning_rate=0.03, max_depth=9, num_leaves=70;, score=0.909 total time=  29.8s\n",
      "[CV 3/5; 78/330] START learning_rate=0.03, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 78/330] END learning_rate=0.03, max_depth=10, num_leaves=50;, score=0.909 total time=  39.3s\n",
      "[CV 3/5; 80/330] START learning_rate=0.03, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 80/330] END learning_rate=0.03, max_depth=10, num_leaves=54;, score=0.909 total time=  26.6s\n",
      "[CV 5/5; 81/330] START learning_rate=0.03, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 81/330] END learning_rate=0.03, max_depth=10, num_leaves=56;, score=0.909 total time=  26.7s\n",
      "[CV 2/5; 83/330] START learning_rate=0.03, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 83/330] END learning_rate=0.03, max_depth=10, num_leaves=60;, score=0.909 total time=  43.3s\n",
      "[CV 3/5; 85/330] START learning_rate=0.03, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 85/330] END learning_rate=0.03, max_depth=10, num_leaves=64;, score=0.909 total time=  43.7s\n",
      "[CV 2/5; 87/330] START learning_rate=0.03, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 87/330] END learning_rate=0.03, max_depth=10, num_leaves=68;, score=0.909 total time=  45.6s\n",
      "[CV 1/5; 89/330] START learning_rate=0.03, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 89/330] END learning_rate=0.03, max_depth=11, num_leaves=50;, score=0.909 total time=  26.8s\n",
      "[CV 2/5; 90/330] START learning_rate=0.03, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 90/330] END learning_rate=0.03, max_depth=11, num_leaves=52;, score=0.909 total time=  28.1s\n",
      "[CV 5/5; 91/330] START learning_rate=0.03, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 91/330] END learning_rate=0.03, max_depth=11, num_leaves=54;, score=0.909 total time=  41.0s\n",
      "[CV 3/5; 93/330] START learning_rate=0.03, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 93/330] END learning_rate=0.03, max_depth=11, num_leaves=58;, score=0.909 total time=  42.9s\n",
      "[CV 2/5; 95/330] START learning_rate=0.03, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 95/330] END learning_rate=0.03, max_depth=11, num_leaves=62;, score=0.909 total time=  27.3s\n",
      "[CV 5/5; 96/330] START learning_rate=0.03, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 96/330] END learning_rate=0.03, max_depth=11, num_leaves=64;, score=0.909 total time=  27.8s\n",
      "[CV 1/5; 98/330] START learning_rate=0.03, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 98/330] END learning_rate=0.03, max_depth=11, num_leaves=68;, score=0.909 total time=  47.9s\n",
      "[CV 4/5; 99/330] START learning_rate=0.03, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 99/330] END learning_rate=0.03, max_depth=11, num_leaves=70;, score=0.909 total time=  28.7s\n",
      "[CV 5/5; 100/330] START learning_rate=0.03, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 100/330] END learning_rate=0.03, max_depth=12, num_leaves=50;, score=0.909 total time=  26.4s\n",
      "[CV 1/5; 102/330] START learning_rate=0.03, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 102/330] END learning_rate=0.03, max_depth=12, num_leaves=54;, score=0.909 total time=  26.8s\n",
      "[CV 2/5; 103/330] START learning_rate=0.03, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 103/330] END learning_rate=0.03, max_depth=12, num_leaves=56;, score=0.909 total time=  26.7s\n",
      "[CV 5/5; 104/330] START learning_rate=0.03, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 104/330] END learning_rate=0.03, max_depth=12, num_leaves=58;, score=0.909 total time=  27.0s\n",
      "[CV 2/5; 106/330] START learning_rate=0.03, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 106/330] END learning_rate=0.03, max_depth=12, num_leaves=62;, score=0.909 total time=  27.0s\n",
      "[CV 2/5; 107/330] START learning_rate=0.03, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 107/330] END learning_rate=0.03, max_depth=12, num_leaves=64;, score=0.909 total time=  43.6s\n",
      "[CV 3/5; 109/330] START learning_rate=0.03, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 72/330] END learning_rate=0.03, max_depth=9, num_leaves=60;, score=0.909 total time=  39.8s\n",
      "[CV 1/5; 75/330] START learning_rate=0.03, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 75/330] END learning_rate=0.03, max_depth=9, num_leaves=66;, score=0.909 total time=  28.5s\n",
      "[CV 3/5; 76/330] START learning_rate=0.03, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 76/330] END learning_rate=0.03, max_depth=9, num_leaves=68;, score=0.909 total time=  28.8s\n",
      "[CV 4/5; 77/330] START learning_rate=0.03, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 77/330] END learning_rate=0.03, max_depth=9, num_leaves=70;, score=0.909 total time=  44.4s\n",
      "[CV 3/5; 79/330] START learning_rate=0.03, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 79/330] END learning_rate=0.03, max_depth=10, num_leaves=52;, score=0.909 total time=  25.4s\n",
      "[CV 1/5; 81/330] START learning_rate=0.03, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 81/330] END learning_rate=0.03, max_depth=10, num_leaves=56;, score=0.909 total time=  41.3s\n",
      "[CV 4/5; 82/330] START learning_rate=0.03, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 82/330] END learning_rate=0.03, max_depth=10, num_leaves=58;, score=0.909 total time=  41.5s\n",
      "[CV 5/5; 84/330] START learning_rate=0.03, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 84/330] END learning_rate=0.03, max_depth=10, num_leaves=62;, score=0.909 total time=  43.4s\n",
      "[CV 4/5; 86/330] START learning_rate=0.03, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 86/330] END learning_rate=0.03, max_depth=10, num_leaves=66;, score=0.909 total time=  28.3s\n",
      "[CV 1/5; 88/330] START learning_rate=0.03, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 88/330] END learning_rate=0.03, max_depth=10, num_leaves=70;, score=0.909 total time=  29.0s\n",
      "[CV 2/5; 89/330] START learning_rate=0.03, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 89/330] END learning_rate=0.03, max_depth=11, num_leaves=50;, score=0.909 total time=  41.2s\n",
      "[CV 2/5; 91/330] START learning_rate=0.03, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 91/330] END learning_rate=0.03, max_depth=11, num_leaves=54;, score=0.909 total time=  26.7s\n",
      "[CV 2/5; 92/330] START learning_rate=0.03, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 92/330] END learning_rate=0.03, max_depth=11, num_leaves=56;, score=0.909 total time=  43.7s\n",
      "[CV 2/5; 94/330] START learning_rate=0.03, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.166239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 94/330] END learning_rate=0.03, max_depth=11, num_leaves=60;, score=0.909 total time=  42.9s\n",
      "[CV 1/5; 96/330] START learning_rate=0.03, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 96/330] END learning_rate=0.03, max_depth=11, num_leaves=64;, score=0.909 total time=  28.1s\n",
      "[CV 3/5; 97/330] START learning_rate=0.03, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 97/330] END learning_rate=0.03, max_depth=11, num_leaves=66;, score=0.909 total time=  45.5s\n",
      "[CV 1/5; 99/330] START learning_rate=0.03, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.327296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 99/330] END learning_rate=0.03, max_depth=11, num_leaves=70;, score=0.909 total time=  29.5s\n",
      "[CV 3/5; 100/330] START learning_rate=0.03, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.345644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 100/330] END learning_rate=0.03, max_depth=12, num_leaves=50;, score=0.909 total time=  40.9s\n",
      "[CV 3/5; 102/330] START learning_rate=0.03, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 102/330] END learning_rate=0.03, max_depth=12, num_leaves=54;, score=0.909 total time=  25.8s\n",
      "[CV 4/5; 103/330] START learning_rate=0.03, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 103/330] END learning_rate=0.03, max_depth=12, num_leaves=56;, score=0.909 total time=  39.4s\n",
      "[CV 3/5; 105/330] START learning_rate=0.03, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 105/330] END learning_rate=0.03, max_depth=12, num_leaves=60;, score=0.909 total time=  42.1s\n",
      "[CV 4/5; 107/330] START learning_rate=0.03, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 107/330] END learning_rate=0.03, max_depth=12, num_leaves=64;, score=0.909 total time=  27.9s\n",
      "[CV 5/5; 108/330] START learning_rate=0.03, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.335328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 108/330] END learning_rate=0.03, max_depth=12, num_leaves=66;, score=0.909 total time=  45.3s\n",
      "[CV 1/5; 111/330] START learning_rate=0.05, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 74/330] END learning_rate=0.03, max_depth=9, num_leaves=64;, score=0.909 total time=  27.5s\n",
      "[CV 2/5; 75/330] START learning_rate=0.03, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.376859 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 75/330] END learning_rate=0.03, max_depth=9, num_leaves=66;, score=0.909 total time=  45.0s\n",
      "[CV 5/5; 77/330] START learning_rate=0.03, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 77/330] END learning_rate=0.03, max_depth=9, num_leaves=70;, score=0.909 total time=  45.6s\n",
      "[CV 4/5; 79/330] START learning_rate=0.03, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 79/330] END learning_rate=0.03, max_depth=10, num_leaves=52;, score=0.909 total time=  26.2s\n",
      "[CV 2/5; 81/330] START learning_rate=0.03, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 81/330] END learning_rate=0.03, max_depth=10, num_leaves=56;, score=0.909 total time=  41.3s\n",
      "[CV 1/5; 83/330] START learning_rate=0.03, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 83/330] END learning_rate=0.03, max_depth=10, num_leaves=60;, score=0.909 total time=  27.3s\n",
      "[CV 2/5; 84/330] START learning_rate=0.03, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.255871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 84/330] END learning_rate=0.03, max_depth=10, num_leaves=62;, score=0.909 total time=  28.6s\n",
      "[CV 5/5; 85/330] START learning_rate=0.03, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 85/330] END learning_rate=0.03, max_depth=10, num_leaves=64;, score=0.909 total time=  44.5s\n",
      "[CV 3/5; 87/330] START learning_rate=0.03, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 87/330] END learning_rate=0.03, max_depth=10, num_leaves=68;, score=0.909 total time=  29.2s\n",
      "[CV 5/5; 88/330] START learning_rate=0.03, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 88/330] END learning_rate=0.03, max_depth=10, num_leaves=70;, score=0.909 total time=  45.1s\n",
      "[CV 5/5; 90/330] START learning_rate=0.03, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 90/330] END learning_rate=0.03, max_depth=11, num_leaves=52;, score=0.909 total time=  39.8s\n",
      "[CV 1/5; 93/330] START learning_rate=0.03, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.206560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 93/330] END learning_rate=0.03, max_depth=11, num_leaves=58;, score=0.909 total time=  44.2s\n",
      "[CV 5/5; 94/330] START learning_rate=0.03, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 94/330] END learning_rate=0.03, max_depth=11, num_leaves=60;, score=0.909 total time=  42.6s\n",
      "[CV 4/5; 96/330] START learning_rate=0.03, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 96/330] END learning_rate=0.03, max_depth=11, num_leaves=64;, score=0.909 total time=  27.6s\n",
      "[CV 5/5; 97/330] START learning_rate=0.03, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 97/330] END learning_rate=0.03, max_depth=11, num_leaves=66;, score=0.909 total time=  45.6s\n",
      "[CV 3/5; 99/330] START learning_rate=0.03, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 99/330] END learning_rate=0.03, max_depth=11, num_leaves=70;, score=0.909 total time=  44.8s\n",
      "[CV 3/5; 101/330] START learning_rate=0.03, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 101/330] END learning_rate=0.03, max_depth=12, num_leaves=52;, score=0.909 total time=  40.1s\n",
      "[CV 3/5; 103/330] START learning_rate=0.03, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 103/330] END learning_rate=0.03, max_depth=12, num_leaves=56;, score=0.909 total time=  26.6s\n",
      "[CV 1/5; 105/330] START learning_rate=0.03, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 105/330] END learning_rate=0.03, max_depth=12, num_leaves=60;, score=0.909 total time=  27.0s\n",
      "[CV 3/5; 106/330] START learning_rate=0.03, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 106/330] END learning_rate=0.03, max_depth=12, num_leaves=62;, score=0.909 total time=  26.7s\n",
      "[CV 1/5; 107/330] START learning_rate=0.03, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 107/330] END learning_rate=0.03, max_depth=12, num_leaves=64;, score=0.909 total time=  27.9s\n",
      "[CV 4/5; 108/330] START learning_rate=0.03, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 108/330] END learning_rate=0.03, max_depth=12, num_leaves=66;, score=0.909 total time=  44.0s\n",
      "[CV 4/5; 110/330] START learning_rate=0.03, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 110/330] END learning_rate=0.03, max_depth=12, num_leaves=70;, score=0.909 total time=  43.8s\n",
      "[CV 5/5; 112/330] START learning_rate=0.05, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 76/330] END learning_rate=0.03, max_depth=9, num_leaves=68;, score=0.909 total time=  43.8s\n",
      "[CV 4/5; 78/330] START learning_rate=0.03, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 78/330] END learning_rate=0.03, max_depth=10, num_leaves=50;, score=0.909 total time=  39.2s\n",
      "[CV 2/5; 80/330] START learning_rate=0.03, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 80/330] END learning_rate=0.03, max_depth=10, num_leaves=54;, score=0.909 total time=  26.6s\n",
      "[CV 4/5; 81/330] START learning_rate=0.03, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 81/330] END learning_rate=0.03, max_depth=10, num_leaves=56;, score=0.909 total time=  40.1s\n",
      "[CV 1/5; 84/330] START learning_rate=0.03, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 84/330] END learning_rate=0.03, max_depth=10, num_leaves=62;, score=0.909 total time=  27.9s\n",
      "[CV 2/5; 85/330] START learning_rate=0.03, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 85/330] END learning_rate=0.03, max_depth=10, num_leaves=64;, score=0.909 total time=  44.7s\n",
      "[CV 1/5; 87/330] START learning_rate=0.03, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 87/330] END learning_rate=0.03, max_depth=10, num_leaves=68;, score=0.909 total time=  27.6s\n",
      "[CV 2/5; 88/330] START learning_rate=0.03, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 88/330] END learning_rate=0.03, max_depth=10, num_leaves=70;, score=0.909 total time=  28.2s\n",
      "[CV 3/5; 89/330] START learning_rate=0.03, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 89/330] END learning_rate=0.03, max_depth=11, num_leaves=50;, score=0.909 total time=  26.2s\n",
      "[CV 3/5; 90/330] START learning_rate=0.03, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 90/330] END learning_rate=0.03, max_depth=11, num_leaves=52;, score=0.909 total time=  40.2s\n",
      "[CV 4/5; 92/330] START learning_rate=0.03, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 92/330] END learning_rate=0.03, max_depth=11, num_leaves=56;, score=0.909 total time=  42.6s\n",
      "[CV 3/5; 94/330] START learning_rate=0.03, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 94/330] END learning_rate=0.03, max_depth=11, num_leaves=60;, score=0.909 total time=  42.8s\n",
      "[CV 2/5; 96/330] START learning_rate=0.03, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 96/330] END learning_rate=0.03, max_depth=11, num_leaves=64;, score=0.909 total time=  44.2s\n",
      "[CV 3/5; 98/330] START learning_rate=0.03, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 98/330] END learning_rate=0.03, max_depth=11, num_leaves=68;, score=0.909 total time=  45.9s\n",
      "[CV 1/5; 100/330] START learning_rate=0.03, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 100/330] END learning_rate=0.03, max_depth=12, num_leaves=50;, score=0.909 total time=  26.6s\n",
      "[CV 1/5; 101/330] START learning_rate=0.03, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 101/330] END learning_rate=0.03, max_depth=12, num_leaves=52;, score=0.909 total time=  41.6s\n",
      "[CV 1/5; 103/330] START learning_rate=0.03, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 103/330] END learning_rate=0.03, max_depth=12, num_leaves=56;, score=0.909 total time=  27.1s\n",
      "[CV 3/5; 104/330] START learning_rate=0.03, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 104/330] END learning_rate=0.03, max_depth=12, num_leaves=58;, score=0.909 total time=  27.1s\n",
      "[CV 5/5; 105/330] START learning_rate=0.03, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 105/330] END learning_rate=0.03, max_depth=12, num_leaves=60;, score=0.909 total time=  44.0s\n",
      "[CV 2/5; 108/330] START learning_rate=0.03, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 108/330] END learning_rate=0.03, max_depth=12, num_leaves=66;, score=0.909 total time=  28.1s\n",
      "[CV 2/5; 109/330] START learning_rate=0.03, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 109/330] END learning_rate=0.03, max_depth=12, num_leaves=68;, score=0.909 total time=  30.4s\n",
      "[CV 3/5; 110/330] START learning_rate=0.03, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 110/330] END learning_rate=0.03, max_depth=12, num_leaves=70;, score=0.909 total time=  27.6s\n",
      "[CV 1/5; 112/330] START learning_rate=0.05, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 112/330] END learning_rate=0.05, max_depth=8, num_leaves=52;, score=0.909 total time=  23.7s\n",
      "[CV 2/5; 113/330] START learning_rate=0.05, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 105/330] END learning_rate=0.03, max_depth=12, num_leaves=60;, score=0.909 total time=  43.7s\n",
      "[CV 3/5; 107/330] START learning_rate=0.03, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 107/330] END learning_rate=0.03, max_depth=12, num_leaves=64;, score=0.909 total time=  43.0s\n",
      "[CV 4/5; 109/330] START learning_rate=0.03, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 109/330] END learning_rate=0.03, max_depth=12, num_leaves=68;, score=0.909 total time=  29.5s\n",
      "[CV 5/5; 110/330] START learning_rate=0.03, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 110/330] END learning_rate=0.03, max_depth=12, num_leaves=70;, score=0.909 total time=  29.1s\n",
      "[CV 3/5; 112/330] START learning_rate=0.05, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 112/330] END learning_rate=0.05, max_depth=8, num_leaves=52;, score=0.910 total time=  35.1s\n",
      "[CV 1/5; 114/330] START learning_rate=0.05, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 114/330] END learning_rate=0.05, max_depth=8, num_leaves=56;, score=0.909 total time=  24.0s\n",
      "[CV 4/5; 115/330] START learning_rate=0.05, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 115/330] END learning_rate=0.05, max_depth=8, num_leaves=58;, score=0.910 total time=  24.9s\n",
      "[CV 1/5; 117/330] START learning_rate=0.05, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 117/330] END learning_rate=0.05, max_depth=8, num_leaves=62;, score=0.909 total time=  26.6s\n",
      "[CV 3/5; 118/330] START learning_rate=0.05, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 118/330] END learning_rate=0.05, max_depth=8, num_leaves=64;, score=0.910 total time=  28.3s\n",
      "[CV 1/5; 120/330] START learning_rate=0.05, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 120/330] END learning_rate=0.05, max_depth=8, num_leaves=68;, score=0.909 total time=  39.7s\n",
      "[CV 5/5; 121/330] START learning_rate=0.05, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 121/330] END learning_rate=0.05, max_depth=8, num_leaves=70;, score=0.909 total time=  26.2s\n",
      "[CV 2/5; 123/330] START learning_rate=0.05, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 123/330] END learning_rate=0.05, max_depth=9, num_leaves=52;, score=0.909 total time=  24.6s\n",
      "[CV 4/5; 124/330] START learning_rate=0.05, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 124/330] END learning_rate=0.05, max_depth=9, num_leaves=54;, score=0.909 total time=  27.4s\n",
      "[CV 1/5; 126/330] START learning_rate=0.05, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 126/330] END learning_rate=0.05, max_depth=9, num_leaves=58;, score=0.909 total time=  26.6s\n",
      "[CV 4/5; 127/330] START learning_rate=0.05, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 127/330] END learning_rate=0.05, max_depth=9, num_leaves=60;, score=0.910 total time=  39.6s\n",
      "[CV 2/5; 129/330] START learning_rate=0.05, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 129/330] END learning_rate=0.05, max_depth=9, num_leaves=64;, score=0.909 total time=  27.3s\n",
      "[CV 5/5; 130/330] START learning_rate=0.05, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 130/330] END learning_rate=0.05, max_depth=9, num_leaves=66;, score=0.909 total time=  39.8s\n",
      "[CV 3/5; 132/330] START learning_rate=0.05, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 132/330] END learning_rate=0.05, max_depth=9, num_leaves=70;, score=0.910 total time=  42.1s\n",
      "[CV 3/5; 134/330] START learning_rate=0.05, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 134/330] END learning_rate=0.05, max_depth=10, num_leaves=52;, score=0.910 total time=  24.6s\n",
      "[CV 1/5; 136/330] START learning_rate=0.05, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 136/330] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.909 total time=  25.9s\n",
      "[CV 3/5; 137/330] START learning_rate=0.05, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 137/330] END learning_rate=0.05, max_depth=10, num_leaves=58;, score=0.910 total time=  40.2s\n",
      "[CV 3/5; 139/330] START learning_rate=0.05, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 139/330] END learning_rate=0.05, max_depth=10, num_leaves=62;, score=0.909 total time=  42.5s\n",
      "[CV 2/5; 141/330] START learning_rate=0.05, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 141/330] END learning_rate=0.05, max_depth=10, num_leaves=66;, score=0.909 total time=  30.1s\n",
      "[CV 4/5; 142/330] START learning_rate=0.05, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[CV 4/5; 106/330] END learning_rate=0.03, max_depth=12, num_leaves=62;, score=0.909 total time=  27.9s\n",
      "[CV 1/5; 108/330] START learning_rate=0.03, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 108/330] END learning_rate=0.03, max_depth=12, num_leaves=66;, score=0.909 total time=  28.2s\n",
      "[CV 1/5; 109/330] START learning_rate=0.03, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 109/330] END learning_rate=0.03, max_depth=12, num_leaves=68;, score=0.909 total time=  30.0s\n",
      "[CV 1/5; 110/330] START learning_rate=0.03, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.253415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 110/330] END learning_rate=0.03, max_depth=12, num_leaves=70;, score=0.909 total time=  28.5s\n",
      "[CV 4/5; 111/330] START learning_rate=0.05, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 111/330] END learning_rate=0.05, max_depth=8, num_leaves=50;, score=0.910 total time=  35.4s\n",
      "[CV 5/5; 113/330] START learning_rate=0.05, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 113/330] END learning_rate=0.05, max_depth=8, num_leaves=54;, score=0.909 total time=  24.2s\n",
      "[CV 2/5; 115/330] START learning_rate=0.05, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 115/330] END learning_rate=0.05, max_depth=8, num_leaves=58;, score=0.909 total time=  24.9s\n",
      "[CV 3/5; 116/330] START learning_rate=0.05, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 116/330] END learning_rate=0.05, max_depth=8, num_leaves=60;, score=0.910 total time=  26.6s\n",
      "[CV 1/5; 118/330] START learning_rate=0.05, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 118/330] END learning_rate=0.05, max_depth=8, num_leaves=64;, score=0.909 total time=  26.2s\n",
      "[CV 3/5; 119/330] START learning_rate=0.05, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 119/330] END learning_rate=0.05, max_depth=8, num_leaves=66;, score=0.910 total time=  39.2s\n",
      "[CV 2/5; 121/330] START learning_rate=0.05, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 121/330] END learning_rate=0.05, max_depth=8, num_leaves=70;, score=0.909 total time=  27.2s\n",
      "[CV 4/5; 122/330] START learning_rate=0.05, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 122/330] END learning_rate=0.05, max_depth=9, num_leaves=50;, score=0.909 total time=  36.7s\n",
      "[CV 5/5; 124/330] START learning_rate=0.05, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 124/330] END learning_rate=0.05, max_depth=9, num_leaves=54;, score=0.909 total time=  39.2s\n",
      "[CV 3/5; 126/330] START learning_rate=0.05, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 126/330] END learning_rate=0.05, max_depth=9, num_leaves=58;, score=0.909 total time=  25.8s\n",
      "[CV 1/5; 128/330] START learning_rate=0.05, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 128/330] END learning_rate=0.05, max_depth=9, num_leaves=62;, score=0.909 total time=  41.5s\n",
      "[CV 5/5; 129/330] START learning_rate=0.05, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 129/330] END learning_rate=0.05, max_depth=9, num_leaves=64;, score=0.909 total time=  40.3s\n",
      "[CV 5/5; 131/330] START learning_rate=0.05, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 131/330] END learning_rate=0.05, max_depth=9, num_leaves=68;, score=0.909 total time=  27.8s\n",
      "[CV 2/5; 133/330] START learning_rate=0.05, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 133/330] END learning_rate=0.05, max_depth=10, num_leaves=50;, score=0.909 total time=  25.8s\n",
      "[CV 5/5; 134/330] START learning_rate=0.05, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 134/330] END learning_rate=0.05, max_depth=10, num_leaves=52;, score=0.909 total time=  24.9s\n",
      "[CV 3/5; 136/330] START learning_rate=0.05, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 136/330] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.909 total time=  25.6s\n",
      "[CV 5/5; 137/330] START learning_rate=0.05, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.253968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 137/330] END learning_rate=0.05, max_depth=10, num_leaves=58;, score=0.909 total time=  41.7s\n",
      "[CV 5/5; 139/330] START learning_rate=0.05, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 139/330] END learning_rate=0.05, max_depth=10, num_leaves=62;, score=0.909 total time=  41.9s\n",
      "[CV 3/5; 141/330] START learning_rate=0.05, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.209496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 141/330] END learning_rate=0.05, max_depth=10, num_leaves=66;, score=0.909 total time=  42.9s\n",
      "[CV 3/5; 143/330] START learning_rate=0.05, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 111/330] END learning_rate=0.05, max_depth=8, num_leaves=50;, score=0.909 total time=  23.5s\n",
      "[CV 1/5; 113/330] START learning_rate=0.05, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 113/330] END learning_rate=0.05, max_depth=8, num_leaves=54;, score=0.909 total time=  24.4s\n",
      "[CV 3/5; 114/330] START learning_rate=0.05, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 114/330] END learning_rate=0.05, max_depth=8, num_leaves=56;, score=0.910 total time=  23.7s\n",
      "[CV 1/5; 116/330] START learning_rate=0.05, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 116/330] END learning_rate=0.05, max_depth=8, num_leaves=60;, score=0.909 total time=  26.2s\n",
      "[CV 2/5; 117/330] START learning_rate=0.05, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 117/330] END learning_rate=0.05, max_depth=8, num_leaves=62;, score=0.909 total time=  27.1s\n",
      "[CV 5/5; 118/330] START learning_rate=0.05, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.224694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 118/330] END learning_rate=0.05, max_depth=8, num_leaves=64;, score=0.909 total time=  38.5s\n",
      "[CV 5/5; 120/330] START learning_rate=0.05, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 120/330] END learning_rate=0.05, max_depth=8, num_leaves=68;, score=0.909 total time=  39.2s\n",
      "[CV 5/5; 122/330] START learning_rate=0.05, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 122/330] END learning_rate=0.05, max_depth=9, num_leaves=50;, score=0.909 total time=  24.5s\n",
      "[CV 1/5; 124/330] START learning_rate=0.05, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 124/330] END learning_rate=0.05, max_depth=9, num_leaves=54;, score=0.909 total time=  26.2s\n",
      "[CV 3/5; 125/330] START learning_rate=0.05, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 125/330] END learning_rate=0.05, max_depth=9, num_leaves=56;, score=0.909 total time=  26.8s\n",
      "[CV 4/5; 126/330] START learning_rate=0.05, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 126/330] END learning_rate=0.05, max_depth=9, num_leaves=58;, score=0.909 total time=  26.2s\n",
      "[CV 2/5; 128/330] START learning_rate=0.05, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 128/330] END learning_rate=0.05, max_depth=9, num_leaves=62;, score=0.909 total time=  41.7s\n",
      "[CV 1/5; 130/330] START learning_rate=0.05, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 130/330] END learning_rate=0.05, max_depth=9, num_leaves=66;, score=0.909 total time=  27.5s\n",
      "[CV 2/5; 131/330] START learning_rate=0.05, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 131/330] END learning_rate=0.05, max_depth=9, num_leaves=68;, score=0.909 total time=  27.8s\n",
      "[CV 5/5; 132/330] START learning_rate=0.05, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 132/330] END learning_rate=0.05, max_depth=9, num_leaves=70;, score=0.909 total time=  28.0s\n",
      "[CV 1/5; 134/330] START learning_rate=0.05, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.258219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 134/330] END learning_rate=0.05, max_depth=10, num_leaves=52;, score=0.909 total time=  24.9s\n",
      "[CV 4/5; 135/330] START learning_rate=0.05, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 135/330] END learning_rate=0.05, max_depth=10, num_leaves=54;, score=0.910 total time=  38.1s\n",
      "[CV 1/5; 138/330] START learning_rate=0.05, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 138/330] END learning_rate=0.05, max_depth=10, num_leaves=60;, score=0.909 total time=  28.6s\n",
      "[CV 1/5; 139/330] START learning_rate=0.05, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 139/330] END learning_rate=0.05, max_depth=10, num_leaves=62;, score=0.909 total time=  29.5s\n",
      "[CV 2/5; 140/330] START learning_rate=0.05, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 140/330] END learning_rate=0.05, max_depth=10, num_leaves=64;, score=0.909 total time=  27.9s\n",
      "[CV 5/5; 141/330] START learning_rate=0.05, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 141/330] END learning_rate=0.05, max_depth=10, num_leaves=66;, score=0.909 total time=  29.7s\n",
      "[CV 1/5; 143/330] START learning_rate=0.05, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 143/330] END learning_rate=0.05, max_depth=10, num_leaves=70;, score=0.909 total time=  28.0s\n",
      "[CV 3/5; 144/330] START learning_rate=0.05, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 111/330] END learning_rate=0.05, max_depth=8, num_leaves=50;, score=0.909 total time=  23.4s\n",
      "[CV 2/5; 112/330] START learning_rate=0.05, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 112/330] END learning_rate=0.05, max_depth=8, num_leaves=52;, score=0.909 total time=  24.1s\n",
      "[CV 4/5; 113/330] START learning_rate=0.05, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174846 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 113/330] END learning_rate=0.05, max_depth=8, num_leaves=54;, score=0.910 total time=  35.6s\n",
      "[CV 3/5; 115/330] START learning_rate=0.05, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 115/330] END learning_rate=0.05, max_depth=8, num_leaves=58;, score=0.910 total time=  36.4s\n",
      "[CV 3/5; 117/330] START learning_rate=0.05, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 117/330] END learning_rate=0.05, max_depth=8, num_leaves=62;, score=0.910 total time=  27.9s\n",
      "[CV 1/5; 119/330] START learning_rate=0.05, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 119/330] END learning_rate=0.05, max_depth=8, num_leaves=66;, score=0.909 total time=  26.4s\n",
      "[CV 3/5; 120/330] START learning_rate=0.05, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 120/330] END learning_rate=0.05, max_depth=8, num_leaves=68;, score=0.910 total time=  38.9s\n",
      "[CV 3/5; 122/330] START learning_rate=0.05, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 122/330] END learning_rate=0.05, max_depth=9, num_leaves=50;, score=0.910 total time=  36.1s\n",
      "[CV 2/5; 124/330] START learning_rate=0.05, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 124/330] END learning_rate=0.05, max_depth=9, num_leaves=54;, score=0.909 total time=  27.5s\n",
      "[CV 5/5; 125/330] START learning_rate=0.05, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 125/330] END learning_rate=0.05, max_depth=9, num_leaves=56;, score=0.909 total time=  27.5s\n",
      "[CV 2/5; 127/330] START learning_rate=0.05, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 127/330] END learning_rate=0.05, max_depth=9, num_leaves=60;, score=0.909 total time=  26.7s\n",
      "[CV 4/5; 128/330] START learning_rate=0.05, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 128/330] END learning_rate=0.05, max_depth=9, num_leaves=62;, score=0.910 total time=  26.5s\n",
      "[CV 4/5; 129/330] START learning_rate=0.05, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.381620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 129/330] END learning_rate=0.05, max_depth=9, num_leaves=64;, score=0.910 total time=  39.8s\n",
      "[CV 3/5; 131/330] START learning_rate=0.05, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 131/330] END learning_rate=0.05, max_depth=9, num_leaves=68;, score=0.910 total time=  27.2s\n",
      "[CV 1/5; 133/330] START learning_rate=0.05, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.257454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 133/330] END learning_rate=0.05, max_depth=10, num_leaves=50;, score=0.909 total time=  25.7s\n",
      "[CV 2/5; 134/330] START learning_rate=0.05, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 134/330] END learning_rate=0.05, max_depth=10, num_leaves=52;, score=0.909 total time=  25.6s\n",
      "[CV 5/5; 135/330] START learning_rate=0.05, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 135/330] END learning_rate=0.05, max_depth=10, num_leaves=54;, score=0.909 total time=  26.7s\n",
      "[CV 2/5; 137/330] START learning_rate=0.05, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 137/330] END learning_rate=0.05, max_depth=10, num_leaves=58;, score=0.909 total time=  28.8s\n",
      "[CV 4/5; 138/330] START learning_rate=0.05, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 138/330] END learning_rate=0.05, max_depth=10, num_leaves=60;, score=0.910 total time=  42.9s\n",
      "[CV 3/5; 140/330] START learning_rate=0.05, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 140/330] END learning_rate=0.05, max_depth=10, num_leaves=64;, score=0.910 total time=  27.2s\n",
      "[CV 1/5; 142/330] START learning_rate=0.05, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 142/330] END learning_rate=0.05, max_depth=10, num_leaves=68;, score=0.909 total time=  30.2s\n",
      "[CV 2/5; 143/330] START learning_rate=0.05, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 143/330] END learning_rate=0.05, max_depth=10, num_leaves=70;, score=0.909 total time=  29.3s\n",
      "[CV 5/5; 144/330] START learning_rate=0.05, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 111/330] END learning_rate=0.05, max_depth=8, num_leaves=50;, score=0.909 total time=  24.2s\n",
      "[CV 4/5; 112/330] START learning_rate=0.05, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 112/330] END learning_rate=0.05, max_depth=8, num_leaves=52;, score=0.910 total time=  35.4s\n",
      "[CV 2/5; 114/330] START learning_rate=0.05, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 114/330] END learning_rate=0.05, max_depth=8, num_leaves=56;, score=0.909 total time=  24.2s\n",
      "[CV 5/5; 115/330] START learning_rate=0.05, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.209172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 115/330] END learning_rate=0.05, max_depth=8, num_leaves=58;, score=0.909 total time=  37.9s\n",
      "[CV 4/5; 117/330] START learning_rate=0.05, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 117/330] END learning_rate=0.05, max_depth=8, num_leaves=62;, score=0.910 total time=  26.9s\n",
      "[CV 2/5; 119/330] START learning_rate=0.05, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 119/330] END learning_rate=0.05, max_depth=8, num_leaves=66;, score=0.909 total time=  27.0s\n",
      "[CV 4/5; 120/330] START learning_rate=0.05, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 120/330] END learning_rate=0.05, max_depth=8, num_leaves=68;, score=0.910 total time=  26.2s\n",
      "[CV 1/5; 122/330] START learning_rate=0.05, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 122/330] END learning_rate=0.05, max_depth=9, num_leaves=50;, score=0.909 total time=  24.9s\n",
      "[CV 1/5; 123/330] START learning_rate=0.05, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 123/330] END learning_rate=0.05, max_depth=9, num_leaves=52;, score=0.909 total time=  25.3s\n",
      "[CV 3/5; 124/330] START learning_rate=0.05, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265513 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 124/330] END learning_rate=0.05, max_depth=9, num_leaves=54;, score=0.910 total time=  40.2s\n",
      "[CV 2/5; 126/330] START learning_rate=0.05, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 126/330] END learning_rate=0.05, max_depth=9, num_leaves=58;, score=0.909 total time=  27.0s\n",
      "[CV 5/5; 127/330] START learning_rate=0.05, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 127/330] END learning_rate=0.05, max_depth=9, num_leaves=60;, score=0.909 total time=  27.0s\n",
      "[CV 3/5; 129/330] START learning_rate=0.05, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 129/330] END learning_rate=0.05, max_depth=9, num_leaves=64;, score=0.910 total time=  26.9s\n",
      "[CV 4/5; 130/330] START learning_rate=0.05, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 130/330] END learning_rate=0.05, max_depth=9, num_leaves=66;, score=0.910 total time=  40.1s\n",
      "[CV 4/5; 132/330] START learning_rate=0.05, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 132/330] END learning_rate=0.05, max_depth=9, num_leaves=70;, score=0.910 total time=  42.4s\n",
      "[CV 4/5; 134/330] START learning_rate=0.05, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 134/330] END learning_rate=0.05, max_depth=10, num_leaves=52;, score=0.910 total time=  24.7s\n",
      "[CV 2/5; 136/330] START learning_rate=0.05, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 136/330] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.909 total time=  25.6s\n",
      "[CV 4/5; 137/330] START learning_rate=0.05, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 137/330] END learning_rate=0.05, max_depth=10, num_leaves=58;, score=0.910 total time=  41.1s\n",
      "[CV 4/5; 139/330] START learning_rate=0.05, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 139/330] END learning_rate=0.05, max_depth=10, num_leaves=62;, score=0.910 total time=  29.1s\n",
      "[CV 5/5; 140/330] START learning_rate=0.05, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 140/330] END learning_rate=0.05, max_depth=10, num_leaves=64;, score=0.909 total time=  42.4s\n",
      "[CV 5/5; 142/330] START learning_rate=0.05, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 142/330] END learning_rate=0.05, max_depth=10, num_leaves=68;, score=0.909 total time=  28.7s\n",
      "[CV 2/5; 144/330] START learning_rate=0.05, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 144/330] END learning_rate=0.05, max_depth=11, num_leaves=50;, score=0.909 total time=  26.5s\n",
      "[CV 3/5; 145/330] START learning_rate=0.05, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.168526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 109/330] END learning_rate=0.03, max_depth=12, num_leaves=68;, score=0.909 total time=  44.5s\n",
      "[CV 3/5; 111/330] START learning_rate=0.05, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 111/330] END learning_rate=0.05, max_depth=8, num_leaves=50;, score=0.910 total time=  35.9s\n",
      "[CV 3/5; 113/330] START learning_rate=0.05, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 113/330] END learning_rate=0.05, max_depth=8, num_leaves=54;, score=0.909 total time=  23.6s\n",
      "[CV 5/5; 114/330] START learning_rate=0.05, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 114/330] END learning_rate=0.05, max_depth=8, num_leaves=56;, score=0.909 total time=  35.8s\n",
      "[CV 5/5; 116/330] START learning_rate=0.05, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.142512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 116/330] END learning_rate=0.05, max_depth=8, num_leaves=60;, score=0.909 total time=  38.8s\n",
      "[CV 4/5; 118/330] START learning_rate=0.05, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 118/330] END learning_rate=0.05, max_depth=8, num_leaves=64;, score=0.910 total time=  26.6s\n",
      "[CV 2/5; 120/330] START learning_rate=0.05, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 120/330] END learning_rate=0.05, max_depth=8, num_leaves=68;, score=0.909 total time=  27.0s\n",
      "[CV 3/5; 121/330] START learning_rate=0.05, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.196968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 121/330] END learning_rate=0.05, max_depth=8, num_leaves=70;, score=0.910 total time=  38.9s\n",
      "[CV 3/5; 123/330] START learning_rate=0.05, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 123/330] END learning_rate=0.05, max_depth=9, num_leaves=52;, score=0.910 total time=  24.5s\n",
      "[CV 1/5; 125/330] START learning_rate=0.05, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 125/330] END learning_rate=0.05, max_depth=9, num_leaves=56;, score=0.909 total time=  40.6s\n",
      "[CV 5/5; 126/330] START learning_rate=0.05, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 126/330] END learning_rate=0.05, max_depth=9, num_leaves=58;, score=0.909 total time=  25.9s\n",
      "[CV 3/5; 128/330] START learning_rate=0.05, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 128/330] END learning_rate=0.05, max_depth=9, num_leaves=62;, score=0.909 total time=  40.3s\n",
      "[CV 2/5; 130/330] START learning_rate=0.05, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 130/330] END learning_rate=0.05, max_depth=9, num_leaves=66;, score=0.909 total time=  41.1s\n",
      "[CV 2/5; 132/330] START learning_rate=0.05, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 132/330] END learning_rate=0.05, max_depth=9, num_leaves=70;, score=0.909 total time=  28.0s\n",
      "[CV 4/5; 133/330] START learning_rate=0.05, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 133/330] END learning_rate=0.05, max_depth=10, num_leaves=50;, score=0.910 total time=  36.8s\n",
      "[CV 3/5; 135/330] START learning_rate=0.05, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 135/330] END learning_rate=0.05, max_depth=10, num_leaves=54;, score=0.909 total time=  25.2s\n",
      "[CV 1/5; 137/330] START learning_rate=0.05, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.320090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 137/330] END learning_rate=0.05, max_depth=10, num_leaves=58;, score=0.909 total time=  43.2s\n",
      "[CV 5/5; 138/330] START learning_rate=0.05, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.319692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 138/330] END learning_rate=0.05, max_depth=10, num_leaves=60;, score=0.909 total time=  41.4s\n",
      "[CV 1/5; 141/330] START learning_rate=0.05, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 141/330] END learning_rate=0.05, max_depth=10, num_leaves=66;, score=0.909 total time=  29.6s\n",
      "[CV 2/5; 142/330] START learning_rate=0.05, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 142/330] END learning_rate=0.05, max_depth=10, num_leaves=68;, score=0.909 total time=  30.4s\n",
      "[CV 5/5; 143/330] START learning_rate=0.05, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 143/330] END learning_rate=0.05, max_depth=10, num_leaves=70;, score=0.909 total time=  44.0s\n",
      "[CV 1/5; 146/330] START learning_rate=0.05, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 146/330] END learning_rate=0.05, max_depth=11, num_leaves=54;, score=0.909 total time=  25.3s\n",
      "[CV 2/5; 147/330] START learning_rate=0.05, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 147/330] END learning_rate=0.05, max_depth=11, num_leaves=56;, score=0.909 total time=  27.5s\n",
      "[CV 5/5; 148/330] START learning_rate=0.05, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 113/330] END learning_rate=0.05, max_depth=8, num_leaves=54;, score=0.909 total time=  24.6s\n",
      "[CV 4/5; 114/330] START learning_rate=0.05, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 114/330] END learning_rate=0.05, max_depth=8, num_leaves=56;, score=0.910 total time=  35.6s\n",
      "[CV 4/5; 116/330] START learning_rate=0.05, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 116/330] END learning_rate=0.05, max_depth=8, num_leaves=60;, score=0.910 total time=  25.4s\n",
      "[CV 2/5; 118/330] START learning_rate=0.05, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 118/330] END learning_rate=0.05, max_depth=8, num_leaves=64;, score=0.909 total time=  27.8s\n",
      "[CV 4/5; 119/330] START learning_rate=0.05, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 119/330] END learning_rate=0.05, max_depth=8, num_leaves=66;, score=0.909 total time=  28.0s\n",
      "[CV 1/5; 121/330] START learning_rate=0.05, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 121/330] END learning_rate=0.05, max_depth=8, num_leaves=70;, score=0.909 total time=  26.4s\n",
      "[CV 2/5; 122/330] START learning_rate=0.05, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 122/330] END learning_rate=0.05, max_depth=9, num_leaves=50;, score=0.909 total time=  24.9s\n",
      "[CV 4/5; 123/330] START learning_rate=0.05, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 123/330] END learning_rate=0.05, max_depth=9, num_leaves=52;, score=0.909 total time=  37.9s\n",
      "[CV 4/5; 125/330] START learning_rate=0.05, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 125/330] END learning_rate=0.05, max_depth=9, num_leaves=56;, score=0.910 total time=  27.0s\n",
      "[CV 1/5; 127/330] START learning_rate=0.05, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 127/330] END learning_rate=0.05, max_depth=9, num_leaves=60;, score=0.909 total time=  39.8s\n",
      "[CV 5/5; 128/330] START learning_rate=0.05, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 128/330] END learning_rate=0.05, max_depth=9, num_leaves=62;, score=0.909 total time=  40.0s\n",
      "[CV 1/5; 131/330] START learning_rate=0.05, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 131/330] END learning_rate=0.05, max_depth=9, num_leaves=68;, score=0.909 total time=  26.1s\n",
      "[CV 1/5; 132/330] START learning_rate=0.05, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 132/330] END learning_rate=0.05, max_depth=9, num_leaves=70;, score=0.909 total time=  27.1s\n",
      "[CV 3/5; 133/330] START learning_rate=0.05, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 133/330] END learning_rate=0.05, max_depth=10, num_leaves=50;, score=0.910 total time=  36.7s\n",
      "[CV 2/5; 135/330] START learning_rate=0.05, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 135/330] END learning_rate=0.05, max_depth=10, num_leaves=54;, score=0.909 total time=  25.3s\n",
      "[CV 5/5; 136/330] START learning_rate=0.05, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 136/330] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.909 total time=  26.7s\n",
      "[CV 3/5; 138/330] START learning_rate=0.05, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 138/330] END learning_rate=0.05, max_depth=10, num_leaves=60;, score=0.910 total time=  42.0s\n",
      "[CV 1/5; 140/330] START learning_rate=0.05, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 140/330] END learning_rate=0.05, max_depth=10, num_leaves=64;, score=0.909 total time=  28.9s\n",
      "[CV 4/5; 141/330] START learning_rate=0.05, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 141/330] END learning_rate=0.05, max_depth=10, num_leaves=66;, score=0.910 total time=  43.5s\n",
      "[CV 4/5; 143/330] START learning_rate=0.05, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 143/330] END learning_rate=0.05, max_depth=10, num_leaves=70;, score=0.910 total time=  42.2s\n",
      "[CV 4/5; 145/330] START learning_rate=0.05, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 145/330] END learning_rate=0.05, max_depth=11, num_leaves=52;, score=0.910 total time=  39.3s\n",
      "[CV 3/5; 147/330] START learning_rate=0.05, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 147/330] END learning_rate=0.05, max_depth=11, num_leaves=56;, score=0.910 total time=  40.0s\n",
      "[CV 1/5; 149/330] START learning_rate=0.05, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 149/330] END learning_rate=0.05, max_depth=11, num_leaves=60;, score=0.909 total time=  41.4s\n",
      "[CV 3/5; 151/330] START learning_rate=0.05, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 112/330] END learning_rate=0.05, max_depth=8, num_leaves=52;, score=0.909 total time=  35.6s\n",
      "[CV 1/5; 115/330] START learning_rate=0.05, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 115/330] END learning_rate=0.05, max_depth=8, num_leaves=58;, score=0.909 total time=  24.7s\n",
      "[CV 2/5; 116/330] START learning_rate=0.05, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 116/330] END learning_rate=0.05, max_depth=8, num_leaves=60;, score=0.909 total time=  27.3s\n",
      "[CV 5/5; 117/330] START learning_rate=0.05, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 117/330] END learning_rate=0.05, max_depth=8, num_leaves=62;, score=0.909 total time=  38.5s\n",
      "[CV 5/5; 119/330] START learning_rate=0.05, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 119/330] END learning_rate=0.05, max_depth=8, num_leaves=66;, score=0.909 total time=  39.5s\n",
      "[CV 4/5; 121/330] START learning_rate=0.05, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 121/330] END learning_rate=0.05, max_depth=8, num_leaves=70;, score=0.910 total time=  39.5s\n",
      "[CV 5/5; 123/330] START learning_rate=0.05, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 123/330] END learning_rate=0.05, max_depth=9, num_leaves=52;, score=0.909 total time=  24.6s\n",
      "[CV 2/5; 125/330] START learning_rate=0.05, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.193030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 125/330] END learning_rate=0.05, max_depth=9, num_leaves=56;, score=0.909 total time=  40.5s\n",
      "[CV 3/5; 127/330] START learning_rate=0.05, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 127/330] END learning_rate=0.05, max_depth=9, num_leaves=60;, score=0.910 total time=  39.1s\n",
      "[CV 1/5; 129/330] START learning_rate=0.05, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 129/330] END learning_rate=0.05, max_depth=9, num_leaves=64;, score=0.909 total time=  27.3s\n",
      "[CV 3/5; 130/330] START learning_rate=0.05, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 130/330] END learning_rate=0.05, max_depth=9, num_leaves=66;, score=0.910 total time=  26.7s\n",
      "[CV 4/5; 131/330] START learning_rate=0.05, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 131/330] END learning_rate=0.05, max_depth=9, num_leaves=68;, score=0.910 total time=  40.7s\n",
      "[CV 5/5; 133/330] START learning_rate=0.05, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 133/330] END learning_rate=0.05, max_depth=10, num_leaves=50;, score=0.909 total time=  25.0s\n",
      "[CV 1/5; 135/330] START learning_rate=0.05, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 135/330] END learning_rate=0.05, max_depth=10, num_leaves=54;, score=0.909 total time=  25.3s\n",
      "[CV 4/5; 136/330] START learning_rate=0.05, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 136/330] END learning_rate=0.05, max_depth=10, num_leaves=56;, score=0.910 total time=  26.9s\n",
      "[CV 2/5; 138/330] START learning_rate=0.05, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 138/330] END learning_rate=0.05, max_depth=10, num_leaves=60;, score=0.909 total time=  28.7s\n",
      "[CV 2/5; 139/330] START learning_rate=0.05, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 139/330] END learning_rate=0.05, max_depth=10, num_leaves=62;, score=0.909 total time=  29.0s\n",
      "[CV 4/5; 140/330] START learning_rate=0.05, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.429997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 140/330] END learning_rate=0.05, max_depth=10, num_leaves=64;, score=0.910 total time=  41.7s\n",
      "[CV 3/5; 142/330] START learning_rate=0.05, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 142/330] END learning_rate=0.05, max_depth=10, num_leaves=68;, score=0.909 total time=  41.2s\n",
      "[CV 4/5; 144/330] START learning_rate=0.05, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 144/330] END learning_rate=0.05, max_depth=11, num_leaves=50;, score=0.910 total time=  38.6s\n",
      "[CV 4/5; 146/330] START learning_rate=0.05, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 146/330] END learning_rate=0.05, max_depth=11, num_leaves=54;, score=0.909 total time=  26.0s\n",
      "[CV 4/5; 147/330] START learning_rate=0.05, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 147/330] END learning_rate=0.05, max_depth=11, num_leaves=56;, score=0.910 total time=  41.2s\n",
      "[CV 3/5; 149/330] START learning_rate=0.05, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 149/330] END learning_rate=0.05, max_depth=11, num_leaves=60;, score=0.910 total time=  27.3s\n",
      "[CV 5/5; 150/330] START learning_rate=0.05, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 142/330] END learning_rate=0.05, max_depth=10, num_leaves=68;, score=0.910 total time=  27.6s\n",
      "[CV 1/5; 144/330] START learning_rate=0.05, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 144/330] END learning_rate=0.05, max_depth=11, num_leaves=50;, score=0.909 total time=  27.0s\n",
      "[CV 2/5; 145/330] START learning_rate=0.05, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 145/330] END learning_rate=0.05, max_depth=11, num_leaves=52;, score=0.909 total time=  27.4s\n",
      "[CV 1/5; 147/330] START learning_rate=0.05, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 147/330] END learning_rate=0.05, max_depth=11, num_leaves=56;, score=0.909 total time=  26.7s\n",
      "[CV 3/5; 148/330] START learning_rate=0.05, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.344735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 148/330] END learning_rate=0.05, max_depth=11, num_leaves=58;, score=0.910 total time=  43.3s\n",
      "[CV 2/5; 150/330] START learning_rate=0.05, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 150/330] END learning_rate=0.05, max_depth=11, num_leaves=62;, score=0.909 total time=  42.4s\n",
      "[CV 1/5; 152/330] START learning_rate=0.05, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 152/330] END learning_rate=0.05, max_depth=11, num_leaves=66;, score=0.909 total time=  28.8s\n",
      "[CV 4/5; 153/330] START learning_rate=0.05, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.195141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 153/330] END learning_rate=0.05, max_depth=11, num_leaves=68;, score=0.910 total time=  42.1s\n",
      "[CV 4/5; 155/330] START learning_rate=0.05, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 155/330] END learning_rate=0.05, max_depth=12, num_leaves=50;, score=0.910 total time=  25.7s\n",
      "[CV 1/5; 157/330] START learning_rate=0.05, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 157/330] END learning_rate=0.05, max_depth=12, num_leaves=54;, score=0.909 total time=  39.6s\n",
      "[CV 1/5; 159/330] START learning_rate=0.05, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 159/330] END learning_rate=0.05, max_depth=12, num_leaves=58;, score=0.909 total time=  31.3s\n",
      "[CV 3/5; 160/330] START learning_rate=0.05, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 160/330] END learning_rate=0.05, max_depth=12, num_leaves=60;, score=0.910 total time=  29.0s\n",
      "[CV 4/5; 161/330] START learning_rate=0.05, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 161/330] END learning_rate=0.05, max_depth=12, num_leaves=62;, score=0.909 total time=  41.9s\n",
      "[CV 5/5; 163/330] START learning_rate=0.05, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 163/330] END learning_rate=0.05, max_depth=12, num_leaves=66;, score=0.909 total time=  42.7s\n",
      "[CV 1/5; 166/330] START learning_rate=0.07, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 166/330] END learning_rate=0.07, max_depth=8, num_leaves=50;, score=0.909 total time=  24.5s\n",
      "[CV 4/5; 166/330] START learning_rate=0.07, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 166/330] END learning_rate=0.07, max_depth=8, num_leaves=50;, score=0.910 total time=  23.6s\n",
      "[CV 1/5; 168/330] START learning_rate=0.07, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 168/330] END learning_rate=0.07, max_depth=8, num_leaves=54;, score=0.909 total time=  23.2s\n",
      "[CV 4/5; 169/330] START learning_rate=0.07, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 169/330] END learning_rate=0.07, max_depth=8, num_leaves=56;, score=0.910 total time=  22.1s\n",
      "[CV 1/5; 171/330] START learning_rate=0.07, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 171/330] END learning_rate=0.07, max_depth=8, num_leaves=60;, score=0.909 total time=  22.5s\n",
      "[CV 4/5; 172/330] START learning_rate=0.07, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 172/330] END learning_rate=0.07, max_depth=8, num_leaves=62;, score=0.910 total time=  22.9s\n",
      "[CV 2/5; 174/330] START learning_rate=0.07, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 174/330] END learning_rate=0.07, max_depth=8, num_leaves=66;, score=0.909 total time=  35.0s\n",
      "[CV 1/5; 176/330] START learning_rate=0.07, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 176/330] END learning_rate=0.07, max_depth=8, num_leaves=70;, score=0.909 total time=  24.5s\n",
      "[CV 4/5; 177/330] START learning_rate=0.07, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 143/330] END learning_rate=0.05, max_depth=10, num_leaves=70;, score=0.910 total time=  29.3s\n",
      "[CV 1/5; 145/330] START learning_rate=0.05, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 145/330] END learning_rate=0.05, max_depth=11, num_leaves=52;, score=0.909 total time=  27.0s\n",
      "[CV 3/5; 146/330] START learning_rate=0.05, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 146/330] END learning_rate=0.05, max_depth=11, num_leaves=54;, score=0.910 total time=  39.2s\n",
      "[CV 4/5; 148/330] START learning_rate=0.05, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 148/330] END learning_rate=0.05, max_depth=11, num_leaves=58;, score=0.910 total time=  29.3s\n",
      "[CV 2/5; 149/330] START learning_rate=0.05, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 149/330] END learning_rate=0.05, max_depth=11, num_leaves=60;, score=0.909 total time=  26.8s\n",
      "[CV 4/5; 150/330] START learning_rate=0.05, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 150/330] END learning_rate=0.05, max_depth=11, num_leaves=62;, score=0.909 total time=  28.5s\n",
      "[CV 2/5; 152/330] START learning_rate=0.05, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 152/330] END learning_rate=0.05, max_depth=11, num_leaves=66;, score=0.909 total time=  28.6s\n",
      "[CV 3/5; 153/330] START learning_rate=0.05, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 153/330] END learning_rate=0.05, max_depth=11, num_leaves=68;, score=0.910 total time=  28.5s\n",
      "[CV 4/5; 154/330] START learning_rate=0.05, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 154/330] END learning_rate=0.05, max_depth=11, num_leaves=70;, score=0.910 total time=  42.5s\n",
      "[CV 2/5; 157/330] START learning_rate=0.05, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 157/330] END learning_rate=0.05, max_depth=12, num_leaves=54;, score=0.909 total time=  26.1s\n",
      "[CV 3/5; 158/330] START learning_rate=0.05, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.205715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 158/330] END learning_rate=0.05, max_depth=12, num_leaves=56;, score=0.910 total time=  41.8s\n",
      "[CV 4/5; 160/330] START learning_rate=0.05, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 160/330] END learning_rate=0.05, max_depth=12, num_leaves=60;, score=0.910 total time=  29.4s\n",
      "[CV 5/5; 161/330] START learning_rate=0.05, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 161/330] END learning_rate=0.05, max_depth=12, num_leaves=62;, score=0.909 total time=  41.3s\n",
      "[CV 4/5; 163/330] START learning_rate=0.05, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 163/330] END learning_rate=0.05, max_depth=12, num_leaves=66;, score=0.910 total time=  42.6s\n",
      "[CV 5/5; 165/330] START learning_rate=0.05, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 165/330] END learning_rate=0.05, max_depth=12, num_leaves=70;, score=0.909 total time=  30.0s\n",
      "[CV 4/5; 167/330] START learning_rate=0.07, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 167/330] END learning_rate=0.07, max_depth=8, num_leaves=52;, score=0.910 total time=  34.5s\n",
      "[CV 2/5; 169/330] START learning_rate=0.07, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 169/330] END learning_rate=0.07, max_depth=8, num_leaves=56;, score=0.909 total time=  23.7s\n",
      "[CV 5/5; 170/330] START learning_rate=0.07, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 170/330] END learning_rate=0.07, max_depth=8, num_leaves=58;, score=0.909 total time=  22.7s\n",
      "[CV 3/5; 172/330] START learning_rate=0.07, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 172/330] END learning_rate=0.07, max_depth=8, num_leaves=62;, score=0.910 total time=  22.9s\n",
      "[CV 1/5; 174/330] START learning_rate=0.07, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 174/330] END learning_rate=0.07, max_depth=8, num_leaves=66;, score=0.909 total time=  34.7s\n",
      "[CV 4/5; 175/330] START learning_rate=0.07, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5; 175/330] END learning_rate=0.07, max_depth=8, num_leaves=68;, score=0.910 total time=  24.1s\n",
      "[CV 2/5; 177/330] START learning_rate=0.07, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 177/330] END learning_rate=0.07, max_depth=9, num_leaves=50;, score=0.909 total time=  23.8s\n",
      "[CV 4/5; 178/330] START learning_rate=0.07, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 178/330] END learning_rate=0.07, max_depth=9, num_leaves=52;, score=0.910 total time=  36.1s\n",
      "[CV 2/5; 180/330] START learning_rate=0.07, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 145/330] END learning_rate=0.05, max_depth=11, num_leaves=52;, score=0.910 total time=  26.4s\n",
      "[CV 5/5; 146/330] START learning_rate=0.05, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 146/330] END learning_rate=0.05, max_depth=11, num_leaves=54;, score=0.909 total time=  25.8s\n",
      "[CV 2/5; 148/330] START learning_rate=0.05, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 148/330] END learning_rate=0.05, max_depth=11, num_leaves=58;, score=0.909 total time=  44.0s\n",
      "[CV 1/5; 150/330] START learning_rate=0.05, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 150/330] END learning_rate=0.05, max_depth=11, num_leaves=62;, score=0.909 total time=  42.6s\n",
      "[CV 5/5; 151/330] START learning_rate=0.05, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 151/330] END learning_rate=0.05, max_depth=11, num_leaves=64;, score=0.909 total time=  40.4s\n",
      "[CV 1/5; 154/330] START learning_rate=0.05, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 154/330] END learning_rate=0.05, max_depth=11, num_leaves=70;, score=0.909 total time=  29.4s\n",
      "[CV 2/5; 155/330] START learning_rate=0.05, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 155/330] END learning_rate=0.05, max_depth=12, num_leaves=50;, score=0.909 total time=  25.6s\n",
      "[CV 4/5; 156/330] START learning_rate=0.05, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 156/330] END learning_rate=0.05, max_depth=12, num_leaves=52;, score=0.910 total time=  26.5s\n",
      "[CV 1/5; 158/330] START learning_rate=0.05, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.286568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 158/330] END learning_rate=0.05, max_depth=12, num_leaves=56;, score=0.909 total time=  26.9s\n",
      "[CV 3/5; 159/330] START learning_rate=0.05, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 159/330] END learning_rate=0.05, max_depth=12, num_leaves=58;, score=0.910 total time=  29.8s\n",
      "[CV 1/5; 161/330] START learning_rate=0.05, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 161/330] END learning_rate=0.05, max_depth=12, num_leaves=62;, score=0.909 total time=  27.9s\n",
      "[CV 3/5; 162/330] START learning_rate=0.05, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 162/330] END learning_rate=0.05, max_depth=12, num_leaves=64;, score=0.910 total time=  42.3s\n",
      "[CV 3/5; 164/330] START learning_rate=0.05, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 164/330] END learning_rate=0.05, max_depth=12, num_leaves=68;, score=0.910 total time=  30.1s\n",
      "[CV 2/5; 165/330] START learning_rate=0.05, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 165/330] END learning_rate=0.05, max_depth=12, num_leaves=70;, score=0.909 total time=  44.8s\n",
      "[CV 5/5; 167/330] START learning_rate=0.07, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 167/330] END learning_rate=0.07, max_depth=8, num_leaves=52;, score=0.909 total time=  22.4s\n",
      "[CV 3/5; 169/330] START learning_rate=0.07, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 169/330] END learning_rate=0.07, max_depth=8, num_leaves=56;, score=0.910 total time=  33.8s\n",
      "[CV 3/5; 171/330] START learning_rate=0.07, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 171/330] END learning_rate=0.07, max_depth=8, num_leaves=60;, score=0.910 total time=  33.4s\n",
      "[CV 5/5; 173/330] START learning_rate=0.07, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 173/330] END learning_rate=0.07, max_depth=8, num_leaves=64;, score=0.909 total time=  23.5s\n",
      "[CV 3/5; 175/330] START learning_rate=0.07, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 175/330] END learning_rate=0.07, max_depth=8, num_leaves=68;, score=0.910 total time=  33.7s\n",
      "[CV 1/5; 177/330] START learning_rate=0.07, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 177/330] END learning_rate=0.07, max_depth=9, num_leaves=50;, score=0.909 total time=  23.2s\n",
      "[CV 1/5; 178/330] START learning_rate=0.07, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.168010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 178/330] END learning_rate=0.07, max_depth=9, num_leaves=52;, score=0.909 total time=  25.2s\n",
      "[CV 3/5; 179/330] START learning_rate=0.07, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 179/330] END learning_rate=0.07, max_depth=9, num_leaves=54;, score=0.910 total time=  24.0s\n",
      "[CV 1/5; 181/330] START learning_rate=0.07, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 181/330] END learning_rate=0.07, max_depth=9, num_leaves=58;, score=0.909 total time=  22.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 144/330] END learning_rate=0.05, max_depth=11, num_leaves=50;, score=0.910 total time=  26.7s\n",
      "[CV 5/5; 145/330] START learning_rate=0.05, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 145/330] END learning_rate=0.05, max_depth=11, num_leaves=52;, score=0.909 total time=  39.3s\n",
      "[CV 5/5; 147/330] START learning_rate=0.05, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 147/330] END learning_rate=0.05, max_depth=11, num_leaves=56;, score=0.909 total time=  41.8s\n",
      "[CV 4/5; 149/330] START learning_rate=0.05, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 149/330] END learning_rate=0.05, max_depth=11, num_leaves=60;, score=0.910 total time=  26.6s\n",
      "[CV 1/5; 151/330] START learning_rate=0.05, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 151/330] END learning_rate=0.05, max_depth=11, num_leaves=64;, score=0.909 total time=  43.3s\n",
      "[CV 5/5; 152/330] START learning_rate=0.05, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 152/330] END learning_rate=0.05, max_depth=11, num_leaves=66;, score=0.909 total time=  27.5s\n",
      "[CV 2/5; 154/330] START learning_rate=0.05, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 154/330] END learning_rate=0.05, max_depth=11, num_leaves=70;, score=0.909 total time=  29.1s\n",
      "[CV 5/5; 155/330] START learning_rate=0.05, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 155/330] END learning_rate=0.05, max_depth=12, num_leaves=50;, score=0.909 total time=  37.1s\n",
      "[CV 4/5; 157/330] START learning_rate=0.05, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 157/330] END learning_rate=0.05, max_depth=12, num_leaves=54;, score=0.910 total time=  26.9s\n",
      "[CV 2/5; 159/330] START learning_rate=0.05, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 159/330] END learning_rate=0.05, max_depth=12, num_leaves=58;, score=0.909 total time=  30.8s\n",
      "[CV 5/5; 160/330] START learning_rate=0.05, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 160/330] END learning_rate=0.05, max_depth=12, num_leaves=60;, score=0.909 total time=  28.9s\n",
      "[CV 1/5; 162/330] START learning_rate=0.05, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 162/330] END learning_rate=0.05, max_depth=12, num_leaves=64;, score=0.909 total time=  42.2s\n",
      "[CV 1/5; 164/330] START learning_rate=0.05, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 164/330] END learning_rate=0.05, max_depth=12, num_leaves=68;, score=0.909 total time=  42.6s\n",
      "[CV 2/5; 166/330] START learning_rate=0.07, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 166/330] END learning_rate=0.07, max_depth=8, num_leaves=50;, score=0.909 total time=  24.6s\n",
      "[CV 1/5; 167/330] START learning_rate=0.07, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 167/330] END learning_rate=0.07, max_depth=8, num_leaves=52;, score=0.909 total time=  24.8s\n",
      "[CV 4/5; 168/330] START learning_rate=0.07, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 168/330] END learning_rate=0.07, max_depth=8, num_leaves=54;, score=0.910 total time=  24.0s\n",
      "[CV 2/5; 170/330] START learning_rate=0.07, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 170/330] END learning_rate=0.07, max_depth=8, num_leaves=58;, score=0.909 total time=  22.5s\n",
      "[CV 5/5; 171/330] START learning_rate=0.07, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 171/330] END learning_rate=0.07, max_depth=8, num_leaves=60;, score=0.909 total time=  22.6s\n",
      "[CV 1/5; 173/330] START learning_rate=0.07, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 173/330] END learning_rate=0.07, max_depth=8, num_leaves=64;, score=0.909 total time=  22.5s\n",
      "[CV 4/5; 174/330] START learning_rate=0.07, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5; 174/330] END learning_rate=0.07, max_depth=8, num_leaves=66;, score=0.910 total time=  24.1s\n",
      "[CV 5/5; 175/330] START learning_rate=0.07, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 175/330] END learning_rate=0.07, max_depth=8, num_leaves=68;, score=0.909 total time=  23.5s\n",
      "[CV 3/5; 177/330] START learning_rate=0.07, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 177/330] END learning_rate=0.07, max_depth=9, num_leaves=50;, score=0.910 total time=  23.9s\n",
      "[CV 5/5; 178/330] START learning_rate=0.07, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.374206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 144/330] END learning_rate=0.05, max_depth=11, num_leaves=50;, score=0.909 total time=  26.2s\n",
      "[CV 2/5; 146/330] START learning_rate=0.05, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 146/330] END learning_rate=0.05, max_depth=11, num_leaves=54;, score=0.909 total time=  39.1s\n",
      "[CV 1/5; 148/330] START learning_rate=0.05, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 148/330] END learning_rate=0.05, max_depth=11, num_leaves=58;, score=0.909 total time=  42.8s\n",
      "[CV 5/5; 149/330] START learning_rate=0.05, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 149/330] END learning_rate=0.05, max_depth=11, num_leaves=60;, score=0.909 total time=  27.6s\n",
      "[CV 2/5; 151/330] START learning_rate=0.05, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 151/330] END learning_rate=0.05, max_depth=11, num_leaves=64;, score=0.909 total time=  28.7s\n",
      "[CV 3/5; 152/330] START learning_rate=0.05, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 152/330] END learning_rate=0.05, max_depth=11, num_leaves=66;, score=0.910 total time=  27.2s\n",
      "[CV 5/5; 153/330] START learning_rate=0.05, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 153/330] END learning_rate=0.05, max_depth=11, num_leaves=68;, score=0.909 total time=  28.7s\n",
      "[CV 1/5; 155/330] START learning_rate=0.05, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 155/330] END learning_rate=0.05, max_depth=12, num_leaves=50;, score=0.909 total time=  25.3s\n",
      "[CV 3/5; 156/330] START learning_rate=0.05, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 156/330] END learning_rate=0.05, max_depth=12, num_leaves=52;, score=0.909 total time=  26.3s\n",
      "[CV 5/5; 157/330] START learning_rate=0.05, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 157/330] END learning_rate=0.05, max_depth=12, num_leaves=54;, score=0.909 total time=  41.9s\n",
      "[CV 5/5; 159/330] START learning_rate=0.05, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 159/330] END learning_rate=0.05, max_depth=12, num_leaves=58;, score=0.909 total time=  41.6s\n",
      "[CV 2/5; 162/330] START learning_rate=0.05, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 162/330] END learning_rate=0.05, max_depth=12, num_leaves=64;, score=0.909 total time=  28.4s\n",
      "[CV 2/5; 163/330] START learning_rate=0.05, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 163/330] END learning_rate=0.05, max_depth=12, num_leaves=66;, score=0.909 total time=  29.6s\n",
      "[CV 5/5; 164/330] START learning_rate=0.05, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.311161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 164/330] END learning_rate=0.05, max_depth=12, num_leaves=68;, score=0.909 total time=  42.4s\n",
      "[CV 3/5; 166/330] START learning_rate=0.07, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.329493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 166/330] END learning_rate=0.07, max_depth=8, num_leaves=50;, score=0.910 total time=  34.1s\n",
      "[CV 1/5; 169/330] START learning_rate=0.07, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 169/330] END learning_rate=0.07, max_depth=8, num_leaves=56;, score=0.909 total time=  24.0s\n",
      "[CV 4/5; 170/330] START learning_rate=0.07, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 170/330] END learning_rate=0.07, max_depth=8, num_leaves=58;, score=0.910 total time=  22.7s\n",
      "[CV 1/5; 172/330] START learning_rate=0.07, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.347901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 172/330] END learning_rate=0.07, max_depth=8, num_leaves=62;, score=0.909 total time=  23.0s\n",
      "[CV 2/5; 173/330] START learning_rate=0.07, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 173/330] END learning_rate=0.07, max_depth=8, num_leaves=64;, score=0.909 total time=  23.3s\n",
      "[CV 5/5; 174/330] START learning_rate=0.07, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 174/330] END learning_rate=0.07, max_depth=8, num_leaves=66;, score=0.909 total time=  35.2s\n",
      "[CV 4/5; 176/330] START learning_rate=0.07, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.236876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 176/330] END learning_rate=0.07, max_depth=8, num_leaves=70;, score=0.910 total time=  35.5s\n",
      "[CV 1/5; 179/330] START learning_rate=0.07, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 179/330] END learning_rate=0.07, max_depth=9, num_leaves=54;, score=0.909 total time=  35.9s\n",
      "[CV 4/5; 180/330] START learning_rate=0.07, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 180/330] END learning_rate=0.07, max_depth=9, num_leaves=56;, score=0.910 total time=  22.3s\n",
      "[CV 4/5; 181/330] START learning_rate=0.07, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.209715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 148/330] END learning_rate=0.05, max_depth=11, num_leaves=58;, score=0.909 total time=  41.4s\n",
      "[CV 3/5; 150/330] START learning_rate=0.05, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 150/330] END learning_rate=0.05, max_depth=11, num_leaves=62;, score=0.909 total time=  28.2s\n",
      "[CV 4/5; 151/330] START learning_rate=0.05, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 151/330] END learning_rate=0.05, max_depth=11, num_leaves=64;, score=0.910 total time=  27.8s\n",
      "[CV 1/5; 153/330] START learning_rate=0.05, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 153/330] END learning_rate=0.05, max_depth=11, num_leaves=68;, score=0.909 total time=  28.6s\n",
      "[CV 3/5; 154/330] START learning_rate=0.05, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 154/330] END learning_rate=0.05, max_depth=11, num_leaves=70;, score=0.910 total time=  28.4s\n",
      "[CV 1/5; 156/330] START learning_rate=0.05, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 156/330] END learning_rate=0.05, max_depth=12, num_leaves=52;, score=0.909 total time=  26.4s\n",
      "[CV 3/5; 157/330] START learning_rate=0.05, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 157/330] END learning_rate=0.05, max_depth=12, num_leaves=54;, score=0.910 total time=  25.7s\n",
      "[CV 5/5; 158/330] START learning_rate=0.05, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 158/330] END learning_rate=0.05, max_depth=12, num_leaves=56;, score=0.909 total time=  30.0s\n",
      "[CV 1/5; 160/330] START learning_rate=0.05, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 160/330] END learning_rate=0.05, max_depth=12, num_leaves=60;, score=0.909 total time=  29.1s\n",
      "[CV 2/5; 161/330] START learning_rate=0.05, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 161/330] END learning_rate=0.05, max_depth=12, num_leaves=62;, score=0.909 total time=  28.8s\n",
      "[CV 5/5; 162/330] START learning_rate=0.05, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 162/330] END learning_rate=0.05, max_depth=12, num_leaves=64;, score=0.909 total time=  29.4s\n",
      "[CV 2/5; 164/330] START learning_rate=0.05, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 164/330] END learning_rate=0.05, max_depth=12, num_leaves=68;, score=0.909 total time=  29.0s\n",
      "[CV 1/5; 165/330] START learning_rate=0.05, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 165/330] END learning_rate=0.05, max_depth=12, num_leaves=70;, score=0.909 total time=  31.0s\n",
      "[CV 5/5; 166/330] START learning_rate=0.07, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 166/330] END learning_rate=0.07, max_depth=8, num_leaves=50;, score=0.909 total time=  23.5s\n",
      "[CV 2/5; 168/330] START learning_rate=0.07, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 168/330] END learning_rate=0.07, max_depth=8, num_leaves=54;, score=0.909 total time=  23.3s\n",
      "[CV 5/5; 169/330] START learning_rate=0.07, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.314972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 169/330] END learning_rate=0.07, max_depth=8, num_leaves=56;, score=0.909 total time=  32.3s\n",
      "[CV 2/5; 172/330] START learning_rate=0.07, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 172/330] END learning_rate=0.07, max_depth=8, num_leaves=62;, score=0.909 total time=  22.5s\n",
      "[CV 3/5; 173/330] START learning_rate=0.07, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 173/330] END learning_rate=0.07, max_depth=8, num_leaves=64;, score=0.910 total time=  23.3s\n",
      "[CV 1/5; 175/330] START learning_rate=0.07, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 175/330] END learning_rate=0.07, max_depth=8, num_leaves=68;, score=0.909 total time=  24.0s\n",
      "[CV 2/5; 176/330] START learning_rate=0.07, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 176/330] END learning_rate=0.07, max_depth=8, num_leaves=70;, score=0.909 total time=  35.3s\n",
      "[CV 5/5; 177/330] START learning_rate=0.07, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131614 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 177/330] END learning_rate=0.07, max_depth=9, num_leaves=50;, score=0.909 total time=  35.2s\n",
      "[CV 5/5; 179/330] START learning_rate=0.07, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 179/330] END learning_rate=0.07, max_depth=9, num_leaves=54;, score=0.909 total time=  34.1s\n",
      "[CV 5/5; 181/330] START learning_rate=0.07, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 151/330] END learning_rate=0.05, max_depth=11, num_leaves=64;, score=0.910 total time=  41.7s\n",
      "[CV 2/5; 153/330] START learning_rate=0.05, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 153/330] END learning_rate=0.05, max_depth=11, num_leaves=68;, score=0.909 total time=  43.6s\n",
      "[CV 3/5; 155/330] START learning_rate=0.05, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 155/330] END learning_rate=0.05, max_depth=12, num_leaves=50;, score=0.910 total time=  25.4s\n",
      "[CV 5/5; 156/330] START learning_rate=0.05, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 156/330] END learning_rate=0.05, max_depth=12, num_leaves=52;, score=0.909 total time=  26.5s\n",
      "[CV 2/5; 158/330] START learning_rate=0.05, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 158/330] END learning_rate=0.05, max_depth=12, num_leaves=56;, score=0.909 total time=  27.6s\n",
      "[CV 4/5; 159/330] START learning_rate=0.05, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 159/330] END learning_rate=0.05, max_depth=12, num_leaves=58;, score=0.910 total time=  42.0s\n",
      "[CV 3/5; 161/330] START learning_rate=0.05, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 161/330] END learning_rate=0.05, max_depth=12, num_leaves=62;, score=0.909 total time=  27.5s\n",
      "[CV 1/5; 163/330] START learning_rate=0.05, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 163/330] END learning_rate=0.05, max_depth=12, num_leaves=66;, score=0.909 total time=  29.6s\n",
      "[CV 4/5; 164/330] START learning_rate=0.05, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 164/330] END learning_rate=0.05, max_depth=12, num_leaves=68;, score=0.910 total time=  30.1s\n",
      "[CV 3/5; 165/330] START learning_rate=0.05, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 165/330] END learning_rate=0.05, max_depth=12, num_leaves=70;, score=0.910 total time=  30.5s\n",
      "[CV 2/5; 167/330] START learning_rate=0.07, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 167/330] END learning_rate=0.07, max_depth=8, num_leaves=52;, score=0.909 total time=  23.5s\n",
      "[CV 3/5; 168/330] START learning_rate=0.07, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 168/330] END learning_rate=0.07, max_depth=8, num_leaves=54;, score=0.910 total time=  23.0s\n",
      "[CV 1/5; 170/330] START learning_rate=0.07, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 170/330] END learning_rate=0.07, max_depth=8, num_leaves=58;, score=0.909 total time=  22.4s\n",
      "[CV 2/5; 171/330] START learning_rate=0.07, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 171/330] END learning_rate=0.07, max_depth=8, num_leaves=60;, score=0.909 total time=  33.6s\n",
      "[CV 4/5; 173/330] START learning_rate=0.07, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 173/330] END learning_rate=0.07, max_depth=8, num_leaves=64;, score=0.910 total time=  22.7s\n",
      "[CV 2/5; 175/330] START learning_rate=0.07, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 2/5; 175/330] END learning_rate=0.07, max_depth=8, num_leaves=68;, score=0.909 total time=  35.1s\n",
      "[CV 5/5; 176/330] START learning_rate=0.07, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 176/330] END learning_rate=0.07, max_depth=8, num_leaves=70;, score=0.909 total time=  24.5s\n",
      "[CV 2/5; 178/330] START learning_rate=0.07, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 178/330] END learning_rate=0.07, max_depth=9, num_leaves=52;, score=0.909 total time=  24.7s\n",
      "[CV 4/5; 179/330] START learning_rate=0.07, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 179/330] END learning_rate=0.07, max_depth=9, num_leaves=54;, score=0.910 total time=  33.9s\n",
      "[CV 2/5; 181/330] START learning_rate=0.07, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 181/330] END learning_rate=0.07, max_depth=9, num_leaves=58;, score=0.909 total time=  23.6s\n",
      "[CV 5/5; 182/330] START learning_rate=0.07, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 182/330] END learning_rate=0.07, max_depth=9, num_leaves=60;, score=0.909 total time=  34.2s\n",
      "[CV 4/5; 184/330] START learning_rate=0.07, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 184/330] END learning_rate=0.07, max_depth=9, num_leaves=64;, score=0.910 total time=  23.6s\n",
      "[CV 1/5; 186/330] START learning_rate=0.07, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 186/330] END learning_rate=0.07, max_depth=9, num_leaves=68;, score=0.909 total time=  37.1s\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 150/330] END learning_rate=0.05, max_depth=11, num_leaves=62;, score=0.909 total time=  42.1s\n",
      "[CV 4/5; 152/330] START learning_rate=0.05, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.397846 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 152/330] END learning_rate=0.05, max_depth=11, num_leaves=66;, score=0.910 total time=  40.8s\n",
      "[CV 5/5; 154/330] START learning_rate=0.05, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 154/330] END learning_rate=0.05, max_depth=11, num_leaves=70;, score=0.909 total time=  27.5s\n",
      "[CV 2/5; 156/330] START learning_rate=0.05, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 156/330] END learning_rate=0.05, max_depth=12, num_leaves=52;, score=0.909 total time=  39.1s\n",
      "[CV 4/5; 158/330] START learning_rate=0.05, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 158/330] END learning_rate=0.05, max_depth=12, num_leaves=56;, score=0.910 total time=  41.4s\n",
      "[CV 2/5; 160/330] START learning_rate=0.05, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 160/330] END learning_rate=0.05, max_depth=12, num_leaves=60;, score=0.909 total time=  42.3s\n",
      "[CV 4/5; 162/330] START learning_rate=0.05, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 162/330] END learning_rate=0.05, max_depth=12, num_leaves=64;, score=0.910 total time=  27.6s\n",
      "[CV 3/5; 163/330] START learning_rate=0.05, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 163/330] END learning_rate=0.05, max_depth=12, num_leaves=66;, score=0.910 total time=  42.9s\n",
      "[CV 4/5; 165/330] START learning_rate=0.05, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 165/330] END learning_rate=0.05, max_depth=12, num_leaves=70;, score=0.910 total time=  30.2s\n",
      "[CV 3/5; 167/330] START learning_rate=0.07, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 167/330] END learning_rate=0.07, max_depth=8, num_leaves=52;, score=0.909 total time=  24.3s\n",
      "[CV 5/5; 168/330] START learning_rate=0.07, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 168/330] END learning_rate=0.07, max_depth=8, num_leaves=54;, score=0.909 total time=  23.9s\n",
      "[CV 3/5; 170/330] START learning_rate=0.07, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 170/330] END learning_rate=0.07, max_depth=8, num_leaves=58;, score=0.910 total time=  21.7s\n",
      "[CV 4/5; 171/330] START learning_rate=0.07, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 171/330] END learning_rate=0.07, max_depth=8, num_leaves=60;, score=0.910 total time=  22.9s\n",
      "[CV 5/5; 172/330] START learning_rate=0.07, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.176997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 172/330] END learning_rate=0.07, max_depth=8, num_leaves=62;, score=0.909 total time=  22.8s\n",
      "[CV 3/5; 174/330] START learning_rate=0.07, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 174/330] END learning_rate=0.07, max_depth=8, num_leaves=66;, score=0.910 total time=  34.8s\n",
      "[CV 3/5; 176/330] START learning_rate=0.07, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 176/330] END learning_rate=0.07, max_depth=8, num_leaves=70;, score=0.910 total time=  36.3s\n",
      "[CV 3/5; 178/330] START learning_rate=0.07, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 178/330] END learning_rate=0.07, max_depth=9, num_leaves=52;, score=0.910 total time=  35.9s\n",
      "[CV 1/5; 180/330] START learning_rate=0.07, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 180/330] END learning_rate=0.07, max_depth=9, num_leaves=56;, score=0.909 total time=  35.2s\n",
      "[CV 2/5; 182/330] START learning_rate=0.07, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 182/330] END learning_rate=0.07, max_depth=9, num_leaves=60;, score=0.909 total time=  23.7s\n",
      "[CV 5/5; 183/330] START learning_rate=0.07, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 183/330] END learning_rate=0.07, max_depth=9, num_leaves=62;, score=0.909 total time=  23.5s\n",
      "[CV 5/5; 184/330] START learning_rate=0.07, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 184/330] END learning_rate=0.07, max_depth=9, num_leaves=64;, score=0.909 total time=  22.5s\n",
      "[CV 2/5; 186/330] START learning_rate=0.07, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 186/330] END learning_rate=0.07, max_depth=9, num_leaves=68;, score=0.909 total time=  23.6s\n",
      "[CV 4/5; 187/330] START learning_rate=0.07, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 177/330] END learning_rate=0.07, max_depth=9, num_leaves=50;, score=0.910 total time=  23.2s\n",
      "[CV 2/5; 179/330] START learning_rate=0.07, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 179/330] END learning_rate=0.07, max_depth=9, num_leaves=54;, score=0.909 total time=  36.0s\n",
      "[CV 5/5; 180/330] START learning_rate=0.07, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 180/330] END learning_rate=0.07, max_depth=9, num_leaves=56;, score=0.909 total time=  34.6s\n",
      "[CV 4/5; 182/330] START learning_rate=0.07, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 182/330] END learning_rate=0.07, max_depth=9, num_leaves=60;, score=0.910 total time=  24.0s\n",
      "[CV 1/5; 184/330] START learning_rate=0.07, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 184/330] END learning_rate=0.07, max_depth=9, num_leaves=64;, score=0.909 total time=  23.9s\n",
      "[CV 2/5; 185/330] START learning_rate=0.07, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 185/330] END learning_rate=0.07, max_depth=9, num_leaves=66;, score=0.909 total time=  23.3s\n",
      "[CV 5/5; 186/330] START learning_rate=0.07, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 186/330] END learning_rate=0.07, max_depth=9, num_leaves=68;, score=0.909 total time=  24.0s\n",
      "[CV 2/5; 188/330] START learning_rate=0.07, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 188/330] END learning_rate=0.07, max_depth=10, num_leaves=50;, score=0.909 total time=  23.3s\n",
      "[CV 3/5; 189/330] START learning_rate=0.07, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 189/330] END learning_rate=0.07, max_depth=10, num_leaves=52;, score=0.910 total time=  23.4s\n",
      "[CV 1/5; 191/330] START learning_rate=0.07, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 191/330] END learning_rate=0.07, max_depth=10, num_leaves=56;, score=0.909 total time=  23.7s\n",
      "[CV 1/5; 192/330] START learning_rate=0.07, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.415488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 192/330] END learning_rate=0.07, max_depth=10, num_leaves=58;, score=0.909 total time=  39.0s\n",
      "[CV 4/5; 194/330] START learning_rate=0.07, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 194/330] END learning_rate=0.07, max_depth=10, num_leaves=62;, score=0.910 total time=  35.7s\n",
      "[CV 2/5; 196/330] START learning_rate=0.07, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 196/330] END learning_rate=0.07, max_depth=10, num_leaves=66;, score=0.909 total time=  26.2s\n",
      "[CV 5/5; 197/330] START learning_rate=0.07, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 197/330] END learning_rate=0.07, max_depth=10, num_leaves=68;, score=0.909 total time=  36.7s\n",
      "[CV 4/5; 199/330] START learning_rate=0.07, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 199/330] END learning_rate=0.07, max_depth=11, num_leaves=50;, score=0.910 total time=  23.8s\n",
      "[CV 2/5; 201/330] START learning_rate=0.07, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 201/330] END learning_rate=0.07, max_depth=11, num_leaves=54;, score=0.909 total time=  25.6s\n",
      "[CV 3/5; 202/330] START learning_rate=0.07, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 202/330] END learning_rate=0.07, max_depth=11, num_leaves=56;, score=0.910 total time=  23.8s\n",
      "[CV 1/5; 204/330] START learning_rate=0.07, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 204/330] END learning_rate=0.07, max_depth=11, num_leaves=60;, score=0.909 total time=  24.0s\n",
      "[CV 3/5; 205/330] START learning_rate=0.07, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 205/330] END learning_rate=0.07, max_depth=11, num_leaves=62;, score=0.910 total time=  24.6s\n",
      "[CV 4/5; 206/330] START learning_rate=0.07, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 206/330] END learning_rate=0.07, max_depth=11, num_leaves=64;, score=0.910 total time=  37.0s\n",
      "[CV 3/5; 208/330] START learning_rate=0.07, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 208/330] END learning_rate=0.07, max_depth=11, num_leaves=68;, score=0.910 total time=  25.9s\n",
      "[CV 5/5; 209/330] START learning_rate=0.07, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 209/330] END learning_rate=0.07, max_depth=11, num_leaves=70;, score=0.909 total time=  26.3s\n",
      "[CV 2/5; 211/330] START learning_rate=0.07, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.242075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 180/330] END learning_rate=0.07, max_depth=9, num_leaves=56;, score=0.909 total time=  23.7s\n",
      "[CV 3/5; 181/330] START learning_rate=0.07, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 181/330] END learning_rate=0.07, max_depth=9, num_leaves=58;, score=0.910 total time=  34.1s\n",
      "[CV 4/5; 183/330] START learning_rate=0.07, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 183/330] END learning_rate=0.07, max_depth=9, num_leaves=62;, score=0.910 total time=  23.7s\n",
      "[CV 1/5; 185/330] START learning_rate=0.07, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 185/330] END learning_rate=0.07, max_depth=9, num_leaves=66;, score=0.909 total time=  23.3s\n",
      "[CV 4/5; 186/330] START learning_rate=0.07, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 186/330] END learning_rate=0.07, max_depth=9, num_leaves=68;, score=0.910 total time=  23.8s\n",
      "[CV 5/5; 187/330] START learning_rate=0.07, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 187/330] END learning_rate=0.07, max_depth=9, num_leaves=70;, score=0.909 total time=  36.1s\n",
      "[CV 1/5; 190/330] START learning_rate=0.07, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 190/330] END learning_rate=0.07, max_depth=10, num_leaves=54;, score=0.909 total time=  23.4s\n",
      "[CV 3/5; 191/330] START learning_rate=0.07, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 191/330] END learning_rate=0.07, max_depth=10, num_leaves=56;, score=0.910 total time=  25.1s\n",
      "[CV 5/5; 192/330] START learning_rate=0.07, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 192/330] END learning_rate=0.07, max_depth=10, num_leaves=58;, score=0.909 total time=  26.3s\n",
      "[CV 2/5; 194/330] START learning_rate=0.07, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 194/330] END learning_rate=0.07, max_depth=10, num_leaves=62;, score=0.909 total time=  23.8s\n",
      "[CV 4/5; 195/330] START learning_rate=0.07, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 195/330] END learning_rate=0.07, max_depth=10, num_leaves=64;, score=0.910 total time=  36.3s\n",
      "[CV 3/5; 197/330] START learning_rate=0.07, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 197/330] END learning_rate=0.07, max_depth=10, num_leaves=68;, score=0.910 total time=  24.9s\n",
      "[CV 1/5; 199/330] START learning_rate=0.07, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 199/330] END learning_rate=0.07, max_depth=11, num_leaves=50;, score=0.909 total time=  33.7s\n",
      "[CV 5/5; 200/330] START learning_rate=0.07, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 200/330] END learning_rate=0.07, max_depth=11, num_leaves=52;, score=0.909 total time=  36.4s\n",
      "[CV 5/5; 202/330] START learning_rate=0.07, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 202/330] END learning_rate=0.07, max_depth=11, num_leaves=56;, score=0.909 total time=  23.8s\n",
      "[CV 2/5; 204/330] START learning_rate=0.07, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 204/330] END learning_rate=0.07, max_depth=11, num_leaves=60;, score=0.909 total time=  24.3s\n",
      "[CV 4/5; 205/330] START learning_rate=0.07, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 205/330] END learning_rate=0.07, max_depth=11, num_leaves=62;, score=0.910 total time=  24.8s\n",
      "[CV 5/5; 206/330] START learning_rate=0.07, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 206/330] END learning_rate=0.07, max_depth=11, num_leaves=64;, score=0.909 total time=  24.9s\n",
      "[CV 1/5; 208/330] START learning_rate=0.07, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 208/330] END learning_rate=0.07, max_depth=11, num_leaves=68;, score=0.909 total time=  39.2s\n",
      "[CV 1/5; 210/330] START learning_rate=0.07, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 210/330] END learning_rate=0.07, max_depth=12, num_leaves=50;, score=0.909 total time=  23.9s\n",
      "[CV 3/5; 211/330] START learning_rate=0.07, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 211/330] END learning_rate=0.07, max_depth=12, num_leaves=52;, score=0.910 total time=  34.5s\n",
      "[CV 2/5; 213/330] START learning_rate=0.07, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 213/330] END learning_rate=0.07, max_depth=12, num_leaves=56;, score=0.909 total time=  24.0s\n",
      "[CV 5/5; 214/330] START learning_rate=0.07, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[CV 1/5; 182/330] START learning_rate=0.07, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 182/330] END learning_rate=0.07, max_depth=9, num_leaves=60;, score=0.909 total time=  24.6s\n",
      "[CV 3/5; 183/330] START learning_rate=0.07, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 183/330] END learning_rate=0.07, max_depth=9, num_leaves=62;, score=0.910 total time=  34.9s\n",
      "[CV 4/5; 185/330] START learning_rate=0.07, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 185/330] END learning_rate=0.07, max_depth=9, num_leaves=66;, score=0.910 total time=  24.5s\n",
      "[CV 1/5; 187/330] START learning_rate=0.07, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 187/330] END learning_rate=0.07, max_depth=9, num_leaves=70;, score=0.909 total time=  24.3s\n",
      "[CV 4/5; 188/330] START learning_rate=0.07, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 188/330] END learning_rate=0.07, max_depth=10, num_leaves=50;, score=0.910 total time=  22.6s\n",
      "[CV 4/5; 189/330] START learning_rate=0.07, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 189/330] END learning_rate=0.07, max_depth=10, num_leaves=52;, score=0.910 total time=  22.6s\n",
      "[CV 2/5; 191/330] START learning_rate=0.07, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 191/330] END learning_rate=0.07, max_depth=10, num_leaves=56;, score=0.909 total time=  24.9s\n",
      "[CV 4/5; 192/330] START learning_rate=0.07, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 192/330] END learning_rate=0.07, max_depth=10, num_leaves=58;, score=0.910 total time=  26.4s\n",
      "[CV 1/5; 194/330] START learning_rate=0.07, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 194/330] END learning_rate=0.07, max_depth=10, num_leaves=62;, score=0.909 total time=  24.6s\n",
      "[CV 3/5; 195/330] START learning_rate=0.07, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 195/330] END learning_rate=0.07, max_depth=10, num_leaves=64;, score=0.910 total time=  24.9s\n",
      "[CV 5/5; 196/330] START learning_rate=0.07, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 196/330] END learning_rate=0.07, max_depth=10, num_leaves=66;, score=0.909 total time=  24.4s\n",
      "[CV 1/5; 198/330] START learning_rate=0.07, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 198/330] END learning_rate=0.07, max_depth=10, num_leaves=70;, score=0.909 total time=  25.8s\n",
      "[CV 3/5; 199/330] START learning_rate=0.07, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 199/330] END learning_rate=0.07, max_depth=11, num_leaves=50;, score=0.910 total time=  24.0s\n",
      "[CV 1/5; 201/330] START learning_rate=0.07, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 201/330] END learning_rate=0.07, max_depth=11, num_leaves=54;, score=0.909 total time=  25.4s\n",
      "[CV 2/5; 202/330] START learning_rate=0.07, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 202/330] END learning_rate=0.07, max_depth=11, num_leaves=56;, score=0.909 total time=  22.9s\n",
      "[CV 5/5; 203/330] START learning_rate=0.07, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 203/330] END learning_rate=0.07, max_depth=11, num_leaves=58;, score=0.909 total time=  36.3s\n",
      "[CV 5/5; 205/330] START learning_rate=0.07, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 205/330] END learning_rate=0.07, max_depth=11, num_leaves=62;, score=0.909 total time=  37.2s\n",
      "[CV 3/5; 207/330] START learning_rate=0.07, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 207/330] END learning_rate=0.07, max_depth=11, num_leaves=66;, score=0.910 total time=  26.2s\n",
      "[CV 5/5; 208/330] START learning_rate=0.07, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 208/330] END learning_rate=0.07, max_depth=11, num_leaves=68;, score=0.909 total time=  38.9s\n",
      "[CV 5/5; 210/330] START learning_rate=0.07, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 210/330] END learning_rate=0.07, max_depth=12, num_leaves=50;, score=0.909 total time=  23.1s\n",
      "[CV 1/5; 212/330] START learning_rate=0.07, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 212/330] END learning_rate=0.07, max_depth=12, num_leaves=54;, score=0.909 total time=  23.4s\n",
      "[CV 4/5; 213/330] START learning_rate=0.07, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 213/330] END learning_rate=0.07, max_depth=12, num_leaves=56;, score=0.910 total time=  23.5s\n",
      "[CV 2/5; 215/330] START learning_rate=0.07, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 181/330] END learning_rate=0.07, max_depth=9, num_leaves=58;, score=0.910 total time=  23.1s\n",
      "[CV 1/5; 183/330] START learning_rate=0.07, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 183/330] END learning_rate=0.07, max_depth=9, num_leaves=62;, score=0.909 total time=  23.8s\n",
      "[CV 3/5; 184/330] START learning_rate=0.07, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 184/330] END learning_rate=0.07, max_depth=9, num_leaves=64;, score=0.910 total time=  24.5s\n",
      "[CV 5/5; 185/330] START learning_rate=0.07, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.365827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 185/330] END learning_rate=0.07, max_depth=9, num_leaves=66;, score=0.909 total time=  35.2s\n",
      "[CV 1/5; 188/330] START learning_rate=0.07, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 188/330] END learning_rate=0.07, max_depth=10, num_leaves=50;, score=0.909 total time=  22.7s\n",
      "[CV 2/5; 189/330] START learning_rate=0.07, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 189/330] END learning_rate=0.07, max_depth=10, num_leaves=52;, score=0.909 total time=  23.6s\n",
      "[CV 5/5; 190/330] START learning_rate=0.07, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 190/330] END learning_rate=0.07, max_depth=10, num_leaves=54;, score=0.909 total time=  35.0s\n",
      "[CV 1/5; 193/330] START learning_rate=0.07, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 193/330] END learning_rate=0.07, max_depth=10, num_leaves=60;, score=0.909 total time=  26.3s\n",
      "[CV 3/5; 194/330] START learning_rate=0.07, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 194/330] END learning_rate=0.07, max_depth=10, num_leaves=62;, score=0.910 total time=  24.9s\n",
      "[CV 5/5; 195/330] START learning_rate=0.07, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 195/330] END learning_rate=0.07, max_depth=10, num_leaves=64;, score=0.909 total time=  36.5s\n",
      "[CV 4/5; 197/330] START learning_rate=0.07, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 197/330] END learning_rate=0.07, max_depth=10, num_leaves=68;, score=0.910 total time=  25.7s\n",
      "[CV 2/5; 199/330] START learning_rate=0.07, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 199/330] END learning_rate=0.07, max_depth=11, num_leaves=50;, score=0.909 total time=  22.8s\n",
      "[CV 3/5; 200/330] START learning_rate=0.07, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 200/330] END learning_rate=0.07, max_depth=11, num_leaves=52;, score=0.910 total time=  24.7s\n",
      "[CV 5/5; 201/330] START learning_rate=0.07, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 201/330] END learning_rate=0.07, max_depth=11, num_leaves=54;, score=0.909 total time=  34.3s\n",
      "[CV 4/5; 203/330] START learning_rate=0.07, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 203/330] END learning_rate=0.07, max_depth=11, num_leaves=58;, score=0.910 total time=  23.9s\n",
      "[CV 1/5; 205/330] START learning_rate=0.07, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 205/330] END learning_rate=0.07, max_depth=11, num_leaves=62;, score=0.909 total time=  36.2s\n",
      "[CV 1/5; 207/330] START learning_rate=0.07, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 207/330] END learning_rate=0.07, max_depth=11, num_leaves=66;, score=0.909 total time=  38.6s\n",
      "[CV 4/5; 208/330] START learning_rate=0.07, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 208/330] END learning_rate=0.07, max_depth=11, num_leaves=68;, score=0.910 total time=  26.7s\n",
      "[CV 2/5; 210/330] START learning_rate=0.07, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 210/330] END learning_rate=0.07, max_depth=12, num_leaves=50;, score=0.909 total time=  23.8s\n",
      "[CV 4/5; 211/330] START learning_rate=0.07, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 211/330] END learning_rate=0.07, max_depth=12, num_leaves=52;, score=0.910 total time=  33.6s\n",
      "[CV 3/5; 213/330] START learning_rate=0.07, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 213/330] END learning_rate=0.07, max_depth=12, num_leaves=56;, score=0.910 total time=  24.1s\n",
      "[CV 1/5; 215/330] START learning_rate=0.07, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 215/330] END learning_rate=0.07, max_depth=12, num_leaves=60;, score=0.909 total time=  24.0s\n",
      "[CV 3/5; 216/330] START learning_rate=0.07, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 178/330] END learning_rate=0.07, max_depth=9, num_leaves=52;, score=0.909 total time=  36.0s\n",
      "[CV 3/5; 180/330] START learning_rate=0.07, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 180/330] END learning_rate=0.07, max_depth=9, num_leaves=56;, score=0.910 total time=  34.4s\n",
      "[CV 3/5; 182/330] START learning_rate=0.07, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 182/330] END learning_rate=0.07, max_depth=9, num_leaves=60;, score=0.910 total time=  34.2s\n",
      "[CV 2/5; 184/330] START learning_rate=0.07, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.203951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 184/330] END learning_rate=0.07, max_depth=9, num_leaves=64;, score=0.909 total time=  35.6s\n",
      "[CV 3/5; 186/330] START learning_rate=0.07, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 186/330] END learning_rate=0.07, max_depth=9, num_leaves=68;, score=0.910 total time=  23.4s\n",
      "[CV 3/5; 187/330] START learning_rate=0.07, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 187/330] END learning_rate=0.07, max_depth=9, num_leaves=70;, score=0.910 total time=  23.5s\n",
      "[CV 1/5; 189/330] START learning_rate=0.07, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.391585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 189/330] END learning_rate=0.07, max_depth=10, num_leaves=52;, score=0.909 total time=  22.8s\n",
      "[CV 4/5; 190/330] START learning_rate=0.07, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 190/330] END learning_rate=0.07, max_depth=10, num_leaves=54;, score=0.910 total time=  36.6s\n",
      "[CV 3/5; 192/330] START learning_rate=0.07, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 192/330] END learning_rate=0.07, max_depth=10, num_leaves=58;, score=0.910 total time=  25.6s\n",
      "[CV 5/5; 193/330] START learning_rate=0.07, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 193/330] END learning_rate=0.07, max_depth=10, num_leaves=60;, score=0.909 total time=  23.9s\n",
      "[CV 2/5; 195/330] START learning_rate=0.07, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.272858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 195/330] END learning_rate=0.07, max_depth=10, num_leaves=64;, score=0.909 total time=  23.8s\n",
      "[CV 4/5; 196/330] START learning_rate=0.07, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.240146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 196/330] END learning_rate=0.07, max_depth=10, num_leaves=66;, score=0.910 total time=  37.0s\n",
      "[CV 5/5; 198/330] START learning_rate=0.07, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 198/330] END learning_rate=0.07, max_depth=10, num_leaves=70;, score=0.909 total time=  35.8s\n",
      "[CV 4/5; 200/330] START learning_rate=0.07, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 200/330] END learning_rate=0.07, max_depth=11, num_leaves=52;, score=0.909 total time=  35.6s\n",
      "[CV 4/5; 202/330] START learning_rate=0.07, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 202/330] END learning_rate=0.07, max_depth=11, num_leaves=56;, score=0.910 total time=  35.4s\n",
      "[CV 4/5; 204/330] START learning_rate=0.07, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 204/330] END learning_rate=0.07, max_depth=11, num_leaves=60;, score=0.910 total time=  35.4s\n",
      "[CV 3/5; 206/330] START learning_rate=0.07, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 206/330] END learning_rate=0.07, max_depth=11, num_leaves=64;, score=0.910 total time=  37.2s\n",
      "[CV 2/5; 208/330] START learning_rate=0.07, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 208/330] END learning_rate=0.07, max_depth=11, num_leaves=68;, score=0.909 total time=  26.4s\n",
      "[CV 4/5; 209/330] START learning_rate=0.07, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.388287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 209/330] END learning_rate=0.07, max_depth=11, num_leaves=70;, score=0.910 total time=  38.2s\n",
      "[CV 5/5; 211/330] START learning_rate=0.07, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 211/330] END learning_rate=0.07, max_depth=12, num_leaves=52;, score=0.909 total time=  23.0s\n",
      "[CV 1/5; 213/330] START learning_rate=0.07, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 213/330] END learning_rate=0.07, max_depth=12, num_leaves=56;, score=0.909 total time=  23.0s\n",
      "[CV 4/5; 214/330] START learning_rate=0.07, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 214/330] END learning_rate=0.07, max_depth=12, num_leaves=58;, score=0.910 total time=  23.2s\n",
      "[CV 1/5; 216/330] START learning_rate=0.07, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 216/330] END learning_rate=0.07, max_depth=12, num_leaves=62;, score=0.909 total time=  37.2s\n",
      "[CV 2/5; 218/330] START learning_rate=0.07, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 181/330] END learning_rate=0.07, max_depth=9, num_leaves=58;, score=0.909 total time=  23.2s\n",
      "[CV 2/5; 183/330] START learning_rate=0.07, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 183/330] END learning_rate=0.07, max_depth=9, num_leaves=62;, score=0.909 total time=  35.6s\n",
      "[CV 3/5; 185/330] START learning_rate=0.07, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 185/330] END learning_rate=0.07, max_depth=9, num_leaves=66;, score=0.910 total time=  34.9s\n",
      "[CV 2/5; 187/330] START learning_rate=0.07, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 187/330] END learning_rate=0.07, max_depth=9, num_leaves=70;, score=0.909 total time=  24.9s\n",
      "[CV 5/5; 188/330] START learning_rate=0.07, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 188/330] END learning_rate=0.07, max_depth=10, num_leaves=50;, score=0.909 total time=  22.8s\n",
      "[CV 2/5; 190/330] START learning_rate=0.07, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 190/330] END learning_rate=0.07, max_depth=10, num_leaves=54;, score=0.909 total time=  35.0s\n",
      "[CV 5/5; 191/330] START learning_rate=0.07, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 191/330] END learning_rate=0.07, max_depth=10, num_leaves=56;, score=0.909 total time=  25.4s\n",
      "[CV 2/5; 193/330] START learning_rate=0.07, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 193/330] END learning_rate=0.07, max_depth=10, num_leaves=60;, score=0.909 total time=  37.5s\n",
      "[CV 1/5; 196/330] START learning_rate=0.07, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 196/330] END learning_rate=0.07, max_depth=10, num_leaves=66;, score=0.909 total time=  24.8s\n",
      "[CV 1/5; 197/330] START learning_rate=0.07, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 197/330] END learning_rate=0.07, max_depth=10, num_leaves=68;, score=0.909 total time=  25.2s\n",
      "[CV 2/5; 198/330] START learning_rate=0.07, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 198/330] END learning_rate=0.07, max_depth=10, num_leaves=70;, score=0.909 total time=  26.4s\n",
      "[CV 1/5; 200/330] START learning_rate=0.07, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 200/330] END learning_rate=0.07, max_depth=11, num_leaves=52;, score=0.909 total time=  24.8s\n",
      "[CV 4/5; 201/330] START learning_rate=0.07, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 201/330] END learning_rate=0.07, max_depth=11, num_leaves=54;, score=0.910 total time=  34.8s\n",
      "[CV 2/5; 203/330] START learning_rate=0.07, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117504 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 203/330] END learning_rate=0.07, max_depth=11, num_leaves=58;, score=0.909 total time=  34.9s\n",
      "[CV 2/5; 205/330] START learning_rate=0.07, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 205/330] END learning_rate=0.07, max_depth=11, num_leaves=62;, score=0.909 total time=  36.9s\n",
      "[CV 2/5; 207/330] START learning_rate=0.07, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 207/330] END learning_rate=0.07, max_depth=11, num_leaves=66;, score=0.909 total time=  39.2s\n",
      "[CV 1/5; 209/330] START learning_rate=0.07, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 209/330] END learning_rate=0.07, max_depth=11, num_leaves=70;, score=0.909 total time=  26.1s\n",
      "[CV 3/5; 210/330] START learning_rate=0.07, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 210/330] END learning_rate=0.07, max_depth=12, num_leaves=50;, score=0.910 total time=  33.5s\n",
      "[CV 2/5; 212/330] START learning_rate=0.07, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 212/330] END learning_rate=0.07, max_depth=12, num_leaves=54;, score=0.909 total time=  23.6s\n",
      "[CV 5/5; 213/330] START learning_rate=0.07, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 213/330] END learning_rate=0.07, max_depth=12, num_leaves=56;, score=0.909 total time=  34.6s\n",
      "[CV 5/5; 215/330] START learning_rate=0.07, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 215/330] END learning_rate=0.07, max_depth=12, num_leaves=60;, score=0.909 total time=  34.9s\n",
      "[CV 5/5; 217/330] START learning_rate=0.07, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.345849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 217/330] END learning_rate=0.07, max_depth=12, num_leaves=64;, score=0.909 total time=  36.1s\n",
      "[CV 1/5; 220/330] START learning_rate=0.07, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 3/5; 188/330] START learning_rate=0.07, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 188/330] END learning_rate=0.07, max_depth=10, num_leaves=50;, score=0.910 total time=  33.2s\n",
      "[CV 3/5; 190/330] START learning_rate=0.07, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 190/330] END learning_rate=0.07, max_depth=10, num_leaves=54;, score=0.910 total time=  35.3s\n",
      "[CV 2/5; 192/330] START learning_rate=0.07, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 192/330] END learning_rate=0.07, max_depth=10, num_leaves=58;, score=0.909 total time=  25.7s\n",
      "[CV 3/5; 193/330] START learning_rate=0.07, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 193/330] END learning_rate=0.07, max_depth=10, num_leaves=60;, score=0.910 total time=  24.5s\n",
      "[CV 5/5; 194/330] START learning_rate=0.07, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 194/330] END learning_rate=0.07, max_depth=10, num_leaves=62;, score=0.909 total time=  35.9s\n",
      "[CV 2/5; 197/330] START learning_rate=0.07, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 197/330] END learning_rate=0.07, max_depth=10, num_leaves=68;, score=0.910 total time=  25.0s\n",
      "[CV 3/5; 198/330] START learning_rate=0.07, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 198/330] END learning_rate=0.07, max_depth=10, num_leaves=70;, score=0.910 total time=  25.6s\n",
      "[CV 5/5; 199/330] START learning_rate=0.07, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 199/330] END learning_rate=0.07, max_depth=11, num_leaves=50;, score=0.909 total time=  24.4s\n",
      "[CV 3/5; 201/330] START learning_rate=0.07, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 201/330] END learning_rate=0.07, max_depth=11, num_leaves=54;, score=0.910 total time=  35.2s\n",
      "[CV 1/5; 203/330] START learning_rate=0.07, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 203/330] END learning_rate=0.07, max_depth=11, num_leaves=58;, score=0.909 total time=  23.7s\n",
      "[CV 3/5; 204/330] START learning_rate=0.07, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 204/330] END learning_rate=0.07, max_depth=11, num_leaves=60;, score=0.910 total time=  35.3s\n",
      "[CV 2/5; 206/330] START learning_rate=0.07, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 206/330] END learning_rate=0.07, max_depth=11, num_leaves=64;, score=0.909 total time=  24.3s\n",
      "[CV 4/5; 207/330] START learning_rate=0.07, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.202349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 207/330] END learning_rate=0.07, max_depth=11, num_leaves=66;, score=0.910 total time=  39.3s\n",
      "[CV 3/5; 209/330] START learning_rate=0.07, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 209/330] END learning_rate=0.07, max_depth=11, num_leaves=70;, score=0.910 total time=  26.1s\n",
      "[CV 1/5; 211/330] START learning_rate=0.07, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 211/330] END learning_rate=0.07, max_depth=12, num_leaves=52;, score=0.909 total time=  23.8s\n",
      "[CV 4/5; 212/330] START learning_rate=0.07, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 212/330] END learning_rate=0.07, max_depth=12, num_leaves=54;, score=0.910 total time=  23.0s\n",
      "[CV 1/5; 214/330] START learning_rate=0.07, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 214/330] END learning_rate=0.07, max_depth=12, num_leaves=58;, score=0.909 total time=  23.9s\n",
      "[CV 3/5; 215/330] START learning_rate=0.07, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 215/330] END learning_rate=0.07, max_depth=12, num_leaves=60;, score=0.910 total time=  34.8s\n",
      "[CV 2/5; 217/330] START learning_rate=0.07, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 217/330] END learning_rate=0.07, max_depth=12, num_leaves=64;, score=0.910 total time=  25.1s\n",
      "[CV 5/5; 218/330] START learning_rate=0.07, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 218/330] END learning_rate=0.07, max_depth=12, num_leaves=66;, score=0.909 total time=  23.6s\n",
      "[CV 2/5; 220/330] START learning_rate=0.07, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 220/330] END learning_rate=0.07, max_depth=12, num_leaves=70;, score=0.909 total time=  23.7s\n",
      "[CV 5/5; 221/330] START learning_rate=0.09, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 221/330] END learning_rate=0.09, max_depth=8, num_leaves=50;, score=0.909 total time=  21.3s\n",
      "[CV 2/5; 223/330] START learning_rate=0.09, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[CV 4/5; 187/330] END learning_rate=0.07, max_depth=9, num_leaves=70;, score=0.910 total time=  36.2s\n",
      "[CV 5/5; 189/330] START learning_rate=0.07, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 189/330] END learning_rate=0.07, max_depth=10, num_leaves=52;, score=0.909 total time=  34.1s\n",
      "[CV 4/5; 191/330] START learning_rate=0.07, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 191/330] END learning_rate=0.07, max_depth=10, num_leaves=56;, score=0.910 total time=  35.3s\n",
      "[CV 4/5; 193/330] START learning_rate=0.07, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 193/330] END learning_rate=0.07, max_depth=10, num_leaves=60;, score=0.910 total time=  25.5s\n",
      "[CV 1/5; 195/330] START learning_rate=0.07, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 195/330] END learning_rate=0.07, max_depth=10, num_leaves=64;, score=0.909 total time=  24.3s\n",
      "[CV 3/5; 196/330] START learning_rate=0.07, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 196/330] END learning_rate=0.07, max_depth=10, num_leaves=66;, score=0.910 total time=  36.0s\n",
      "[CV 4/5; 198/330] START learning_rate=0.07, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 198/330] END learning_rate=0.07, max_depth=10, num_leaves=70;, score=0.910 total time=  26.4s\n",
      "[CV 2/5; 200/330] START learning_rate=0.07, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 200/330] END learning_rate=0.07, max_depth=11, num_leaves=52;, score=0.909 total time=  35.4s\n",
      "[CV 1/5; 202/330] START learning_rate=0.07, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 202/330] END learning_rate=0.07, max_depth=11, num_leaves=56;, score=0.909 total time=  23.8s\n",
      "[CV 3/5; 203/330] START learning_rate=0.07, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 203/330] END learning_rate=0.07, max_depth=11, num_leaves=58;, score=0.910 total time=  24.8s\n",
      "[CV 5/5; 204/330] START learning_rate=0.07, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 204/330] END learning_rate=0.07, max_depth=11, num_leaves=60;, score=0.909 total time=  24.4s\n",
      "[CV 1/5; 206/330] START learning_rate=0.07, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 206/330] END learning_rate=0.07, max_depth=11, num_leaves=64;, score=0.909 total time=  37.4s\n",
      "[CV 5/5; 207/330] START learning_rate=0.07, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 207/330] END learning_rate=0.07, max_depth=11, num_leaves=66;, score=0.909 total time=  25.8s\n",
      "[CV 2/5; 209/330] START learning_rate=0.07, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 209/330] END learning_rate=0.07, max_depth=11, num_leaves=70;, score=0.909 total time=  25.6s\n",
      "[CV 4/5; 210/330] START learning_rate=0.07, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 210/330] END learning_rate=0.07, max_depth=12, num_leaves=50;, score=0.909 total time=  33.6s\n",
      "[CV 3/5; 212/330] START learning_rate=0.07, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 212/330] END learning_rate=0.07, max_depth=12, num_leaves=54;, score=0.909 total time=  34.4s\n",
      "[CV 3/5; 214/330] START learning_rate=0.07, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 214/330] END learning_rate=0.07, max_depth=12, num_leaves=58;, score=0.910 total time=  35.3s\n",
      "[CV 5/5; 216/330] START learning_rate=0.07, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 216/330] END learning_rate=0.07, max_depth=12, num_leaves=62;, score=0.909 total time=  24.0s\n",
      "[CV 1/5; 218/330] START learning_rate=0.07, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 218/330] END learning_rate=0.07, max_depth=12, num_leaves=66;, score=0.909 total time=  23.8s\n",
      "[CV 2/5; 219/330] START learning_rate=0.07, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 219/330] END learning_rate=0.07, max_depth=12, num_leaves=68;, score=0.909 total time=  23.7s\n",
      "[CV 5/5; 220/330] START learning_rate=0.07, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 220/330] END learning_rate=0.07, max_depth=12, num_leaves=70;, score=0.909 total time=  35.7s\n",
      "[CV 3/5; 223/330] START learning_rate=0.09, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 223/330] END learning_rate=0.09, max_depth=8, num_leaves=54;, score=0.910 total time=  20.8s\n",
      "[CV 5/5; 224/330] START learning_rate=0.09, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 224/330] END learning_rate=0.09, max_depth=8, num_leaves=56;, score=0.909 total time=  20.4s\n",
      "[CV 3/5; 226/330] START learning_rate=0.09, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 211/330] END learning_rate=0.07, max_depth=12, num_leaves=52;, score=0.909 total time=  23.3s\n",
      "[CV 5/5; 212/330] START learning_rate=0.07, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 212/330] END learning_rate=0.07, max_depth=12, num_leaves=54;, score=0.909 total time=  23.3s\n",
      "[CV 2/5; 214/330] START learning_rate=0.07, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 214/330] END learning_rate=0.07, max_depth=12, num_leaves=58;, score=0.909 total time=  24.3s\n",
      "[CV 4/5; 215/330] START learning_rate=0.07, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 215/330] END learning_rate=0.07, max_depth=12, num_leaves=60;, score=0.910 total time=  24.0s\n",
      "[CV 1/5; 217/330] START learning_rate=0.07, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 217/330] END learning_rate=0.07, max_depth=12, num_leaves=64;, score=0.909 total time=  24.3s\n",
      "[CV 3/5; 218/330] START learning_rate=0.07, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 218/330] END learning_rate=0.07, max_depth=12, num_leaves=66;, score=0.910 total time=  35.5s\n",
      "[CV 3/5; 220/330] START learning_rate=0.07, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 220/330] END learning_rate=0.07, max_depth=12, num_leaves=70;, score=0.910 total time=  36.7s\n",
      "[CV 4/5; 222/330] START learning_rate=0.09, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 222/330] END learning_rate=0.09, max_depth=8, num_leaves=52;, score=0.910 total time=  20.6s\n",
      "[CV 2/5; 224/330] START learning_rate=0.09, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 224/330] END learning_rate=0.09, max_depth=8, num_leaves=56;, score=0.909 total time=  20.4s\n",
      "[CV 3/5; 225/330] START learning_rate=0.09, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 225/330] END learning_rate=0.09, max_depth=8, num_leaves=58;, score=0.910 total time=  31.6s\n",
      "[CV 2/5; 227/330] START learning_rate=0.09, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 227/330] END learning_rate=0.09, max_depth=8, num_leaves=62;, score=0.909 total time=  20.7s\n",
      "[CV 4/5; 228/330] START learning_rate=0.09, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5; 228/330] END learning_rate=0.09, max_depth=8, num_leaves=64;, score=0.910 total time=  31.8s\n",
      "[CV 1/5; 231/330] START learning_rate=0.09, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 231/330] END learning_rate=0.09, max_depth=8, num_leaves=70;, score=0.909 total time=  21.8s\n",
      "[CV 2/5; 232/330] START learning_rate=0.09, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 232/330] END learning_rate=0.09, max_depth=9, num_leaves=50;, score=0.909 total time=  20.9s\n",
      "[CV 4/5; 233/330] START learning_rate=0.09, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 233/330] END learning_rate=0.09, max_depth=9, num_leaves=52;, score=0.910 total time=  21.7s\n",
      "[CV 1/5; 235/330] START learning_rate=0.09, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 235/330] END learning_rate=0.09, max_depth=9, num_leaves=56;, score=0.909 total time=  25.9s\n",
      "[CV 2/5; 236/330] START learning_rate=0.09, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 236/330] END learning_rate=0.09, max_depth=9, num_leaves=58;, score=0.909 total time=  22.5s\n",
      "[CV 4/5; 237/330] START learning_rate=0.09, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 237/330] END learning_rate=0.09, max_depth=9, num_leaves=60;, score=0.910 total time=  22.0s\n",
      "[CV 1/5; 239/330] START learning_rate=0.09, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 239/330] END learning_rate=0.09, max_depth=9, num_leaves=64;, score=0.909 total time=  33.0s\n",
      "[CV 2/5; 241/330] START learning_rate=0.09, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 241/330] END learning_rate=0.09, max_depth=9, num_leaves=68;, score=0.909 total time=  23.6s\n",
      "[CV 3/5; 242/330] START learning_rate=0.09, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 242/330] END learning_rate=0.09, max_depth=9, num_leaves=70;, score=0.910 total time=  23.0s\n",
      "[CV 4/5; 243/330] START learning_rate=0.09, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 243/330] END learning_rate=0.09, max_depth=10, num_leaves=50;, score=0.910 total time=  23.1s\n",
      "[CV 2/5; 245/330] START learning_rate=0.09, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 215/330] END learning_rate=0.07, max_depth=12, num_leaves=60;, score=0.909 total time=  23.8s\n",
      "[CV 4/5; 216/330] START learning_rate=0.07, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.278719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 216/330] END learning_rate=0.07, max_depth=12, num_leaves=62;, score=0.910 total time=  35.1s\n",
      "[CV 4/5; 218/330] START learning_rate=0.07, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 218/330] END learning_rate=0.07, max_depth=12, num_leaves=66;, score=0.910 total time=  24.3s\n",
      "[CV 5/5; 219/330] START learning_rate=0.07, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 219/330] END learning_rate=0.07, max_depth=12, num_leaves=68;, score=0.909 total time=  24.5s\n",
      "[CV 3/5; 221/330] START learning_rate=0.09, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 221/330] END learning_rate=0.09, max_depth=8, num_leaves=50;, score=0.910 total time=  20.7s\n",
      "[CV 5/5; 222/330] START learning_rate=0.09, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 222/330] END learning_rate=0.09, max_depth=8, num_leaves=52;, score=0.909 total time=  29.7s\n",
      "[CV 1/5; 225/330] START learning_rate=0.09, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 225/330] END learning_rate=0.09, max_depth=8, num_leaves=58;, score=0.909 total time=  21.5s\n",
      "[CV 4/5; 226/330] START learning_rate=0.09, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 226/330] END learning_rate=0.09, max_depth=8, num_leaves=60;, score=0.910 total time=  32.0s\n",
      "[CV 2/5; 228/330] START learning_rate=0.09, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 228/330] END learning_rate=0.09, max_depth=8, num_leaves=64;, score=0.909 total time=  21.7s\n",
      "[CV 4/5; 229/330] START learning_rate=0.09, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.301251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 229/330] END learning_rate=0.09, max_depth=8, num_leaves=66;, score=0.910 total time=  31.2s\n",
      "[CV 3/5; 231/330] START learning_rate=0.09, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 231/330] END learning_rate=0.09, max_depth=8, num_leaves=70;, score=0.910 total time=  22.4s\n",
      "[CV 1/5; 233/330] START learning_rate=0.09, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 233/330] END learning_rate=0.09, max_depth=9, num_leaves=52;, score=0.909 total time=  21.6s\n",
      "[CV 2/5; 234/330] START learning_rate=0.09, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107600 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 234/330] END learning_rate=0.09, max_depth=9, num_leaves=54;, score=0.909 total time=  34.5s\n",
      "[CV 3/5; 236/330] START learning_rate=0.09, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 236/330] END learning_rate=0.09, max_depth=9, num_leaves=58;, score=0.910 total time=  23.4s\n",
      "[CV 1/5; 238/330] START learning_rate=0.09, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 238/330] END learning_rate=0.09, max_depth=9, num_leaves=62;, score=0.909 total time=  22.7s\n",
      "[CV 4/5; 239/330] START learning_rate=0.09, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 239/330] END learning_rate=0.09, max_depth=9, num_leaves=64;, score=0.909 total time=  22.8s\n",
      "[CV 2/5; 240/330] START learning_rate=0.09, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 240/330] END learning_rate=0.09, max_depth=9, num_leaves=66;, score=0.909 total time=  25.4s\n",
      "[CV 5/5; 241/330] START learning_rate=0.09, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 241/330] END learning_rate=0.09, max_depth=9, num_leaves=68;, score=0.909 total time=  23.7s\n",
      "[CV 2/5; 243/330] START learning_rate=0.09, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 243/330] END learning_rate=0.09, max_depth=10, num_leaves=50;, score=0.909 total time=  22.5s\n",
      "[CV 4/5; 244/330] START learning_rate=0.09, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 244/330] END learning_rate=0.09, max_depth=10, num_leaves=52;, score=0.910 total time=  21.6s\n",
      "[CV 2/5; 246/330] START learning_rate=0.09, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 246/330] END learning_rate=0.09, max_depth=10, num_leaves=56;, score=0.909 total time=  23.6s\n",
      "[CV 5/5; 247/330] START learning_rate=0.09, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 247/330] END learning_rate=0.09, max_depth=10, num_leaves=58;, score=0.909 total time=  22.7s\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 216/330] END learning_rate=0.07, max_depth=12, num_leaves=62;, score=0.910 total time=  23.3s\n",
      "[CV 4/5; 217/330] START learning_rate=0.07, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 217/330] END learning_rate=0.07, max_depth=12, num_leaves=64;, score=0.910 total time=  24.2s\n",
      "[CV 1/5; 219/330] START learning_rate=0.07, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 219/330] END learning_rate=0.07, max_depth=12, num_leaves=68;, score=0.909 total time=  24.6s\n",
      "[CV 4/5; 220/330] START learning_rate=0.07, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 220/330] END learning_rate=0.07, max_depth=12, num_leaves=70;, score=0.910 total time=  24.9s\n",
      "[CV 1/5; 222/330] START learning_rate=0.09, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 222/330] END learning_rate=0.09, max_depth=8, num_leaves=52;, score=0.909 total time=  20.9s\n",
      "[CV 4/5; 223/330] START learning_rate=0.09, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 223/330] END learning_rate=0.09, max_depth=8, num_leaves=54;, score=0.910 total time=  29.0s\n",
      "[CV 4/5; 225/330] START learning_rate=0.09, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 225/330] END learning_rate=0.09, max_depth=8, num_leaves=58;, score=0.910 total time=  22.6s\n",
      "[CV 5/5; 226/330] START learning_rate=0.09, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 226/330] END learning_rate=0.09, max_depth=8, num_leaves=60;, score=0.909 total time=  31.4s\n",
      "[CV 5/5; 228/330] START learning_rate=0.09, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 228/330] END learning_rate=0.09, max_depth=8, num_leaves=64;, score=0.909 total time=  21.6s\n",
      "[CV 1/5; 230/330] START learning_rate=0.09, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 230/330] END learning_rate=0.09, max_depth=8, num_leaves=68;, score=0.909 total time=  21.1s\n",
      "[CV 4/5; 231/330] START learning_rate=0.09, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 231/330] END learning_rate=0.09, max_depth=8, num_leaves=70;, score=0.910 total time=  21.8s\n",
      "[CV 2/5; 233/330] START learning_rate=0.09, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 233/330] END learning_rate=0.09, max_depth=9, num_leaves=52;, score=0.909 total time=  21.9s\n",
      "[CV 3/5; 234/330] START learning_rate=0.09, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.259354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 234/330] END learning_rate=0.09, max_depth=9, num_leaves=54;, score=0.910 total time=  22.9s\n",
      "[CV 5/5; 235/330] START learning_rate=0.09, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 235/330] END learning_rate=0.09, max_depth=9, num_leaves=56;, score=0.909 total time=  33.8s\n",
      "[CV 5/5; 237/330] START learning_rate=0.09, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 237/330] END learning_rate=0.09, max_depth=9, num_leaves=60;, score=0.909 total time=  21.8s\n",
      "[CV 3/5; 239/330] START learning_rate=0.09, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 239/330] END learning_rate=0.09, max_depth=9, num_leaves=64;, score=0.910 total time=  32.6s\n",
      "[CV 3/5; 241/330] START learning_rate=0.09, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 241/330] END learning_rate=0.09, max_depth=9, num_leaves=68;, score=0.910 total time=  34.2s\n",
      "[CV 1/5; 243/330] START learning_rate=0.09, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 243/330] END learning_rate=0.09, max_depth=10, num_leaves=50;, score=0.909 total time=  23.6s\n",
      "[CV 3/5; 244/330] START learning_rate=0.09, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 244/330] END learning_rate=0.09, max_depth=10, num_leaves=52;, score=0.910 total time=  21.7s\n",
      "[CV 1/5; 246/330] START learning_rate=0.09, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 246/330] END learning_rate=0.09, max_depth=10, num_leaves=56;, score=0.909 total time=  22.5s\n",
      "[CV 3/5; 247/330] START learning_rate=0.09, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 247/330] END learning_rate=0.09, max_depth=10, num_leaves=58;, score=0.910 total time=  32.8s\n",
      "[CV 3/5; 249/330] START learning_rate=0.09, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 249/330] END learning_rate=0.09, max_depth=10, num_leaves=62;, score=0.910 total time=  33.0s\n",
      "[CV 3/5; 251/330] START learning_rate=0.09, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 218/330] END learning_rate=0.07, max_depth=12, num_leaves=66;, score=0.909 total time=  23.6s\n",
      "[CV 3/5; 219/330] START learning_rate=0.07, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 219/330] END learning_rate=0.07, max_depth=12, num_leaves=68;, score=0.910 total time=  24.2s\n",
      "[CV 1/5; 221/330] START learning_rate=0.09, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 221/330] END learning_rate=0.09, max_depth=8, num_leaves=50;, score=0.909 total time=  20.6s\n",
      "[CV 2/5; 222/330] START learning_rate=0.09, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 222/330] END learning_rate=0.09, max_depth=8, num_leaves=52;, score=0.909 total time=  20.2s\n",
      "[CV 5/5; 223/330] START learning_rate=0.09, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 223/330] END learning_rate=0.09, max_depth=8, num_leaves=54;, score=0.909 total time=  30.1s\n",
      "[CV 5/5; 225/330] START learning_rate=0.09, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 225/330] END learning_rate=0.09, max_depth=8, num_leaves=58;, score=0.909 total time=  30.8s\n",
      "[CV 3/5; 227/330] START learning_rate=0.09, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 227/330] END learning_rate=0.09, max_depth=8, num_leaves=62;, score=0.910 total time=  20.8s\n",
      "[CV 1/5; 229/330] START learning_rate=0.09, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 229/330] END learning_rate=0.09, max_depth=8, num_leaves=66;, score=0.909 total time=  21.7s\n",
      "[CV 2/5; 230/330] START learning_rate=0.09, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 2/5; 230/330] END learning_rate=0.09, max_depth=8, num_leaves=68;, score=0.909 total time=  31.8s\n",
      "[CV 3/5; 232/330] START learning_rate=0.09, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 232/330] END learning_rate=0.09, max_depth=9, num_leaves=50;, score=0.909 total time=  30.7s\n",
      "[CV 1/5; 234/330] START learning_rate=0.09, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 234/330] END learning_rate=0.09, max_depth=9, num_leaves=54;, score=0.909 total time=  23.5s\n",
      "[CV 4/5; 235/330] START learning_rate=0.09, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 235/330] END learning_rate=0.09, max_depth=9, num_leaves=56;, score=0.910 total time=  33.5s\n",
      "[CV 3/5; 237/330] START learning_rate=0.09, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 237/330] END learning_rate=0.09, max_depth=9, num_leaves=60;, score=0.910 total time=  22.5s\n",
      "[CV 2/5; 239/330] START learning_rate=0.09, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 239/330] END learning_rate=0.09, max_depth=9, num_leaves=64;, score=0.909 total time=  21.7s\n",
      "[CV 1/5; 240/330] START learning_rate=0.09, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 240/330] END learning_rate=0.09, max_depth=9, num_leaves=66;, score=0.909 total time=  24.5s\n",
      "[CV 4/5; 241/330] START learning_rate=0.09, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 241/330] END learning_rate=0.09, max_depth=9, num_leaves=68;, score=0.910 total time=  34.4s\n",
      "[CV 5/5; 243/330] START learning_rate=0.09, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 243/330] END learning_rate=0.09, max_depth=10, num_leaves=50;, score=0.909 total time=  32.3s\n",
      "[CV 5/5; 245/330] START learning_rate=0.09, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 245/330] END learning_rate=0.09, max_depth=10, num_leaves=54;, score=0.909 total time=  22.9s\n",
      "[CV 4/5; 247/330] START learning_rate=0.09, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 247/330] END learning_rate=0.09, max_depth=10, num_leaves=58;, score=0.909 total time=  32.6s\n",
      "[CV 2/5; 249/330] START learning_rate=0.09, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 249/330] END learning_rate=0.09, max_depth=10, num_leaves=62;, score=0.909 total time=  22.3s\n",
      "[CV 3/5; 250/330] START learning_rate=0.09, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 250/330] END learning_rate=0.09, max_depth=10, num_leaves=64;, score=0.910 total time=  23.2s\n",
      "[CV 1/5; 252/330] START learning_rate=0.09, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 252/330] END learning_rate=0.09, max_depth=10, num_leaves=68;, score=0.909 total time=  22.7s\n",
      "[CV 3/5; 253/330] START learning_rate=0.09, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 214/330] END learning_rate=0.07, max_depth=12, num_leaves=58;, score=0.909 total time=  23.8s\n",
      "[CV 2/5; 216/330] START learning_rate=0.07, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 216/330] END learning_rate=0.07, max_depth=12, num_leaves=62;, score=0.909 total time=  24.6s\n",
      "[CV 3/5; 217/330] START learning_rate=0.07, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 217/330] END learning_rate=0.07, max_depth=12, num_leaves=64;, score=0.909 total time=  36.3s\n",
      "[CV 4/5; 219/330] START learning_rate=0.07, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 219/330] END learning_rate=0.07, max_depth=12, num_leaves=68;, score=0.910 total time=  23.7s\n",
      "[CV 2/5; 221/330] START learning_rate=0.09, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 221/330] END learning_rate=0.09, max_depth=8, num_leaves=50;, score=0.909 total time=  20.1s\n",
      "[CV 3/5; 222/330] START learning_rate=0.09, max_depth=8, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 222/330] END learning_rate=0.09, max_depth=8, num_leaves=52;, score=0.910 total time=  21.5s\n",
      "[CV 1/5; 224/330] START learning_rate=0.09, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 224/330] END learning_rate=0.09, max_depth=8, num_leaves=56;, score=0.909 total time=  20.6s\n",
      "[CV 2/5; 225/330] START learning_rate=0.09, max_depth=8, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 225/330] END learning_rate=0.09, max_depth=8, num_leaves=58;, score=0.909 total time=  31.1s\n",
      "[CV 1/5; 227/330] START learning_rate=0.09, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 227/330] END learning_rate=0.09, max_depth=8, num_leaves=62;, score=0.909 total time=  20.8s\n",
      "[CV 3/5; 228/330] START learning_rate=0.09, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 228/330] END learning_rate=0.09, max_depth=8, num_leaves=64;, score=0.910 total time=  31.8s\n",
      "[CV 5/5; 230/330] START learning_rate=0.09, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 230/330] END learning_rate=0.09, max_depth=8, num_leaves=68;, score=0.909 total time=  31.7s\n",
      "[CV 4/5; 232/330] START learning_rate=0.09, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 232/330] END learning_rate=0.09, max_depth=9, num_leaves=50;, score=0.910 total time=  30.6s\n",
      "[CV 4/5; 234/330] START learning_rate=0.09, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 234/330] END learning_rate=0.09, max_depth=9, num_leaves=54;, score=0.910 total time=  22.3s\n",
      "[CV 1/5; 236/330] START learning_rate=0.09, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 236/330] END learning_rate=0.09, max_depth=9, num_leaves=58;, score=0.909 total time=  24.0s\n",
      "[CV 2/5; 237/330] START learning_rate=0.09, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 237/330] END learning_rate=0.09, max_depth=9, num_leaves=60;, score=0.909 total time=  21.4s\n",
      "[CV 4/5; 238/330] START learning_rate=0.09, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 238/330] END learning_rate=0.09, max_depth=9, num_leaves=62;, score=0.910 total time=  33.5s\n",
      "[CV 4/5; 240/330] START learning_rate=0.09, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 240/330] END learning_rate=0.09, max_depth=9, num_leaves=66;, score=0.910 total time=  35.2s\n",
      "[CV 4/5; 242/330] START learning_rate=0.09, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 242/330] END learning_rate=0.09, max_depth=9, num_leaves=70;, score=0.910 total time=  35.9s\n",
      "[CV 5/5; 244/330] START learning_rate=0.09, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 244/330] END learning_rate=0.09, max_depth=10, num_leaves=52;, score=0.909 total time=  21.7s\n",
      "[CV 3/5; 246/330] START learning_rate=0.09, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 246/330] END learning_rate=0.09, max_depth=10, num_leaves=56;, score=0.910 total time=  23.5s\n",
      "[CV 1/5; 248/330] START learning_rate=0.09, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 248/330] END learning_rate=0.09, max_depth=10, num_leaves=60;, score=0.909 total time=  33.0s\n",
      "[CV 5/5; 249/330] START learning_rate=0.09, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 249/330] END learning_rate=0.09, max_depth=10, num_leaves=62;, score=0.909 total time=  21.3s\n",
      "[CV 4/5; 250/330] START learning_rate=0.09, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[CV 1/5; 220/330] END learning_rate=0.07, max_depth=12, num_leaves=70;, score=0.909 total time=  24.9s\n",
      "[CV 4/5; 221/330] START learning_rate=0.09, max_depth=8, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 221/330] END learning_rate=0.09, max_depth=8, num_leaves=50;, score=0.910 total time=  20.2s\n",
      "[CV 1/5; 223/330] START learning_rate=0.09, max_depth=8, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 223/330] END learning_rate=0.09, max_depth=8, num_leaves=54;, score=0.909 total time=  20.4s\n",
      "[CV 3/5; 224/330] START learning_rate=0.09, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 224/330] END learning_rate=0.09, max_depth=8, num_leaves=56;, score=0.910 total time=  21.0s\n",
      "[CV 1/5; 226/330] START learning_rate=0.09, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 226/330] END learning_rate=0.09, max_depth=8, num_leaves=60;, score=0.909 total time=  31.8s\n",
      "[CV 4/5; 227/330] START learning_rate=0.09, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 227/330] END learning_rate=0.09, max_depth=8, num_leaves=62;, score=0.910 total time=  21.1s\n",
      "[CV 2/5; 229/330] START learning_rate=0.09, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 229/330] END learning_rate=0.09, max_depth=8, num_leaves=66;, score=0.909 total time=  21.2s\n",
      "[CV 3/5; 230/330] START learning_rate=0.09, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 230/330] END learning_rate=0.09, max_depth=8, num_leaves=68;, score=0.910 total time=  21.9s\n",
      "[CV 5/5; 231/330] START learning_rate=0.09, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 231/330] END learning_rate=0.09, max_depth=8, num_leaves=70;, score=0.909 total time=  22.4s\n",
      "[CV 3/5; 233/330] START learning_rate=0.09, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.132225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 233/330] END learning_rate=0.09, max_depth=9, num_leaves=52;, score=0.910 total time=  31.1s\n",
      "[CV 2/5; 235/330] START learning_rate=0.09, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 235/330] END learning_rate=0.09, max_depth=9, num_leaves=56;, score=0.909 total time=  36.4s\n",
      "[CV 1/5; 237/330] START learning_rate=0.09, max_depth=9, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 237/330] END learning_rate=0.09, max_depth=9, num_leaves=60;, score=0.909 total time=  21.5s\n",
      "[CV 3/5; 238/330] START learning_rate=0.09, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 238/330] END learning_rate=0.09, max_depth=9, num_leaves=62;, score=0.910 total time=  32.9s\n",
      "[CV 3/5; 240/330] START learning_rate=0.09, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 240/330] END learning_rate=0.09, max_depth=9, num_leaves=66;, score=0.909 total time=  25.9s\n",
      "[CV 1/5; 242/330] START learning_rate=0.09, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.142156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 242/330] END learning_rate=0.09, max_depth=9, num_leaves=70;, score=0.909 total time=  35.5s\n",
      "[CV 1/5; 244/330] START learning_rate=0.09, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 244/330] END learning_rate=0.09, max_depth=10, num_leaves=52;, score=0.909 total time=  22.2s\n",
      "[CV 3/5; 245/330] START learning_rate=0.09, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 245/330] END learning_rate=0.09, max_depth=10, num_leaves=54;, score=0.910 total time=  21.9s\n",
      "[CV 1/5; 247/330] START learning_rate=0.09, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.202273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 247/330] END learning_rate=0.09, max_depth=10, num_leaves=58;, score=0.909 total time=  34.2s\n",
      "[CV 5/5; 248/330] START learning_rate=0.09, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 248/330] END learning_rate=0.09, max_depth=10, num_leaves=60;, score=0.909 total time=  31.9s\n",
      "[CV 5/5; 250/330] START learning_rate=0.09, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 250/330] END learning_rate=0.09, max_depth=10, num_leaves=64;, score=0.909 total time=  23.4s\n",
      "[CV 2/5; 252/330] START learning_rate=0.09, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 252/330] END learning_rate=0.09, max_depth=10, num_leaves=68;, score=0.909 total time=  21.9s\n",
      "[CV 5/5; 253/330] START learning_rate=0.09, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 253/330] END learning_rate=0.09, max_depth=10, num_leaves=70;, score=0.909 total time=  33.8s\n",
      "[CV 5/5; 255/330] START learning_rate=0.09, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 255/330] END learning_rate=0.09, max_depth=11, num_leaves=52;, score=0.909 total time=  33.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 223/330] END learning_rate=0.09, max_depth=8, num_leaves=54;, score=0.909 total time=  20.0s\n",
      "[CV 4/5; 224/330] START learning_rate=0.09, max_depth=8, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 224/330] END learning_rate=0.09, max_depth=8, num_leaves=56;, score=0.910 total time=  20.2s\n",
      "[CV 2/5; 226/330] START learning_rate=0.09, max_depth=8, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 226/330] END learning_rate=0.09, max_depth=8, num_leaves=60;, score=0.909 total time=  32.8s\n",
      "[CV 5/5; 227/330] START learning_rate=0.09, max_depth=8, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 227/330] END learning_rate=0.09, max_depth=8, num_leaves=62;, score=0.909 total time=  20.9s\n",
      "[CV 3/5; 229/330] START learning_rate=0.09, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 229/330] END learning_rate=0.09, max_depth=8, num_leaves=66;, score=0.910 total time=  21.6s\n",
      "[CV 4/5; 230/330] START learning_rate=0.09, max_depth=8, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5; 230/330] END learning_rate=0.09, max_depth=8, num_leaves=68;, score=0.910 total time=  22.4s\n",
      "[CV 1/5; 232/330] START learning_rate=0.09, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 232/330] END learning_rate=0.09, max_depth=9, num_leaves=50;, score=0.909 total time=  30.9s\n",
      "[CV 5/5; 233/330] START learning_rate=0.09, max_depth=9, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 233/330] END learning_rate=0.09, max_depth=9, num_leaves=52;, score=0.909 total time=  23.1s\n",
      "[CV 3/5; 235/330] START learning_rate=0.09, max_depth=9, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 235/330] END learning_rate=0.09, max_depth=9, num_leaves=56;, score=0.909 total time=  24.3s\n",
      "[CV 5/5; 236/330] START learning_rate=0.09, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 236/330] END learning_rate=0.09, max_depth=9, num_leaves=58;, score=0.909 total time=  32.2s\n",
      "[CV 5/5; 238/330] START learning_rate=0.09, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 238/330] END learning_rate=0.09, max_depth=9, num_leaves=62;, score=0.909 total time=  32.0s\n",
      "[CV 1/5; 241/330] START learning_rate=0.09, max_depth=9, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 241/330] END learning_rate=0.09, max_depth=9, num_leaves=68;, score=0.909 total time=  24.8s\n",
      "[CV 2/5; 242/330] START learning_rate=0.09, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.218059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 242/330] END learning_rate=0.09, max_depth=9, num_leaves=70;, score=0.909 total time=  24.0s\n",
      "[CV 3/5; 243/330] START learning_rate=0.09, max_depth=10, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 243/330] END learning_rate=0.09, max_depth=10, num_leaves=50;, score=0.910 total time=  22.3s\n",
      "[CV 1/5; 245/330] START learning_rate=0.09, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 245/330] END learning_rate=0.09, max_depth=10, num_leaves=54;, score=0.909 total time=  21.4s\n",
      "[CV 4/5; 246/330] START learning_rate=0.09, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 246/330] END learning_rate=0.09, max_depth=10, num_leaves=56;, score=0.910 total time=  33.5s\n",
      "[CV 3/5; 248/330] START learning_rate=0.09, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.195366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 248/330] END learning_rate=0.09, max_depth=10, num_leaves=60;, score=0.910 total time=  31.8s\n",
      "[CV 2/5; 250/330] START learning_rate=0.09, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 250/330] END learning_rate=0.09, max_depth=10, num_leaves=64;, score=0.909 total time=  23.0s\n",
      "[CV 5/5; 251/330] START learning_rate=0.09, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 251/330] END learning_rate=0.09, max_depth=10, num_leaves=66;, score=0.909 total time=  32.6s\n",
      "[CV 4/5; 253/330] START learning_rate=0.09, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 253/330] END learning_rate=0.09, max_depth=10, num_leaves=70;, score=0.910 total time=  34.4s\n",
      "[CV 4/5; 255/330] START learning_rate=0.09, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 255/330] END learning_rate=0.09, max_depth=11, num_leaves=52;, score=0.910 total time=  22.5s\n",
      "[CV 1/5; 257/330] START learning_rate=0.09, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 257/330] END learning_rate=0.09, max_depth=11, num_leaves=56;, score=0.909 total time=  24.0s\n",
      "[CV 2/5; 258/330] START learning_rate=0.09, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 226/330] END learning_rate=0.09, max_depth=8, num_leaves=60;, score=0.910 total time=  31.6s\n",
      "[CV 1/5; 228/330] START learning_rate=0.09, max_depth=8, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 228/330] END learning_rate=0.09, max_depth=8, num_leaves=64;, score=0.909 total time=  31.0s\n",
      "[CV 5/5; 229/330] START learning_rate=0.09, max_depth=8, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 229/330] END learning_rate=0.09, max_depth=8, num_leaves=66;, score=0.909 total time=  21.5s\n",
      "[CV 2/5; 231/330] START learning_rate=0.09, max_depth=8, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 231/330] END learning_rate=0.09, max_depth=8, num_leaves=70;, score=0.909 total time=  21.7s\n",
      "[CV 5/5; 232/330] START learning_rate=0.09, max_depth=9, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 232/330] END learning_rate=0.09, max_depth=9, num_leaves=50;, score=0.909 total time=  31.1s\n",
      "[CV 5/5; 234/330] START learning_rate=0.09, max_depth=9, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 234/330] END learning_rate=0.09, max_depth=9, num_leaves=54;, score=0.909 total time=  34.1s\n",
      "[CV 4/5; 236/330] START learning_rate=0.09, max_depth=9, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 236/330] END learning_rate=0.09, max_depth=9, num_leaves=58;, score=0.910 total time=  22.5s\n",
      "[CV 2/5; 238/330] START learning_rate=0.09, max_depth=9, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 238/330] END learning_rate=0.09, max_depth=9, num_leaves=62;, score=0.909 total time=  21.7s\n",
      "[CV 5/5; 239/330] START learning_rate=0.09, max_depth=9, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 239/330] END learning_rate=0.09, max_depth=9, num_leaves=64;, score=0.909 total time=  22.8s\n",
      "[CV 5/5; 240/330] START learning_rate=0.09, max_depth=9, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 240/330] END learning_rate=0.09, max_depth=9, num_leaves=66;, score=0.909 total time=  35.9s\n",
      "[CV 5/5; 242/330] START learning_rate=0.09, max_depth=9, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 242/330] END learning_rate=0.09, max_depth=9, num_leaves=70;, score=0.909 total time=  24.1s\n",
      "[CV 2/5; 244/330] START learning_rate=0.09, max_depth=10, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.218424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 244/330] END learning_rate=0.09, max_depth=10, num_leaves=52;, score=0.909 total time=  22.7s\n",
      "[CV 4/5; 245/330] START learning_rate=0.09, max_depth=10, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 245/330] END learning_rate=0.09, max_depth=10, num_leaves=54;, score=0.910 total time=  22.7s\n",
      "[CV 2/5; 247/330] START learning_rate=0.09, max_depth=10, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 247/330] END learning_rate=0.09, max_depth=10, num_leaves=58;, score=0.909 total time=  23.5s\n",
      "[CV 4/5; 248/330] START learning_rate=0.09, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 248/330] END learning_rate=0.09, max_depth=10, num_leaves=60;, score=0.910 total time=  22.4s\n",
      "[CV 1/5; 250/330] START learning_rate=0.09, max_depth=10, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 250/330] END learning_rate=0.09, max_depth=10, num_leaves=64;, score=0.909 total time=  22.5s\n",
      "[CV 2/5; 251/330] START learning_rate=0.09, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 251/330] END learning_rate=0.09, max_depth=10, num_leaves=66;, score=0.909 total time=  32.7s\n",
      "[CV 1/5; 253/330] START learning_rate=0.09, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 253/330] END learning_rate=0.09, max_depth=10, num_leaves=70;, score=0.909 total time=  22.4s\n",
      "[CV 3/5; 254/330] START learning_rate=0.09, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 254/330] END learning_rate=0.09, max_depth=11, num_leaves=50;, score=0.910 total time=  31.4s\n",
      "[CV 3/5; 256/330] START learning_rate=0.09, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 256/330] END learning_rate=0.09, max_depth=11, num_leaves=54;, score=0.910 total time=  32.3s\n",
      "[CV 1/5; 258/330] START learning_rate=0.09, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 258/330] END learning_rate=0.09, max_depth=11, num_leaves=58;, score=0.909 total time=  23.7s\n",
      "[CV 4/5; 259/330] START learning_rate=0.09, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 259/330] END learning_rate=0.09, max_depth=11, num_leaves=60;, score=0.910 total time=  31.6s\n",
      "[CV 5/5; 261/330] START learning_rate=0.09, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 245/330] END learning_rate=0.09, max_depth=10, num_leaves=54;, score=0.909 total time=  22.1s\n",
      "[CV 5/5; 246/330] START learning_rate=0.09, max_depth=10, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 246/330] END learning_rate=0.09, max_depth=10, num_leaves=56;, score=0.909 total time=  23.7s\n",
      "[CV 2/5; 248/330] START learning_rate=0.09, max_depth=10, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 248/330] END learning_rate=0.09, max_depth=10, num_leaves=60;, score=0.909 total time=  21.7s\n",
      "[CV 4/5; 249/330] START learning_rate=0.09, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 249/330] END learning_rate=0.09, max_depth=10, num_leaves=62;, score=0.910 total time=  31.9s\n",
      "[CV 4/5; 251/330] START learning_rate=0.09, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.462225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 251/330] END learning_rate=0.09, max_depth=10, num_leaves=66;, score=0.910 total time=  32.5s\n",
      "[CV 2/5; 253/330] START learning_rate=0.09, max_depth=10, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 253/330] END learning_rate=0.09, max_depth=10, num_leaves=70;, score=0.909 total time=  22.4s\n",
      "[CV 4/5; 254/330] START learning_rate=0.09, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 254/330] END learning_rate=0.09, max_depth=11, num_leaves=50;, score=0.910 total time=  21.6s\n",
      "[CV 1/5; 256/330] START learning_rate=0.09, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 256/330] END learning_rate=0.09, max_depth=11, num_leaves=54;, score=0.909 total time=  23.2s\n",
      "[CV 3/5; 257/330] START learning_rate=0.09, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 257/330] END learning_rate=0.09, max_depth=11, num_leaves=56;, score=0.910 total time=  23.3s\n",
      "[CV 5/5; 258/330] START learning_rate=0.09, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 258/330] END learning_rate=0.09, max_depth=11, num_leaves=58;, score=0.909 total time=  22.6s\n",
      "[CV 2/5; 260/330] START learning_rate=0.09, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 260/330] END learning_rate=0.09, max_depth=11, num_leaves=62;, score=0.909 total time=  22.1s\n",
      "[CV 4/5; 261/330] START learning_rate=0.09, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 261/330] END learning_rate=0.09, max_depth=11, num_leaves=64;, score=0.910 total time=  22.4s\n",
      "[CV 5/5; 262/330] START learning_rate=0.09, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 262/330] END learning_rate=0.09, max_depth=11, num_leaves=66;, score=0.909 total time=  33.7s\n",
      "[CV 4/5; 264/330] START learning_rate=0.09, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 264/330] END learning_rate=0.09, max_depth=11, num_leaves=70;, score=0.910 total time=  33.6s\n",
      "[CV 5/5; 266/330] START learning_rate=0.09, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 266/330] END learning_rate=0.09, max_depth=12, num_leaves=52;, score=0.909 total time=  30.3s\n",
      "[CV 4/5; 268/330] START learning_rate=0.09, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 268/330] END learning_rate=0.09, max_depth=12, num_leaves=56;, score=0.910 total time=  32.3s\n",
      "[CV 1/5; 271/330] START learning_rate=0.09, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 271/330] END learning_rate=0.09, max_depth=12, num_leaves=62;, score=0.909 total time=  21.8s\n",
      "[CV 1/5; 272/330] START learning_rate=0.09, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 272/330] END learning_rate=0.09, max_depth=12, num_leaves=64;, score=0.909 total time=  34.4s\n",
      "[CV 3/5; 274/330] START learning_rate=0.09, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 274/330] END learning_rate=0.09, max_depth=12, num_leaves=68;, score=0.910 total time=  22.6s\n",
      "[CV 3/5; 275/330] START learning_rate=0.09, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 275/330] END learning_rate=0.09, max_depth=12, num_leaves=70;, score=0.910 total time=  32.5s\n",
      "[CV 5/5; 277/330] START learning_rate=0.1, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 277/330] END learning_rate=0.1, max_depth=8, num_leaves=52;, score=0.909 total time=  19.7s\n",
      "[CV 1/5; 279/330] START learning_rate=0.1, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 279/330] END learning_rate=0.1, max_depth=8, num_leaves=56;, score=0.909 total time=  29.6s\n",
      "[CV 1/5; 281/330] START learning_rate=0.1, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 281/330] END learning_rate=0.1, max_depth=8, num_leaves=60;, score=0.909 total time=  20.3s\n",
      "[CV 1/5; 249/330] START learning_rate=0.09, max_depth=10, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.332490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 249/330] END learning_rate=0.09, max_depth=10, num_leaves=62;, score=0.909 total time=  33.2s\n",
      "[CV 1/5; 251/330] START learning_rate=0.09, max_depth=10, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 251/330] END learning_rate=0.09, max_depth=10, num_leaves=66;, score=0.909 total time=  23.1s\n",
      "[CV 3/5; 252/330] START learning_rate=0.09, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 252/330] END learning_rate=0.09, max_depth=10, num_leaves=68;, score=0.910 total time=  22.5s\n",
      "[CV 1/5; 254/330] START learning_rate=0.09, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 254/330] END learning_rate=0.09, max_depth=11, num_leaves=50;, score=0.909 total time=  21.3s\n",
      "[CV 1/5; 255/330] START learning_rate=0.09, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 255/330] END learning_rate=0.09, max_depth=11, num_leaves=52;, score=0.909 total time=  21.8s\n",
      "[CV 2/5; 256/330] START learning_rate=0.09, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.288281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 256/330] END learning_rate=0.09, max_depth=11, num_leaves=54;, score=0.909 total time=  23.0s\n",
      "[CV 5/5; 257/330] START learning_rate=0.09, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 257/330] END learning_rate=0.09, max_depth=11, num_leaves=56;, score=0.909 total time=  31.9s\n",
      "[CV 3/5; 259/330] START learning_rate=0.09, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 259/330] END learning_rate=0.09, max_depth=11, num_leaves=60;, score=0.910 total time=  22.0s\n",
      "[CV 1/5; 261/330] START learning_rate=0.09, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 261/330] END learning_rate=0.09, max_depth=11, num_leaves=64;, score=0.909 total time=  22.5s\n",
      "[CV 3/5; 262/330] START learning_rate=0.09, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.196320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 262/330] END learning_rate=0.09, max_depth=11, num_leaves=66;, score=0.910 total time=  32.8s\n",
      "[CV 1/5; 264/330] START learning_rate=0.09, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 264/330] END learning_rate=0.09, max_depth=11, num_leaves=70;, score=0.909 total time=  23.3s\n",
      "[CV 3/5; 265/330] START learning_rate=0.09, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 265/330] END learning_rate=0.09, max_depth=12, num_leaves=50;, score=0.910 total time=  20.9s\n",
      "[CV 1/5; 267/330] START learning_rate=0.09, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 267/330] END learning_rate=0.09, max_depth=12, num_leaves=54;, score=0.909 total time=  31.3s\n",
      "[CV 1/5; 269/330] START learning_rate=0.09, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 269/330] END learning_rate=0.09, max_depth=12, num_leaves=58;, score=0.909 total time=  21.6s\n",
      "[CV 1/5; 270/330] START learning_rate=0.09, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 270/330] END learning_rate=0.09, max_depth=12, num_leaves=60;, score=0.909 total time=  22.4s\n",
      "[CV 3/5; 271/330] START learning_rate=0.09, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 271/330] END learning_rate=0.09, max_depth=12, num_leaves=62;, score=0.910 total time=  33.4s\n",
      "[CV 3/5; 273/330] START learning_rate=0.09, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 273/330] END learning_rate=0.09, max_depth=12, num_leaves=66;, score=0.910 total time=  32.2s\n",
      "[CV 4/5; 275/330] START learning_rate=0.09, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 275/330] END learning_rate=0.09, max_depth=12, num_leaves=70;, score=0.910 total time=  22.8s\n",
      "[CV 5/5; 276/330] START learning_rate=0.1, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 276/330] END learning_rate=0.1, max_depth=8, num_leaves=50;, score=0.909 total time=  20.0s\n",
      "[CV 2/5; 278/330] START learning_rate=0.1, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 278/330] END learning_rate=0.1, max_depth=8, num_leaves=54;, score=0.909 total time=  19.7s\n",
      "[CV 5/5; 279/330] START learning_rate=0.1, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 279/330] END learning_rate=0.1, max_depth=8, num_leaves=56;, score=0.909 total time=  20.9s\n",
      "[CV 2/5; 281/330] START learning_rate=0.1, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 281/330] END learning_rate=0.1, max_depth=8, num_leaves=60;, score=0.909 total time=  20.5s\n",
      "[CV 3/5; 282/330] START learning_rate=0.1, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 253/330] END learning_rate=0.09, max_depth=10, num_leaves=70;, score=0.910 total time=  32.9s\n",
      "[CV 3/5; 255/330] START learning_rate=0.09, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 255/330] END learning_rate=0.09, max_depth=11, num_leaves=52;, score=0.910 total time=  32.4s\n",
      "[CV 2/5; 257/330] START learning_rate=0.09, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 257/330] END learning_rate=0.09, max_depth=11, num_leaves=56;, score=0.909 total time=  23.2s\n",
      "[CV 4/5; 258/330] START learning_rate=0.09, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 258/330] END learning_rate=0.09, max_depth=11, num_leaves=58;, score=0.910 total time=  33.3s\n",
      "[CV 4/5; 260/330] START learning_rate=0.09, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 260/330] END learning_rate=0.09, max_depth=11, num_leaves=62;, score=0.910 total time=  32.1s\n",
      "[CV 4/5; 262/330] START learning_rate=0.09, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 262/330] END learning_rate=0.09, max_depth=11, num_leaves=66;, score=0.910 total time=  32.7s\n",
      "[CV 3/5; 264/330] START learning_rate=0.09, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 264/330] END learning_rate=0.09, max_depth=11, num_leaves=70;, score=0.910 total time=  23.3s\n",
      "[CV 5/5; 265/330] START learning_rate=0.09, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 265/330] END learning_rate=0.09, max_depth=12, num_leaves=50;, score=0.909 total time=  21.1s\n",
      "[CV 2/5; 267/330] START learning_rate=0.09, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 267/330] END learning_rate=0.09, max_depth=12, num_leaves=54;, score=0.909 total time=  21.5s\n",
      "[CV 3/5; 268/330] START learning_rate=0.09, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 268/330] END learning_rate=0.09, max_depth=12, num_leaves=56;, score=0.910 total time=  31.2s\n",
      "[CV 3/5; 270/330] START learning_rate=0.09, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 270/330] END learning_rate=0.09, max_depth=12, num_leaves=60;, score=0.910 total time=  22.6s\n",
      "[CV 5/5; 271/330] START learning_rate=0.09, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 271/330] END learning_rate=0.09, max_depth=12, num_leaves=62;, score=0.909 total time=  23.1s\n",
      "[CV 1/5; 273/330] START learning_rate=0.09, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 273/330] END learning_rate=0.09, max_depth=12, num_leaves=66;, score=0.909 total time=  24.1s\n",
      "[CV 4/5; 274/330] START learning_rate=0.09, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 274/330] END learning_rate=0.09, max_depth=12, num_leaves=68;, score=0.910 total time=  22.7s\n",
      "[CV 2/5; 276/330] START learning_rate=0.1, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 276/330] END learning_rate=0.1, max_depth=8, num_leaves=50;, score=0.909 total time=  18.8s\n",
      "[CV 3/5; 277/330] START learning_rate=0.1, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 277/330] END learning_rate=0.1, max_depth=8, num_leaves=52;, score=0.910 total time=  19.7s\n",
      "[CV 5/5; 278/330] START learning_rate=0.1, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 278/330] END learning_rate=0.1, max_depth=8, num_leaves=54;, score=0.909 total time=  20.4s\n",
      "[CV 1/5; 280/330] START learning_rate=0.1, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 280/330] END learning_rate=0.1, max_depth=8, num_leaves=58;, score=0.909 total time=  21.1s\n",
      "[CV 3/5; 281/330] START learning_rate=0.1, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 281/330] END learning_rate=0.1, max_depth=8, num_leaves=60;, score=0.910 total time=  30.2s\n",
      "[CV 2/5; 283/330] START learning_rate=0.1, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 283/330] END learning_rate=0.1, max_depth=8, num_leaves=64;, score=0.909 total time=  21.3s\n",
      "[CV 5/5; 284/330] START learning_rate=0.1, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 284/330] END learning_rate=0.1, max_depth=8, num_leaves=66;, score=0.909 total time=  21.4s\n",
      "[CV 1/5; 286/330] START learning_rate=0.1, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 250/330] END learning_rate=0.09, max_depth=10, num_leaves=64;, score=0.910 total time=  33.3s\n",
      "[CV 5/5; 252/330] START learning_rate=0.09, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.318615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 252/330] END learning_rate=0.09, max_depth=10, num_leaves=68;, score=0.909 total time=  34.2s\n",
      "[CV 5/5; 254/330] START learning_rate=0.09, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 254/330] END learning_rate=0.09, max_depth=11, num_leaves=50;, score=0.909 total time=  32.7s\n",
      "[CV 5/5; 256/330] START learning_rate=0.09, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 256/330] END learning_rate=0.09, max_depth=11, num_leaves=54;, score=0.909 total time=  33.1s\n",
      "[CV 1/5; 259/330] START learning_rate=0.09, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 259/330] END learning_rate=0.09, max_depth=11, num_leaves=60;, score=0.909 total time=  33.6s\n",
      "[CV 5/5; 260/330] START learning_rate=0.09, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 260/330] END learning_rate=0.09, max_depth=11, num_leaves=62;, score=0.909 total time=  22.9s\n",
      "[CV 2/5; 262/330] START learning_rate=0.09, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 262/330] END learning_rate=0.09, max_depth=11, num_leaves=66;, score=0.909 total time=  23.6s\n",
      "[CV 5/5; 263/330] START learning_rate=0.09, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 263/330] END learning_rate=0.09, max_depth=11, num_leaves=68;, score=0.909 total time=  32.7s\n",
      "[CV 4/5; 265/330] START learning_rate=0.09, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 265/330] END learning_rate=0.09, max_depth=12, num_leaves=50;, score=0.910 total time=  30.0s\n",
      "[CV 3/5; 267/330] START learning_rate=0.09, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 267/330] END learning_rate=0.09, max_depth=12, num_leaves=54;, score=0.910 total time=  21.9s\n",
      "[CV 5/5; 268/330] START learning_rate=0.09, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 268/330] END learning_rate=0.09, max_depth=12, num_leaves=56;, score=0.909 total time=  20.9s\n",
      "[CV 5/5; 269/330] START learning_rate=0.09, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 269/330] END learning_rate=0.09, max_depth=12, num_leaves=58;, score=0.909 total time=  31.5s\n",
      "[CV 2/5; 272/330] START learning_rate=0.09, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 272/330] END learning_rate=0.09, max_depth=12, num_leaves=64;, score=0.910 total time=  24.3s\n",
      "[CV 2/5; 273/330] START learning_rate=0.09, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 273/330] END learning_rate=0.09, max_depth=12, num_leaves=66;, score=0.910 total time=  22.1s\n",
      "[CV 5/5; 274/330] START learning_rate=0.09, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 274/330] END learning_rate=0.09, max_depth=12, num_leaves=68;, score=0.909 total time=  21.7s\n",
      "[CV 3/5; 276/330] START learning_rate=0.1, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 276/330] END learning_rate=0.1, max_depth=8, num_leaves=50;, score=0.909 total time=  29.1s\n",
      "[CV 1/5; 278/330] START learning_rate=0.1, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 278/330] END learning_rate=0.1, max_depth=8, num_leaves=54;, score=0.909 total time=  20.6s\n",
      "[CV 4/5; 279/330] START learning_rate=0.1, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 279/330] END learning_rate=0.1, max_depth=8, num_leaves=56;, score=0.910 total time=  29.5s\n",
      "[CV 4/5; 281/330] START learning_rate=0.1, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 281/330] END learning_rate=0.1, max_depth=8, num_leaves=60;, score=0.910 total time=  31.0s\n",
      "[CV 4/5; 283/330] START learning_rate=0.1, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5; 283/330] END learning_rate=0.1, max_depth=8, num_leaves=64;, score=0.910 total time=  30.5s\n",
      "[CV 3/5; 285/330] START learning_rate=0.1, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 285/330] END learning_rate=0.1, max_depth=8, num_leaves=68;, score=0.910 total time=  21.1s\n",
      "[CV 1/5; 287/330] START learning_rate=0.1, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 287/330] END learning_rate=0.1, max_depth=9, num_leaves=50;, score=0.909 total time=  19.0s\n",
      "[CV 3/5; 288/330] START learning_rate=0.1, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.254625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 288/330] END learning_rate=0.1, max_depth=9, num_leaves=52;, score=0.910 total time=  20.7s\n",
      "[CV 4/5; 289/330] START learning_rate=0.1, max_depth=9, num_leaves=54...........\n",
      "\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 251/330] END learning_rate=0.09, max_depth=10, num_leaves=66;, score=0.910 total time=  22.9s\n",
      "[CV 4/5; 252/330] START learning_rate=0.09, max_depth=10, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 252/330] END learning_rate=0.09, max_depth=10, num_leaves=68;, score=0.910 total time=  22.4s\n",
      "[CV 2/5; 254/330] START learning_rate=0.09, max_depth=11, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 254/330] END learning_rate=0.09, max_depth=11, num_leaves=50;, score=0.909 total time=  21.2s\n",
      "[CV 2/5; 255/330] START learning_rate=0.09, max_depth=11, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 255/330] END learning_rate=0.09, max_depth=11, num_leaves=52;, score=0.909 total time=  22.0s\n",
      "[CV 4/5; 256/330] START learning_rate=0.09, max_depth=11, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 256/330] END learning_rate=0.09, max_depth=11, num_leaves=54;, score=0.910 total time=  33.3s\n",
      "[CV 3/5; 258/330] START learning_rate=0.09, max_depth=11, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 258/330] END learning_rate=0.09, max_depth=11, num_leaves=58;, score=0.910 total time=  22.8s\n",
      "[CV 1/5; 260/330] START learning_rate=0.09, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 260/330] END learning_rate=0.09, max_depth=11, num_leaves=62;, score=0.909 total time=  21.4s\n",
      "[CV 3/5; 261/330] START learning_rate=0.09, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 261/330] END learning_rate=0.09, max_depth=11, num_leaves=64;, score=0.910 total time=  32.9s\n",
      "[CV 4/5; 263/330] START learning_rate=0.09, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 263/330] END learning_rate=0.09, max_depth=11, num_leaves=68;, score=0.910 total time=  33.1s\n",
      "[CV 5/5; 264/330] START learning_rate=0.09, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 264/330] END learning_rate=0.09, max_depth=11, num_leaves=70;, score=0.909 total time=  23.4s\n",
      "[CV 4/5; 266/330] START learning_rate=0.09, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 266/330] END learning_rate=0.09, max_depth=12, num_leaves=52;, score=0.910 total time=  21.6s\n",
      "[CV 1/5; 268/330] START learning_rate=0.09, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 268/330] END learning_rate=0.09, max_depth=12, num_leaves=56;, score=0.909 total time=  21.3s\n",
      "[CV 3/5; 269/330] START learning_rate=0.09, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 269/330] END learning_rate=0.09, max_depth=12, num_leaves=58;, score=0.910 total time=  21.7s\n",
      "[CV 5/5; 270/330] START learning_rate=0.09, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.298483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 270/330] END learning_rate=0.09, max_depth=12, num_leaves=60;, score=0.909 total time=  32.1s\n",
      "[CV 5/5; 272/330] START learning_rate=0.09, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 272/330] END learning_rate=0.09, max_depth=12, num_leaves=64;, score=0.909 total time=  23.6s\n",
      "[CV 1/5; 274/330] START learning_rate=0.09, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 274/330] END learning_rate=0.09, max_depth=12, num_leaves=68;, score=0.909 total time=  22.2s\n",
      "[CV 2/5; 275/330] START learning_rate=0.09, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 275/330] END learning_rate=0.09, max_depth=12, num_leaves=70;, score=0.909 total time=  22.4s\n",
      "[CV 4/5; 276/330] START learning_rate=0.1, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 276/330] END learning_rate=0.1, max_depth=8, num_leaves=50;, score=0.910 total time=  28.9s\n",
      "[CV 4/5; 278/330] START learning_rate=0.1, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 278/330] END learning_rate=0.1, max_depth=8, num_leaves=54;, score=0.910 total time=  29.3s\n",
      "[CV 4/5; 280/330] START learning_rate=0.1, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 280/330] END learning_rate=0.1, max_depth=8, num_leaves=58;, score=0.910 total time=  30.2s\n",
      "[CV 5/5; 282/330] START learning_rate=0.1, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172683 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5; 282/330] END learning_rate=0.1, max_depth=8, num_leaves=62;, score=0.909 total time=  30.7s\n",
      "[CV 3/5; 284/330] START learning_rate=0.1, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.142838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 284/330] END learning_rate=0.1, max_depth=8, num_leaves=66;, score=0.910 total time=  31.1s\n",
      "[CV 3/5; 286/330] START learning_rate=0.1, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 286/330] END learning_rate=0.1, max_depth=8, num_leaves=70;, score=0.910 total time=  20.7s\n",
      "[CV 1/5; 288/330] START learning_rate=0.1, max_depth=9, num_leaves=52...........\n",
      "[CV 4/5; 257/330] START learning_rate=0.09, max_depth=11, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 257/330] END learning_rate=0.09, max_depth=11, num_leaves=56;, score=0.910 total time=  22.6s\n",
      "[CV 2/5; 259/330] START learning_rate=0.09, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 259/330] END learning_rate=0.09, max_depth=11, num_leaves=60;, score=0.909 total time=  22.7s\n",
      "[CV 3/5; 260/330] START learning_rate=0.09, max_depth=11, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 260/330] END learning_rate=0.09, max_depth=11, num_leaves=62;, score=0.910 total time=  22.6s\n",
      "[CV 1/5; 262/330] START learning_rate=0.09, max_depth=11, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 262/330] END learning_rate=0.09, max_depth=11, num_leaves=66;, score=0.909 total time=  23.3s\n",
      "[CV 2/5; 263/330] START learning_rate=0.09, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 263/330] END learning_rate=0.09, max_depth=11, num_leaves=68;, score=0.909 total time=  33.9s\n",
      "[CV 1/5; 265/330] START learning_rate=0.09, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 265/330] END learning_rate=0.09, max_depth=12, num_leaves=50;, score=0.909 total time=  21.2s\n",
      "[CV 2/5; 266/330] START learning_rate=0.09, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 266/330] END learning_rate=0.09, max_depth=12, num_leaves=52;, score=0.909 total time=  21.1s\n",
      "[CV 4/5; 267/330] START learning_rate=0.09, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 267/330] END learning_rate=0.09, max_depth=12, num_leaves=54;, score=0.910 total time=  31.0s\n",
      "[CV 4/5; 269/330] START learning_rate=0.09, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 269/330] END learning_rate=0.09, max_depth=12, num_leaves=58;, score=0.910 total time=  22.5s\n",
      "[CV 2/5; 271/330] START learning_rate=0.09, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.179679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 271/330] END learning_rate=0.09, max_depth=12, num_leaves=62;, score=0.909 total time=  22.4s\n",
      "[CV 3/5; 272/330] START learning_rate=0.09, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 272/330] END learning_rate=0.09, max_depth=12, num_leaves=64;, score=0.910 total time=  23.7s\n",
      "[CV 5/5; 273/330] START learning_rate=0.09, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 273/330] END learning_rate=0.09, max_depth=12, num_leaves=66;, score=0.909 total time=  33.4s\n",
      "[CV 5/5; 275/330] START learning_rate=0.09, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 275/330] END learning_rate=0.09, max_depth=12, num_leaves=70;, score=0.909 total time=  22.8s\n",
      "[CV 4/5; 277/330] START learning_rate=0.1, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 277/330] END learning_rate=0.1, max_depth=8, num_leaves=52;, score=0.910 total time=  29.2s\n",
      "[CV 3/5; 279/330] START learning_rate=0.1, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 279/330] END learning_rate=0.1, max_depth=8, num_leaves=56;, score=0.910 total time=  20.5s\n",
      "[CV 5/5; 280/330] START learning_rate=0.1, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 280/330] END learning_rate=0.1, max_depth=8, num_leaves=58;, score=0.909 total time=  20.1s\n",
      "[CV 1/5; 282/330] START learning_rate=0.1, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 282/330] END learning_rate=0.1, max_depth=8, num_leaves=62;, score=0.909 total time=  20.2s\n",
      "[CV 1/5; 283/330] START learning_rate=0.1, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 283/330] END learning_rate=0.1, max_depth=8, num_leaves=64;, score=0.909 total time=  21.8s\n",
      "[CV 4/5; 284/330] START learning_rate=0.1, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 284/330] END learning_rate=0.1, max_depth=8, num_leaves=66;, score=0.910 total time=  30.5s\n",
      "[CV 4/5; 286/330] START learning_rate=0.1, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5; 286/330] END learning_rate=0.1, max_depth=8, num_leaves=70;, score=0.910 total time=  20.4s\n",
      "[CV 2/5; 288/330] START learning_rate=0.1, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 288/330] END learning_rate=0.1, max_depth=9, num_leaves=52;, score=0.909 total time=  28.8s\n",
      "[CV 1/5; 290/330] START learning_rate=0.1, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 258/330] END learning_rate=0.09, max_depth=11, num_leaves=58;, score=0.909 total time=  22.7s\n",
      "[CV 5/5; 259/330] START learning_rate=0.09, max_depth=11, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 259/330] END learning_rate=0.09, max_depth=11, num_leaves=60;, score=0.909 total time=  22.2s\n",
      "[CV 2/5; 261/330] START learning_rate=0.09, max_depth=11, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 261/330] END learning_rate=0.09, max_depth=11, num_leaves=64;, score=0.909 total time=  34.1s\n",
      "[CV 3/5; 263/330] START learning_rate=0.09, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 263/330] END learning_rate=0.09, max_depth=11, num_leaves=68;, score=0.910 total time=  34.3s\n",
      "[CV 2/5; 265/330] START learning_rate=0.09, max_depth=12, num_leaves=50.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 265/330] END learning_rate=0.09, max_depth=12, num_leaves=50;, score=0.909 total time=  21.3s\n",
      "[CV 3/5; 266/330] START learning_rate=0.09, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 266/330] END learning_rate=0.09, max_depth=12, num_leaves=52;, score=0.910 total time=  21.8s\n",
      "[CV 5/5; 267/330] START learning_rate=0.09, max_depth=12, num_leaves=54.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 267/330] END learning_rate=0.09, max_depth=12, num_leaves=54;, score=0.909 total time=  21.0s\n",
      "[CV 2/5; 269/330] START learning_rate=0.09, max_depth=12, num_leaves=58.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 269/330] END learning_rate=0.09, max_depth=12, num_leaves=58;, score=0.909 total time=  21.8s\n",
      "[CV 4/5; 270/330] START learning_rate=0.09, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 270/330] END learning_rate=0.09, max_depth=12, num_leaves=60;, score=0.910 total time=  33.1s\n",
      "[CV 4/5; 272/330] START learning_rate=0.09, max_depth=12, num_leaves=64.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 272/330] END learning_rate=0.09, max_depth=12, num_leaves=64;, score=0.910 total time=  23.7s\n",
      "[CV 2/5; 274/330] START learning_rate=0.09, max_depth=12, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 274/330] END learning_rate=0.09, max_depth=12, num_leaves=68;, score=0.909 total time=  33.0s\n",
      "[CV 1/5; 276/330] START learning_rate=0.1, max_depth=8, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 276/330] END learning_rate=0.1, max_depth=8, num_leaves=50;, score=0.909 total time=  19.4s\n",
      "[CV 2/5; 277/330] START learning_rate=0.1, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 277/330] END learning_rate=0.1, max_depth=8, num_leaves=52;, score=0.909 total time=  20.0s\n",
      "[CV 3/5; 278/330] START learning_rate=0.1, max_depth=8, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 278/330] END learning_rate=0.1, max_depth=8, num_leaves=54;, score=0.910 total time=  28.8s\n",
      "[CV 3/5; 280/330] START learning_rate=0.1, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 280/330] END learning_rate=0.1, max_depth=8, num_leaves=58;, score=0.910 total time=  30.1s\n",
      "[CV 4/5; 282/330] START learning_rate=0.1, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 282/330] END learning_rate=0.1, max_depth=8, num_leaves=62;, score=0.910 total time=  21.4s\n",
      "[CV 1/5; 284/330] START learning_rate=0.1, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 284/330] END learning_rate=0.1, max_depth=8, num_leaves=66;, score=0.909 total time=  21.1s\n",
      "[CV 1/5; 285/330] START learning_rate=0.1, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 285/330] END learning_rate=0.1, max_depth=8, num_leaves=68;, score=0.909 total time=  21.0s\n",
      "[CV 2/5; 286/330] START learning_rate=0.1, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.285162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 286/330] END learning_rate=0.1, max_depth=8, num_leaves=70;, score=0.909 total time=  20.6s\n",
      "[CV 5/5; 287/330] START learning_rate=0.1, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 287/330] END learning_rate=0.1, max_depth=9, num_leaves=50;, score=0.909 total time=  20.2s\n",
      "[CV 2/5; 289/330] START learning_rate=0.1, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 289/330] END learning_rate=0.1, max_depth=9, num_leaves=54;, score=0.909 total time=  19.9s\n",
      "[CV 5/5; 290/330] START learning_rate=0.1, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 261/330] END learning_rate=0.09, max_depth=11, num_leaves=64;, score=0.909 total time=  21.9s\n",
      "[CV 1/5; 263/330] START learning_rate=0.09, max_depth=11, num_leaves=68.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 263/330] END learning_rate=0.09, max_depth=11, num_leaves=68;, score=0.909 total time=  23.9s\n",
      "[CV 2/5; 264/330] START learning_rate=0.09, max_depth=11, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 264/330] END learning_rate=0.09, max_depth=11, num_leaves=70;, score=0.909 total time=  34.2s\n",
      "[CV 1/5; 266/330] START learning_rate=0.09, max_depth=12, num_leaves=52.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.298216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 266/330] END learning_rate=0.09, max_depth=12, num_leaves=52;, score=0.909 total time=  31.1s\n",
      "[CV 2/5; 268/330] START learning_rate=0.09, max_depth=12, num_leaves=56.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164859 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 268/330] END learning_rate=0.09, max_depth=12, num_leaves=56;, score=0.909 total time=  31.8s\n",
      "[CV 2/5; 270/330] START learning_rate=0.09, max_depth=12, num_leaves=60.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 270/330] END learning_rate=0.09, max_depth=12, num_leaves=60;, score=0.910 total time=  21.9s\n",
      "[CV 4/5; 271/330] START learning_rate=0.09, max_depth=12, num_leaves=62.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 271/330] END learning_rate=0.09, max_depth=12, num_leaves=62;, score=0.910 total time=  33.0s\n",
      "[CV 4/5; 273/330] START learning_rate=0.09, max_depth=12, num_leaves=66.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 273/330] END learning_rate=0.09, max_depth=12, num_leaves=66;, score=0.910 total time=  22.3s\n",
      "[CV 1/5; 275/330] START learning_rate=0.09, max_depth=12, num_leaves=70.........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 275/330] END learning_rate=0.09, max_depth=12, num_leaves=70;, score=0.909 total time=  33.1s\n",
      "[CV 1/5; 277/330] START learning_rate=0.1, max_depth=8, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 277/330] END learning_rate=0.1, max_depth=8, num_leaves=52;, score=0.909 total time=  29.2s\n",
      "[CV 2/5; 279/330] START learning_rate=0.1, max_depth=8, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 279/330] END learning_rate=0.1, max_depth=8, num_leaves=56;, score=0.909 total time=  19.5s\n",
      "[CV 2/5; 280/330] START learning_rate=0.1, max_depth=8, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 280/330] END learning_rate=0.1, max_depth=8, num_leaves=58;, score=0.909 total time=  21.3s\n",
      "[CV 5/5; 281/330] START learning_rate=0.1, max_depth=8, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 281/330] END learning_rate=0.1, max_depth=8, num_leaves=60;, score=0.909 total time=  30.0s\n",
      "[CV 5/5; 283/330] START learning_rate=0.1, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 283/330] END learning_rate=0.1, max_depth=8, num_leaves=64;, score=0.909 total time=  29.9s\n",
      "[CV 4/5; 285/330] START learning_rate=0.1, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 285/330] END learning_rate=0.1, max_depth=8, num_leaves=68;, score=0.910 total time=  20.5s\n",
      "[CV 2/5; 287/330] START learning_rate=0.1, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 287/330] END learning_rate=0.1, max_depth=9, num_leaves=50;, score=0.909 total time=  20.3s\n",
      "[CV 5/5; 288/330] START learning_rate=0.1, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 288/330] END learning_rate=0.1, max_depth=9, num_leaves=52;, score=0.909 total time=  30.4s\n",
      "[CV 4/5; 290/330] START learning_rate=0.1, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 290/330] END learning_rate=0.1, max_depth=9, num_leaves=56;, score=0.910 total time=  20.3s\n",
      "[CV 1/5; 292/330] START learning_rate=0.1, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 292/330] END learning_rate=0.1, max_depth=9, num_leaves=60;, score=0.909 total time=  30.7s\n",
      "[CV 5/5; 293/330] START learning_rate=0.1, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 293/330] END learning_rate=0.1, max_depth=9, num_leaves=62;, score=0.909 total time=  20.6s\n",
      "[CV 2/5; 295/330] START learning_rate=0.1, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 295/330] END learning_rate=0.1, max_depth=9, num_leaves=66;, score=0.909 total time=  21.5s\n",
      "[CV 3/5; 296/330] START learning_rate=0.1, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.210832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 296/330] END learning_rate=0.1, max_depth=9, num_leaves=68;, score=0.910 total time=  31.9s\n",
      "[CV 2/5; 298/330] START learning_rate=0.1, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.197171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 298/330] END learning_rate=0.1, max_depth=10, num_leaves=50;, score=0.909 total time=  29.5s\n",
      "[CV 3/5; 300/330] START learning_rate=0.1, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Number of positive: 116035, number of negative: 1070417\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6189\n",
      "[LightGBM] [Info] Number of data points in the train set: 1186452, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "En iyi parametreler: {'learning_rate': 0.07, 'max_depth': 10, 'num_leaves': 68}\n",
      "[CV 2/5; 282/330] START learning_rate=0.1, max_depth=8, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 282/330] END learning_rate=0.1, max_depth=8, num_leaves=62;, score=0.909 total time=  20.7s\n",
      "[CV 3/5; 283/330] START learning_rate=0.1, max_depth=8, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 283/330] END learning_rate=0.1, max_depth=8, num_leaves=64;, score=0.910 total time=  30.7s\n",
      "[CV 2/5; 285/330] START learning_rate=0.1, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 285/330] END learning_rate=0.1, max_depth=8, num_leaves=68;, score=0.909 total time=  21.2s\n",
      "[CV 5/5; 286/330] START learning_rate=0.1, max_depth=8, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 286/330] END learning_rate=0.1, max_depth=8, num_leaves=70;, score=0.909 total time=  20.6s\n",
      "[CV 4/5; 288/330] START learning_rate=0.1, max_depth=9, num_leaves=52...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 288/330] END learning_rate=0.1, max_depth=9, num_leaves=52;, score=0.910 total time=  29.1s\n",
      "[CV 2/5; 290/330] START learning_rate=0.1, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 290/330] END learning_rate=0.1, max_depth=9, num_leaves=56;, score=0.909 total time=  19.7s\n",
      "[CV 2/5; 291/330] START learning_rate=0.1, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 291/330] END learning_rate=0.1, max_depth=9, num_leaves=58;, score=0.909 total time=  20.4s\n",
      "[CV 5/5; 292/330] START learning_rate=0.1, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 292/330] END learning_rate=0.1, max_depth=9, num_leaves=60;, score=0.909 total time=  21.3s\n",
      "[CV 2/5; 294/330] START learning_rate=0.1, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 294/330] END learning_rate=0.1, max_depth=9, num_leaves=64;, score=0.909 total time=  20.3s\n",
      "[CV 5/5; 295/330] START learning_rate=0.1, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 295/330] END learning_rate=0.1, max_depth=9, num_leaves=66;, score=0.909 total time=  21.6s\n",
      "[CV 1/5; 297/330] START learning_rate=0.1, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 297/330] END learning_rate=0.1, max_depth=9, num_leaves=70;, score=0.909 total time=  21.5s\n",
      "[CV 5/5; 297/330] START learning_rate=0.1, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 297/330] END learning_rate=0.1, max_depth=9, num_leaves=70;, score=0.909 total time=  22.0s\n",
      "[CV 3/5; 299/330] START learning_rate=0.1, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.132246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 299/330] END learning_rate=0.1, max_depth=10, num_leaves=52;, score=0.910 total time=  29.1s\n",
      "[CV 1/5; 301/330] START learning_rate=0.1, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 301/330] END learning_rate=0.1, max_depth=10, num_leaves=56;, score=0.909 total time=  21.4s\n",
      "[CV 3/5; 302/330] START learning_rate=0.1, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 302/330] END learning_rate=0.1, max_depth=10, num_leaves=58;, score=0.910 total time=  21.0s\n",
      "[CV 1/5; 304/330] START learning_rate=0.1, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 304/330] END learning_rate=0.1, max_depth=10, num_leaves=62;, score=0.909 total time=  21.7s\n",
      "[CV 4/5; 305/330] START learning_rate=0.1, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.395521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 305/330] END learning_rate=0.1, max_depth=10, num_leaves=64;, score=0.910 total time=  31.4s\n",
      "[CV 4/5; 307/330] START learning_rate=0.1, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 307/330] END learning_rate=0.1, max_depth=10, num_leaves=68;, score=0.910 total time=  32.6s\n",
      "[CV 4/5; 309/330] START learning_rate=0.1, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 309/330] END learning_rate=0.1, max_depth=11, num_leaves=50;, score=0.910 total time=  29.4s\n",
      "[CV 3/5; 311/330] START learning_rate=0.1, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 311/330] END learning_rate=0.1, max_depth=11, num_leaves=54;, score=0.910 total time=  21.3s\n",
      "[CV 5/5; 312/330] START learning_rate=0.1, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 312/330] END learning_rate=0.1, max_depth=11, num_leaves=56;, score=0.909 total time=  29.2s\n",
      "[CV 4/5; 314/330] START learning_rate=0.1, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 314/330] END learning_rate=0.1, max_depth=11, num_leaves=60;, score=0.910 total time=  21.2s\n",
      "[CV 1/5; 316/330] START learning_rate=0.1, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 316/330] END learning_rate=0.1, max_depth=11, num_leaves=64;, score=0.909 total time=  31.2s\n",
      "[CV 1/5; 318/330] START learning_rate=0.1, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 318/330] END learning_rate=0.1, max_depth=11, num_leaves=68;, score=0.909 total time=  32.5s\n",
      "[CV 5/5; 319/330] START learning_rate=0.1, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 319/330] END learning_rate=0.1, max_depth=11, num_leaves=70;, score=0.909 total time=  31.8s\n",
      "[CV 3/5; 322/330] START learning_rate=0.1, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 322/330] END learning_rate=0.1, max_depth=12, num_leaves=54;, score=0.910 total time=  30.0s\n",
      "[CV 1/5; 324/330] START learning_rate=0.1, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 324/330] END learning_rate=0.1, max_depth=12, num_leaves=58;, score=0.909 total time=  21.4s\n",
      "[CV 4/5; 325/330] START learning_rate=0.1, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 325/330] END learning_rate=0.1, max_depth=12, num_leaves=60;, score=0.910 total time=  30.9s\n",
      "[CV 4/5; 327/330] START learning_rate=0.1, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.346458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 327/330] END learning_rate=0.1, max_depth=12, num_leaves=64;, score=0.910 total time=  30.3s\n",
      "[CV 2/5; 329/330] START learning_rate=0.1, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 329/330] END learning_rate=0.1, max_depth=12, num_leaves=68;, score=0.909 total time=  32.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 300/330] END learning_rate=0.1, max_depth=10, num_leaves=54;, score=0.910 total time=  20.9s\n",
      "[CV 4/5; 301/330] START learning_rate=0.1, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 301/330] END learning_rate=0.1, max_depth=10, num_leaves=56;, score=0.910 total time=  30.1s\n",
      "[CV 4/5; 303/330] START learning_rate=0.1, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 303/330] END learning_rate=0.1, max_depth=10, num_leaves=60;, score=0.910 total time=  21.2s\n",
      "[CV 2/5; 305/330] START learning_rate=0.1, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 305/330] END learning_rate=0.1, max_depth=10, num_leaves=64;, score=0.909 total time=  20.8s\n",
      "[CV 3/5; 306/330] START learning_rate=0.1, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 306/330] END learning_rate=0.1, max_depth=10, num_leaves=66;, score=0.910 total time=  30.9s\n",
      "[CV 2/5; 308/330] START learning_rate=0.1, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.355105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 308/330] END learning_rate=0.1, max_depth=10, num_leaves=70;, score=0.909 total time=  32.6s\n",
      "[CV 1/5; 310/330] START learning_rate=0.1, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 310/330] END learning_rate=0.1, max_depth=11, num_leaves=52;, score=0.909 total time=  20.2s\n",
      "[CV 4/5; 311/330] START learning_rate=0.1, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 311/330] END learning_rate=0.1, max_depth=11, num_leaves=54;, score=0.910 total time=  21.0s\n",
      "[CV 1/5; 313/330] START learning_rate=0.1, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 313/330] END learning_rate=0.1, max_depth=11, num_leaves=58;, score=0.909 total time=  29.7s\n",
      "[CV 5/5; 314/330] START learning_rate=0.1, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 314/330] END learning_rate=0.1, max_depth=11, num_leaves=60;, score=0.909 total time=  30.9s\n",
      "[CV 4/5; 316/330] START learning_rate=0.1, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 316/330] END learning_rate=0.1, max_depth=11, num_leaves=64;, score=0.910 total time=  20.7s\n",
      "[CV 2/5; 318/330] START learning_rate=0.1, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 318/330] END learning_rate=0.1, max_depth=11, num_leaves=68;, score=0.909 total time=  32.2s\n",
      "[CV 1/5; 320/330] START learning_rate=0.1, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 320/330] END learning_rate=0.1, max_depth=12, num_leaves=50;, score=0.909 total time=  20.0s\n",
      "[CV 1/5; 321/330] START learning_rate=0.1, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 321/330] END learning_rate=0.1, max_depth=12, num_leaves=52;, score=0.909 total time=  19.4s\n",
      "[CV 4/5; 322/330] START learning_rate=0.1, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 322/330] END learning_rate=0.1, max_depth=12, num_leaves=54;, score=0.910 total time=  21.4s\n",
      "[CV 2/5; 324/330] START learning_rate=0.1, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 324/330] END learning_rate=0.1, max_depth=12, num_leaves=58;, score=0.909 total time=  30.4s\n",
      "[CV 2/5; 326/330] START learning_rate=0.1, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 326/330] END learning_rate=0.1, max_depth=12, num_leaves=62;, score=0.909 total time=  22.1s\n",
      "[CV 5/5; 327/330] START learning_rate=0.1, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 327/330] END learning_rate=0.1, max_depth=12, num_leaves=64;, score=0.909 total time=  31.7s\n",
      "[CV 3/5; 329/330] START learning_rate=0.1, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 329/330] END learning_rate=0.1, max_depth=12, num_leaves=68;, score=0.910 total time=  32.4s\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 286/330] END learning_rate=0.1, max_depth=8, num_leaves=70;, score=0.909 total time=  21.3s\n",
      "[CV 3/5; 287/330] START learning_rate=0.1, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 287/330] END learning_rate=0.1, max_depth=9, num_leaves=50;, score=0.910 total time=  20.3s\n",
      "[CV 1/5; 289/330] START learning_rate=0.1, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 289/330] END learning_rate=0.1, max_depth=9, num_leaves=54;, score=0.909 total time=  20.3s\n",
      "[CV 3/5; 290/330] START learning_rate=0.1, max_depth=9, num_leaves=56...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 290/330] END learning_rate=0.1, max_depth=9, num_leaves=56;, score=0.910 total time=  20.2s\n",
      "[CV 4/5; 291/330] START learning_rate=0.1, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 291/330] END learning_rate=0.1, max_depth=9, num_leaves=58;, score=0.910 total time=  30.3s\n",
      "[CV 3/5; 293/330] START learning_rate=0.1, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 293/330] END learning_rate=0.1, max_depth=9, num_leaves=62;, score=0.910 total time=  21.4s\n",
      "[CV 1/5; 295/330] START learning_rate=0.1, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 295/330] END learning_rate=0.1, max_depth=9, num_leaves=66;, score=0.909 total time=  21.2s\n",
      "[CV 2/5; 296/330] START learning_rate=0.1, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.339340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 296/330] END learning_rate=0.1, max_depth=9, num_leaves=68;, score=0.909 total time=  32.1s\n",
      "[CV 1/5; 298/330] START learning_rate=0.1, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 298/330] END learning_rate=0.1, max_depth=10, num_leaves=50;, score=0.909 total time=  19.9s\n",
      "[CV 4/5; 299/330] START learning_rate=0.1, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 299/330] END learning_rate=0.1, max_depth=10, num_leaves=52;, score=0.910 total time=  30.0s\n",
      "[CV 2/5; 301/330] START learning_rate=0.1, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 301/330] END learning_rate=0.1, max_depth=10, num_leaves=56;, score=0.909 total time=  20.8s\n",
      "[CV 4/5; 302/330] START learning_rate=0.1, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 302/330] END learning_rate=0.1, max_depth=10, num_leaves=58;, score=0.910 total time=  20.4s\n",
      "[CV 2/5; 304/330] START learning_rate=0.1, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 304/330] END learning_rate=0.1, max_depth=10, num_leaves=62;, score=0.909 total time=  20.5s\n",
      "[CV 3/5; 305/330] START learning_rate=0.1, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 305/330] END learning_rate=0.1, max_depth=10, num_leaves=64;, score=0.910 total time=  31.2s\n",
      "[CV 2/5; 307/330] START learning_rate=0.1, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 307/330] END learning_rate=0.1, max_depth=10, num_leaves=68;, score=0.909 total time=  22.0s\n",
      "[CV 5/5; 308/330] START learning_rate=0.1, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 308/330] END learning_rate=0.1, max_depth=10, num_leaves=70;, score=0.909 total time=  21.8s\n",
      "[CV 2/5; 310/330] START learning_rate=0.1, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.210563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 310/330] END learning_rate=0.1, max_depth=11, num_leaves=52;, score=0.909 total time=  30.1s\n",
      "[CV 4/5; 312/330] START learning_rate=0.1, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 312/330] END learning_rate=0.1, max_depth=11, num_leaves=56;, score=0.910 total time=  21.2s\n",
      "[CV 5/5; 313/330] START learning_rate=0.1, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 313/330] END learning_rate=0.1, max_depth=11, num_leaves=58;, score=0.909 total time=  19.7s\n",
      "[CV 1/5; 315/330] START learning_rate=0.1, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 315/330] END learning_rate=0.1, max_depth=11, num_leaves=62;, score=0.909 total time=  31.7s\n",
      "[CV 5/5; 316/330] START learning_rate=0.1, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 316/330] END learning_rate=0.1, max_depth=11, num_leaves=64;, score=0.909 total time=  20.9s\n",
      "[CV 3/5; 318/330] START learning_rate=0.1, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 318/330] END learning_rate=0.1, max_depth=11, num_leaves=68;, score=0.910 total time=  21.8s\n",
      "[CV 4/5; 319/330] START learning_rate=0.1, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 319/330] END learning_rate=0.1, max_depth=11, num_leaves=70;, score=0.910 total time=  31.1s\n",
      "[CV 3/5; 321/330] START learning_rate=0.1, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 321/330] END learning_rate=0.1, max_depth=12, num_leaves=52;, score=0.910 total time=  29.8s\n",
      "[CV 4/5; 323/330] START learning_rate=0.1, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 323/330] END learning_rate=0.1, max_depth=12, num_leaves=56;, score=0.910 total time=  30.3s\n",
      "[CV 3/5; 325/330] START learning_rate=0.1, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 325/330] END learning_rate=0.1, max_depth=12, num_leaves=60;, score=0.910 total time=  20.1s\n",
      "[CV 1/5; 327/330] START learning_rate=0.1, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 327/330] END learning_rate=0.1, max_depth=12, num_leaves=64;, score=0.909 total time=  21.8s\n",
      "[CV 1/5; 328/330] START learning_rate=0.1, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 328/330] END learning_rate=0.1, max_depth=12, num_leaves=66;, score=0.909 total time=  22.1s\n",
      "[CV 4/5; 329/330] START learning_rate=0.1, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 329/330] END learning_rate=0.1, max_depth=12, num_leaves=68;, score=0.910 total time=  32.0s\n",
      "[CV 5/5; 290/330] END learning_rate=0.1, max_depth=9, num_leaves=56;, score=0.909 total time=  20.0s\n",
      "[CV 2/5; 292/330] START learning_rate=0.1, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 292/330] END learning_rate=0.1, max_depth=9, num_leaves=60;, score=0.909 total time=  30.3s\n",
      "[CV 1/5; 294/330] START learning_rate=0.1, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 294/330] END learning_rate=0.1, max_depth=9, num_leaves=64;, score=0.909 total time=  21.1s\n",
      "[CV 3/5; 295/330] START learning_rate=0.1, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 295/330] END learning_rate=0.1, max_depth=9, num_leaves=66;, score=0.910 total time=  20.6s\n",
      "[CV 4/5; 296/330] START learning_rate=0.1, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 296/330] END learning_rate=0.1, max_depth=9, num_leaves=68;, score=0.909 total time=  32.4s\n",
      "[CV 3/5; 298/330] START learning_rate=0.1, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 298/330] END learning_rate=0.1, max_depth=10, num_leaves=50;, score=0.910 total time=  29.0s\n",
      "[CV 4/5; 300/330] START learning_rate=0.1, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 300/330] END learning_rate=0.1, max_depth=10, num_leaves=54;, score=0.910 total time=  28.7s\n",
      "[CV 2/5; 302/330] START learning_rate=0.1, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 302/330] END learning_rate=0.1, max_depth=10, num_leaves=58;, score=0.909 total time=  20.8s\n",
      "[CV 5/5; 303/330] START learning_rate=0.1, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 303/330] END learning_rate=0.1, max_depth=10, num_leaves=60;, score=0.909 total time=  30.4s\n",
      "[CV 5/5; 305/330] START learning_rate=0.1, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 305/330] END learning_rate=0.1, max_depth=10, num_leaves=64;, score=0.909 total time=  31.1s\n",
      "[CV 5/5; 307/330] START learning_rate=0.1, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 307/330] END learning_rate=0.1, max_depth=10, num_leaves=68;, score=0.909 total time=  32.3s\n",
      "[CV 3/5; 309/330] START learning_rate=0.1, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 309/330] END learning_rate=0.1, max_depth=11, num_leaves=50;, score=0.910 total time=  29.5s\n",
      "[CV 2/5; 311/330] START learning_rate=0.1, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 311/330] END learning_rate=0.1, max_depth=11, num_leaves=54;, score=0.909 total time=  30.2s\n",
      "[CV 3/5; 313/330] START learning_rate=0.1, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 313/330] END learning_rate=0.1, max_depth=11, num_leaves=58;, score=0.910 total time=  30.7s\n",
      "[CV 3/5; 315/330] START learning_rate=0.1, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 315/330] END learning_rate=0.1, max_depth=11, num_leaves=62;, score=0.910 total time=  31.3s\n",
      "[CV 2/5; 317/330] START learning_rate=0.1, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 317/330] END learning_rate=0.1, max_depth=11, num_leaves=66;, score=0.909 total time=  21.4s\n",
      "[CV 5/5; 318/330] START learning_rate=0.1, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 318/330] END learning_rate=0.1, max_depth=11, num_leaves=68;, score=0.909 total time=  31.7s\n",
      "[CV 4/5; 320/330] START learning_rate=0.1, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 320/330] END learning_rate=0.1, max_depth=12, num_leaves=50;, score=0.910 total time=  19.5s\n",
      "[CV 1/5; 322/330] START learning_rate=0.1, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 322/330] END learning_rate=0.1, max_depth=12, num_leaves=54;, score=0.909 total time=  20.9s\n",
      "[CV 3/5; 323/330] START learning_rate=0.1, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 323/330] END learning_rate=0.1, max_depth=12, num_leaves=56;, score=0.910 total time=  29.6s\n",
      "[CV 1/5; 325/330] START learning_rate=0.1, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 325/330] END learning_rate=0.1, max_depth=12, num_leaves=60;, score=0.909 total time=  20.4s\n",
      "[CV 3/5; 326/330] START learning_rate=0.1, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.379171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 326/330] END learning_rate=0.1, max_depth=12, num_leaves=62;, score=0.910 total time=  31.3s\n",
      "[CV 2/5; 328/330] START learning_rate=0.1, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 328/330] END learning_rate=0.1, max_depth=12, num_leaves=66;, score=0.909 total time=  22.2s\n",
      "[CV 5/5; 329/330] START learning_rate=0.1, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 329/330] END learning_rate=0.1, max_depth=12, num_leaves=68;, score=0.909 total time=  28.8s\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5; 282/330] END learning_rate=0.1, max_depth=8, num_leaves=62;, score=0.910 total time=  30.0s\n",
      "[CV 2/5; 284/330] START learning_rate=0.1, max_depth=8, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 284/330] END learning_rate=0.1, max_depth=8, num_leaves=66;, score=0.909 total time=  21.3s\n",
      "[CV 5/5; 285/330] START learning_rate=0.1, max_depth=8, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 285/330] END learning_rate=0.1, max_depth=8, num_leaves=68;, score=0.909 total time=  30.2s\n",
      "[CV 4/5; 287/330] START learning_rate=0.1, max_depth=9, num_leaves=50...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 287/330] END learning_rate=0.1, max_depth=9, num_leaves=50;, score=0.910 total time=  30.3s\n",
      "[CV 5/5; 289/330] START learning_rate=0.1, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.197004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 289/330] END learning_rate=0.1, max_depth=9, num_leaves=54;, score=0.909 total time=  29.2s\n",
      "[CV 5/5; 291/330] START learning_rate=0.1, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 291/330] END learning_rate=0.1, max_depth=9, num_leaves=58;, score=0.909 total time=  30.1s\n",
      "[CV 4/5; 293/330] START learning_rate=0.1, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 293/330] END learning_rate=0.1, max_depth=9, num_leaves=62;, score=0.910 total time=  21.1s\n",
      "[CV 5/5; 294/330] START learning_rate=0.1, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 294/330] END learning_rate=0.1, max_depth=9, num_leaves=64;, score=0.909 total time=  30.7s\n",
      "[CV 2/5; 297/330] START learning_rate=0.1, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 297/330] END learning_rate=0.1, max_depth=9, num_leaves=70;, score=0.909 total time=  32.8s\n",
      "[CV 1/5; 299/330] START learning_rate=0.1, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 299/330] END learning_rate=0.1, max_depth=10, num_leaves=52;, score=0.909 total time=  20.0s\n",
      "[CV 2/5; 300/330] START learning_rate=0.1, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 300/330] END learning_rate=0.1, max_depth=10, num_leaves=54;, score=0.909 total time=  21.2s\n",
      "[CV 3/5; 301/330] START learning_rate=0.1, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 301/330] END learning_rate=0.1, max_depth=10, num_leaves=56;, score=0.910 total time=  20.9s\n",
      "[CV 5/5; 302/330] START learning_rate=0.1, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.174046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 302/330] END learning_rate=0.1, max_depth=10, num_leaves=58;, score=0.909 total time=  30.1s\n",
      "[CV 1/5; 305/330] START learning_rate=0.1, max_depth=10, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 305/330] END learning_rate=0.1, max_depth=10, num_leaves=64;, score=0.909 total time=  31.6s\n",
      "[CV 5/5; 306/330] START learning_rate=0.1, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 306/330] END learning_rate=0.1, max_depth=10, num_leaves=66;, score=0.909 total time=  21.7s\n",
      "[CV 3/5; 308/330] START learning_rate=0.1, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 308/330] END learning_rate=0.1, max_depth=10, num_leaves=70;, score=0.910 total time=  21.7s\n",
      "[CV 5/5; 309/330] START learning_rate=0.1, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 309/330] END learning_rate=0.1, max_depth=11, num_leaves=50;, score=0.909 total time=  19.4s\n",
      "[CV 1/5; 311/330] START learning_rate=0.1, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 311/330] END learning_rate=0.1, max_depth=11, num_leaves=54;, score=0.909 total time=  20.4s\n",
      "[CV 3/5; 312/330] START learning_rate=0.1, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.278468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 312/330] END learning_rate=0.1, max_depth=11, num_leaves=56;, score=0.910 total time=  30.1s\n",
      "[CV 2/5; 314/330] START learning_rate=0.1, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 314/330] END learning_rate=0.1, max_depth=11, num_leaves=60;, score=0.909 total time=  31.1s\n",
      "[CV 2/5; 316/330] START learning_rate=0.1, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 316/330] END learning_rate=0.1, max_depth=11, num_leaves=64;, score=0.909 total time=  21.6s\n",
      "[CV 3/5; 317/330] START learning_rate=0.1, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.406233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 317/330] END learning_rate=0.1, max_depth=11, num_leaves=66;, score=0.910 total time=  31.7s\n",
      "[CV 3/5; 319/330] START learning_rate=0.1, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 319/330] END learning_rate=0.1, max_depth=11, num_leaves=70;, score=0.910 total time=  31.2s\n",
      "[CV 2/5; 321/330] START learning_rate=0.1, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 321/330] END learning_rate=0.1, max_depth=12, num_leaves=52;, score=0.909 total time=  20.5s\n",
      "[CV 5/5; 322/330] START learning_rate=0.1, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 322/330] END learning_rate=0.1, max_depth=12, num_leaves=54;, score=0.909 total time=  29.8s\n",
      "[CV 5/5; 324/330] START learning_rate=0.1, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 324/330] END learning_rate=0.1, max_depth=12, num_leaves=58;, score=0.909 total time=  29.8s\n",
      "[CV 5/5; 326/330] START learning_rate=0.1, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 326/330] END learning_rate=0.1, max_depth=12, num_leaves=62;, score=0.909 total time=  31.0s\n",
      "[CV 5/5; 328/330] START learning_rate=0.1, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 328/330] END learning_rate=0.1, max_depth=12, num_leaves=66;, score=0.909 total time=  21.3s\n",
      "[CV 2/5; 330/330] START learning_rate=0.1, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 330/330] END learning_rate=0.1, max_depth=12, num_leaves=70;, score=0.909 total time=  29.6s\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 288/330] END learning_rate=0.1, max_depth=9, num_leaves=52;, score=0.909 total time=  20.6s\n",
      "[CV 3/5; 289/330] START learning_rate=0.1, max_depth=9, num_leaves=54...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 289/330] END learning_rate=0.1, max_depth=9, num_leaves=54;, score=0.910 total time=  19.9s\n",
      "[CV 1/5; 291/330] START learning_rate=0.1, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5; 291/330] END learning_rate=0.1, max_depth=9, num_leaves=58;, score=0.909 total time=  30.1s\n",
      "[CV 4/5; 292/330] START learning_rate=0.1, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 292/330] END learning_rate=0.1, max_depth=9, num_leaves=60;, score=0.910 total time=  30.6s\n",
      "[CV 4/5; 294/330] START learning_rate=0.1, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 294/330] END learning_rate=0.1, max_depth=9, num_leaves=64;, score=0.910 total time=  31.5s\n",
      "[CV 5/5; 296/330] START learning_rate=0.1, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 296/330] END learning_rate=0.1, max_depth=9, num_leaves=68;, score=0.909 total time=  32.1s\n",
      "[CV 4/5; 298/330] START learning_rate=0.1, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 298/330] END learning_rate=0.1, max_depth=10, num_leaves=50;, score=0.910 total time=  20.1s\n",
      "[CV 5/5; 299/330] START learning_rate=0.1, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.197566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 299/330] END learning_rate=0.1, max_depth=10, num_leaves=52;, score=0.909 total time=  30.0s\n",
      "[CV 5/5; 301/330] START learning_rate=0.1, max_depth=10, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 301/330] END learning_rate=0.1, max_depth=10, num_leaves=56;, score=0.909 total time=  21.0s\n",
      "[CV 2/5; 303/330] START learning_rate=0.1, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 303/330] END learning_rate=0.1, max_depth=10, num_leaves=60;, score=0.909 total time=  20.9s\n",
      "[CV 4/5; 304/330] START learning_rate=0.1, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 304/330] END learning_rate=0.1, max_depth=10, num_leaves=62;, score=0.910 total time=  30.7s\n",
      "[CV 4/5; 306/330] START learning_rate=0.1, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 306/330] END learning_rate=0.1, max_depth=10, num_leaves=66;, score=0.910 total time=  21.7s\n",
      "[CV 1/5; 308/330] START learning_rate=0.1, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 308/330] END learning_rate=0.1, max_depth=10, num_leaves=70;, score=0.909 total time=  22.2s\n",
      "[CV 2/5; 309/330] START learning_rate=0.1, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 309/330] END learning_rate=0.1, max_depth=11, num_leaves=50;, score=0.909 total time=  20.4s\n",
      "[CV 4/5; 310/330] START learning_rate=0.1, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 310/330] END learning_rate=0.1, max_depth=11, num_leaves=52;, score=0.910 total time=  20.4s\n",
      "[CV 1/5; 312/330] START learning_rate=0.1, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 312/330] END learning_rate=0.1, max_depth=11, num_leaves=56;, score=0.909 total time=  20.3s\n",
      "[CV 2/5; 313/330] START learning_rate=0.1, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.258734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 313/330] END learning_rate=0.1, max_depth=11, num_leaves=58;, score=0.909 total time=  20.7s\n",
      "[CV 3/5; 314/330] START learning_rate=0.1, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 314/330] END learning_rate=0.1, max_depth=11, num_leaves=60;, score=0.910 total time=  21.4s\n",
      "[CV 5/5; 315/330] START learning_rate=0.1, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 315/330] END learning_rate=0.1, max_depth=11, num_leaves=62;, score=0.909 total time=  30.9s\n",
      "[CV 5/5; 317/330] START learning_rate=0.1, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 317/330] END learning_rate=0.1, max_depth=11, num_leaves=66;, score=0.909 total time=  21.4s\n",
      "[CV 2/5; 319/330] START learning_rate=0.1, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 319/330] END learning_rate=0.1, max_depth=11, num_leaves=70;, score=0.909 total time=  23.0s\n",
      "[CV 3/5; 320/330] START learning_rate=0.1, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 320/330] END learning_rate=0.1, max_depth=12, num_leaves=50;, score=0.910 total time=  19.3s\n",
      "[CV 5/5; 321/330] START learning_rate=0.1, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 321/330] END learning_rate=0.1, max_depth=12, num_leaves=52;, score=0.909 total time=  20.9s\n",
      "[CV 2/5; 323/330] START learning_rate=0.1, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 323/330] END learning_rate=0.1, max_depth=12, num_leaves=56;, score=0.909 total time=  21.3s\n",
      "[CV 4/5; 324/330] START learning_rate=0.1, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 324/330] END learning_rate=0.1, max_depth=12, num_leaves=58;, score=0.910 total time=  21.7s\n",
      "[CV 1/5; 326/330] START learning_rate=0.1, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 326/330] END learning_rate=0.1, max_depth=12, num_leaves=62;, score=0.909 total time=  20.2s\n",
      "[CV 3/5; 327/330] START learning_rate=0.1, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 327/330] END learning_rate=0.1, max_depth=12, num_leaves=64;, score=0.910 total time=  31.6s\n",
      "[CV 1/5; 329/330] START learning_rate=0.1, max_depth=12, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 329/330] END learning_rate=0.1, max_depth=12, num_leaves=68;, score=0.909 total time=  22.0s\n",
      "[CV 4/5; 330/330] START learning_rate=0.1, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 330/330] END learning_rate=0.1, max_depth=12, num_leaves=70;, score=0.910 total time=  18.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 290/330] END learning_rate=0.1, max_depth=9, num_leaves=56;, score=0.909 total time=  30.1s\n",
      "[CV 3/5; 292/330] START learning_rate=0.1, max_depth=9, num_leaves=60...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 292/330] END learning_rate=0.1, max_depth=9, num_leaves=60;, score=0.910 total time=  20.2s\n",
      "[CV 2/5; 293/330] START learning_rate=0.1, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 293/330] END learning_rate=0.1, max_depth=9, num_leaves=62;, score=0.909 total time=  31.0s\n",
      "[CV 4/5; 295/330] START learning_rate=0.1, max_depth=9, num_leaves=66...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.160638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 295/330] END learning_rate=0.1, max_depth=9, num_leaves=66;, score=0.910 total time=  30.5s\n",
      "[CV 3/5; 297/330] START learning_rate=0.1, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 297/330] END learning_rate=0.1, max_depth=9, num_leaves=70;, score=0.910 total time=  31.4s\n",
      "[CV 2/5; 299/330] START learning_rate=0.1, max_depth=10, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.205855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 299/330] END learning_rate=0.1, max_depth=10, num_leaves=52;, score=0.909 total time=  30.4s\n",
      "[CV 5/5; 300/330] START learning_rate=0.1, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 300/330] END learning_rate=0.1, max_depth=10, num_leaves=54;, score=0.909 total time=  29.9s\n",
      "[CV 1/5; 303/330] START learning_rate=0.1, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 303/330] END learning_rate=0.1, max_depth=10, num_leaves=60;, score=0.909 total time=  20.4s\n",
      "[CV 3/5; 304/330] START learning_rate=0.1, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 304/330] END learning_rate=0.1, max_depth=10, num_leaves=62;, score=0.909 total time=  21.2s\n",
      "[CV 1/5; 306/330] START learning_rate=0.1, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 306/330] END learning_rate=0.1, max_depth=10, num_leaves=66;, score=0.909 total time=  21.8s\n",
      "[CV 1/5; 307/330] START learning_rate=0.1, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 307/330] END learning_rate=0.1, max_depth=10, num_leaves=68;, score=0.909 total time=  21.5s\n",
      "[CV 4/5; 308/330] START learning_rate=0.1, max_depth=10, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 308/330] END learning_rate=0.1, max_depth=10, num_leaves=70;, score=0.910 total time=  31.7s\n",
      "[CV 3/5; 310/330] START learning_rate=0.1, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 310/330] END learning_rate=0.1, max_depth=11, num_leaves=52;, score=0.910 total time=  19.9s\n",
      "[CV 5/5; 311/330] START learning_rate=0.1, max_depth=11, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 311/330] END learning_rate=0.1, max_depth=11, num_leaves=54;, score=0.909 total time=  29.4s\n",
      "[CV 1/5; 314/330] START learning_rate=0.1, max_depth=11, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 314/330] END learning_rate=0.1, max_depth=11, num_leaves=60;, score=0.909 total time=  20.3s\n",
      "[CV 2/5; 315/330] START learning_rate=0.1, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 315/330] END learning_rate=0.1, max_depth=11, num_leaves=62;, score=0.909 total time=  21.7s\n",
      "[CV 3/5; 316/330] START learning_rate=0.1, max_depth=11, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 316/330] END learning_rate=0.1, max_depth=11, num_leaves=64;, score=0.910 total time=  21.1s\n",
      "[CV 4/5; 317/330] START learning_rate=0.1, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 317/330] END learning_rate=0.1, max_depth=11, num_leaves=66;, score=0.910 total time=  21.1s\n",
      "[CV 1/5; 319/330] START learning_rate=0.1, max_depth=11, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 319/330] END learning_rate=0.1, max_depth=11, num_leaves=70;, score=0.909 total time=  21.8s\n",
      "[CV 2/5; 320/330] START learning_rate=0.1, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.315535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 320/330] END learning_rate=0.1, max_depth=12, num_leaves=50;, score=0.909 total time=  20.2s\n",
      "[CV 4/5; 321/330] START learning_rate=0.1, max_depth=12, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 321/330] END learning_rate=0.1, max_depth=12, num_leaves=52;, score=0.910 total time=  20.6s\n",
      "[CV 1/5; 323/330] START learning_rate=0.1, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 323/330] END learning_rate=0.1, max_depth=12, num_leaves=56;, score=0.909 total time=  20.9s\n",
      "[CV 3/5; 324/330] START learning_rate=0.1, max_depth=12, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 324/330] END learning_rate=0.1, max_depth=12, num_leaves=58;, score=0.910 total time=  21.9s\n",
      "[CV 5/5; 325/330] START learning_rate=0.1, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 325/330] END learning_rate=0.1, max_depth=12, num_leaves=60;, score=0.909 total time=  21.1s\n",
      "[CV 2/5; 327/330] START learning_rate=0.1, max_depth=12, num_leaves=64..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.268731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 327/330] END learning_rate=0.1, max_depth=12, num_leaves=64;, score=0.909 total time=  21.4s\n",
      "[CV 3/5; 328/330] START learning_rate=0.1, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 328/330] END learning_rate=0.1, max_depth=12, num_leaves=66;, score=0.910 total time=  31.6s\n",
      "[CV 3/5; 330/330] START learning_rate=0.1, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 330/330] END learning_rate=0.1, max_depth=12, num_leaves=70;, score=0.910 total time=  22.6s\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 289/330] END learning_rate=0.1, max_depth=9, num_leaves=54;, score=0.910 total time=  29.4s\n",
      "[CV 3/5; 291/330] START learning_rate=0.1, max_depth=9, num_leaves=58...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 291/330] END learning_rate=0.1, max_depth=9, num_leaves=58;, score=0.910 total time=  21.2s\n",
      "[CV 1/5; 293/330] START learning_rate=0.1, max_depth=9, num_leaves=62...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 293/330] END learning_rate=0.1, max_depth=9, num_leaves=62;, score=0.909 total time=  20.4s\n",
      "[CV 3/5; 294/330] START learning_rate=0.1, max_depth=9, num_leaves=64...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 294/330] END learning_rate=0.1, max_depth=9, num_leaves=64;, score=0.910 total time=  30.3s\n",
      "[CV 1/5; 296/330] START learning_rate=0.1, max_depth=9, num_leaves=68...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 296/330] END learning_rate=0.1, max_depth=9, num_leaves=68;, score=0.909 total time=  22.2s\n",
      "[CV 4/5; 297/330] START learning_rate=0.1, max_depth=9, num_leaves=70...........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 297/330] END learning_rate=0.1, max_depth=9, num_leaves=70;, score=0.910 total time=  22.0s\n",
      "[CV 5/5; 298/330] START learning_rate=0.1, max_depth=10, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 298/330] END learning_rate=0.1, max_depth=10, num_leaves=50;, score=0.909 total time=  19.7s\n",
      "[CV 1/5; 300/330] START learning_rate=0.1, max_depth=10, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 300/330] END learning_rate=0.1, max_depth=10, num_leaves=54;, score=0.909 total time=  29.7s\n",
      "[CV 1/5; 302/330] START learning_rate=0.1, max_depth=10, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 302/330] END learning_rate=0.1, max_depth=10, num_leaves=58;, score=0.909 total time=  20.8s\n",
      "[CV 3/5; 303/330] START learning_rate=0.1, max_depth=10, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 303/330] END learning_rate=0.1, max_depth=10, num_leaves=60;, score=0.909 total time=  21.6s\n",
      "[CV 5/5; 304/330] START learning_rate=0.1, max_depth=10, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 304/330] END learning_rate=0.1, max_depth=10, num_leaves=62;, score=0.909 total time=  21.7s\n",
      "[CV 2/5; 306/330] START learning_rate=0.1, max_depth=10, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 306/330] END learning_rate=0.1, max_depth=10, num_leaves=66;, score=0.909 total time=  21.3s\n",
      "[CV 3/5; 307/330] START learning_rate=0.1, max_depth=10, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6184\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 3/5; 307/330] END learning_rate=0.1, max_depth=10, num_leaves=68;, score=0.910 total time=  22.0s\n",
      "[CV 1/5; 309/330] START learning_rate=0.1, max_depth=11, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 309/330] END learning_rate=0.1, max_depth=11, num_leaves=50;, score=0.909 total time=  29.2s\n",
      "[CV 5/5; 310/330] START learning_rate=0.1, max_depth=11, num_leaves=52..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 310/330] END learning_rate=0.1, max_depth=11, num_leaves=52;, score=0.909 total time=  20.2s\n",
      "[CV 2/5; 312/330] START learning_rate=0.1, max_depth=11, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 312/330] END learning_rate=0.1, max_depth=11, num_leaves=56;, score=0.909 total time=  20.7s\n",
      "[CV 4/5; 313/330] START learning_rate=0.1, max_depth=11, num_leaves=58..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 313/330] END learning_rate=0.1, max_depth=11, num_leaves=58;, score=0.909 total time=  30.8s\n",
      "[CV 4/5; 315/330] START learning_rate=0.1, max_depth=11, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 315/330] END learning_rate=0.1, max_depth=11, num_leaves=62;, score=0.910 total time=  31.1s\n",
      "[CV 1/5; 317/330] START learning_rate=0.1, max_depth=11, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 317/330] END learning_rate=0.1, max_depth=11, num_leaves=66;, score=0.909 total time=  21.4s\n",
      "[CV 4/5; 318/330] START learning_rate=0.1, max_depth=11, num_leaves=68..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 318/330] END learning_rate=0.1, max_depth=11, num_leaves=68;, score=0.910 total time=  32.4s\n",
      "[CV 5/5; 320/330] START learning_rate=0.1, max_depth=12, num_leaves=50..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 320/330] END learning_rate=0.1, max_depth=12, num_leaves=50;, score=0.909 total time=  19.2s\n",
      "[CV 2/5; 322/330] START learning_rate=0.1, max_depth=12, num_leaves=54..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 322/330] END learning_rate=0.1, max_depth=12, num_leaves=54;, score=0.909 total time=  30.4s\n",
      "[CV 5/5; 323/330] START learning_rate=0.1, max_depth=12, num_leaves=56..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 323/330] END learning_rate=0.1, max_depth=12, num_leaves=56;, score=0.909 total time=  21.3s\n",
      "[CV 2/5; 325/330] START learning_rate=0.1, max_depth=12, num_leaves=60..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6186\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 2/5; 325/330] END learning_rate=0.1, max_depth=12, num_leaves=60;, score=0.909 total time=  20.3s\n",
      "[CV 4/5; 326/330] START learning_rate=0.1, max_depth=12, num_leaves=62..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 326/330] END learning_rate=0.1, max_depth=12, num_leaves=62;, score=0.910 total time=  31.2s\n",
      "[CV 4/5; 328/330] START learning_rate=0.1, max_depth=12, num_leaves=66..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6188\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 4/5; 328/330] END learning_rate=0.1, max_depth=12, num_leaves=66;, score=0.910 total time=  21.3s\n",
      "[CV 1/5; 330/330] START learning_rate=0.1, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856333\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6194\n",
      "[LightGBM] [Info] Number of data points in the train set: 949161, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221911\n",
      "[LightGBM] [Info] Start training from score -2.221911\n",
      "[CV 1/5; 330/330] END learning_rate=0.1, max_depth=12, num_leaves=70;, score=0.909 total time=  22.3s\n",
      "[CV 5/5; 330/330] START learning_rate=0.1, max_depth=12, num_leaves=70..........\n",
      "[LightGBM] [Info] Number of positive: 92828, number of negative: 856334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6176\n",
      "[LightGBM] [Info] Number of data points in the train set: 949162, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[CV 5/5; 330/330] END learning_rate=0.1, max_depth=12, num_leaves=70;, score=0.909 total time=  10.4s\n"
     ]
    }
   ],
   "source": [
    "#grid search cv\n",
    "param_grid_grid = {\n",
    "    'num_leaves': list(range(50, 71, 2)),\n",
    "    'max_depth': list(range(8, 13)),\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.09, 0.1]\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary')\n",
    "grid_cfl = GridSearchCV(lgb_model, param_grid=param_grid_grid, verbose=10, n_jobs=-1)\n",
    "grid_cfl.fit(X_train, y_train)\n",
    "\n",
    "# En iyi parametreleri görüntüle\n",
    "print(\"En iyi parametreler:\", grid_cfl.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b887eae8-d5db-41b1-865e-99047863bf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.07, 'max_depth': 10, 'num_leaves': 68}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cfl.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6a216ba-bd9a-49fd-8773-18aee9a67279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 116035, number of negative: 1070417\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6189\n",
      "[LightGBM] [Info] Number of data points in the train set: 1186452, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n"
     ]
    }
   ],
   "source": [
    "# To get the best threshold for max F1 score\n",
    "# Initializing the model\n",
    "lgbm = lgb.LGBMClassifier(objective='binary', num_leaves=68, max_depth=10,learning_rate=0.07)\n",
    "lgbm.fit(X_train,y_train)\n",
    "thresholds = lgbm.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7eee475-c797-420e-ba7b-ab1211ef63b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Info] Number of positive: 116035, number of negative: 1070417\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5965\n",
      "[LightGBM] [Info] Number of data points in the train set: 1186452, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# En iyi parametreleri kullanarak modeli oluşturma\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    num_leaves=74,  # En iyi parametre olarak belirlenen yaprak sayısı\n",
    "    max_depth=7,    # En iyi parametre olarak belirlenen maksimum derinlik\n",
    "    learning_rate=0.0820643967899203,  # En iyi parametre olarak belirlenen öğrenme oranı\n",
    "    min_data_in_leaf=31,  # En iyi parametre olarak belirlenen her yaprakta olması gereken minimum veri sayısı\n",
    "    bagging_fraction=0.9701151207124787,  # En iyi parametre olarak belirlenen bagging oranı\n",
    "    bagging_freq=8,  # En iyi parametre olarak belirlenen bagging sıklığı\n",
    "    feature_fraction=0.8385841711914909,  # En iyi parametre olarak belirlenen özellik oranı\n",
    "    lambda_l1=0.5733670416719333,  # En iyi parametre olarak belirlenen L1 düzenlileştirme\n",
    "    lambda_l2=0.12850035323391018,  # En iyi parametre olarak belirlenen L2 düzenlileştirme\n",
    "    max_bin=242  # En iyi parametre olarak belirlenen maksimum bin sayısı\n",
    ")\n",
    "\n",
    "# Modeli eğitim verileriyle eğitme\n",
    "lgbm.fit(X_train, y_train)\n",
    "thresholds = lgbm.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd23cfe1-38d0-4758-aaf2-adc96e9226da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the mean and standard deviation of thresholds \n",
    "mean = np.mean(thresholds)\n",
    "std = np.std(thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67e0633e-8531-40ec-9e05-b5094b9d7f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n"
     ]
    }
   ],
   "source": [
    "threshold_values = np.arange(mean-2*std,mean+2*std,0.02)\n",
    "f1_scores = []\n",
    "for threshold in threshold_values:\n",
    "    y_pred = (lgbm.predict_proba(X_test)[:, 1] >= threshold).astype('int')\n",
    "    f1_scores.append(f1_score(y_test , y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fd9d11b-e479-4ef7-960d-bf54e72e78cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAHWCAYAAAAsM2MeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNzklEQVR4nO3deVxU5f4H8M/MAMO+KLKjKC7ggigoF/cS066VZl2XLI3SFrvXjDbNm5pWqNfMFsublanVlSxbfmWUEmQauSC4IuIKiqzKLgPMPL8/kLFJUGCGObN83q/XvF5y5pwz33MAPzznnOd5ZEIIASIiImozudQFEBERmTuGKRERkZ4YpkRERHpimBIREemJYUpERKQnhikREZGeGKZERER6YpgSERHpiWFKRESkJ4YpWYyUlBTIZDJ8+eWXUpcCoH3qWbJkCWQyWYvWlclkWLJkicE+m4iaxzAlkyaTyVr0SklJkbpUaqPGPxCaeq1bt067XkJCAh588EH06NEDMpkMo0aNkq5oor+wkboAopvZvHmzztebNm3Cjh07blgeGhqKzMxMY5ZGBvb+++/D2dlZZ1lUVJTO+2lpaRg0aBBKSkqMXR7RTTFMyaQ9+OCDOl//8ccf2LFjxw3LAegdptXV1XB0dNRrH9R2999/Pzw9PZt9f/PmzfD394dcLkffvn2NWJnhVFVVwcnJSeoyqB3wMi9ZHI1Gg9deew0BAQGwt7fH6NGjcerUKZ11Ro0ahb59+yItLQ0jRoyAo6MjXnrpJQCASqXC4sWL0b17dyiVSgQGBuKFF16ASqXS2ceOHTswbNgwuLu7w9nZGb169dLuo7X1AMDWrVsREREBBwcHeHp64sEHH8TFixdvebwqlQrPPPMMOnXqBBcXF9xzzz24cOHCDetVVFRg3rx5CAoKglKphJeXF8aMGYODBw82u+8vv/wSMpkMv/766w3v/fe//4VMJsPRo0cBAPn5+YiNjUVAQACUSiV8fX0xYcIEnDt37pbH0BKBgYGQy9v+X1ZL6/vxxx8xcuRIuLi4wNXVFYMGDcLnn3+us05LvlcPP/wwnJ2dcfr0afz973+Hi4sLpk+fDqDhZ2LNmjXo06cP7O3t4e3tjccffxxXrlxp8/GRtNgyJYuzfPlyyOVyPPfccygrK8PKlSsxffp07N27V2e9kpIS3HnnnZg6dSoefPBBeHt7Q6PR4J577sHu3bvx2GOPITQ0FEeOHMGbb76JkydP4ptvvgEAHDt2DHfddRfCwsKwdOlSKJVKnDp1Cnv27GlTPZ988gliY2MxaNAgxMfHo6CgAG+99Rb27NmD9PR0uLu7N3u8s2bNwqeffooHHngAQ4YMwS+//ILx48ffsN4TTzyBL7/8Ev/85z/Ru3dvlJSUYPfu3cjMzMTAgQOb3Pf48ePh7OyML774AiNHjtR5LyEhAX369NG2Eu+77z4cO3YM//rXvxAUFITCwkLs2LEDOTk5CAoKarb+RpcvX9b5WqFQwMPD45bbtVRL6vvkk0/wyCOPoE+fPliwYAHc3d2Rnp6OxMREPPDAA9p1Wvq9qq+vx9ixYzFs2DCsWrVKe+Xj8ccf1+5n7ty5OHv2LN59912kp6djz549sLW1Ndhxk5EIIjPy1FNPieZ+bJOTkwUAERoaKlQqlXb5W2+9JQCII0eOaJeNHDlSABDr1q3T2cfmzZuFXC4Xv/32m87ydevWCQBiz549Qggh3nzzTQFAFBUVNVtrS+upra0VXl5eom/fvuLq1ava9b7//nsBQCxatEi7bPHixTrHn5GRIQCIOXPm6Hz2Aw88IACIxYsXa5e5ubmJp556qtl6mzNt2jTh5eUl6uvrtcsuXbok5HK5WLp0qRBCiCtXrggA4j//+U+r9994TH99denSpdlt+vTpI0aOHNniz2hJfaWlpcLFxUVERUXpfB+EEEKj0QghWve9mjlzpgAg5s+fr7Ov3377TQAQn332mc7yxMTEJpeTeeBlXrI4sbGxsLOz0349fPhwAMCZM2d01lMqlYiNjdVZtnXrVoSGhiIkJATFxcXa1+233w4ASE5OBgBt6+Pbb7+FRqPRq54DBw6gsLAQc+bMgb29vXa98ePHIyQkBD/88EOz+96+fTsAYO7cuTrL582bd8O67u7u2Lt3L/Ly8m5a719NmTIFhYWFOk9Mf/nll9BoNJgyZQoAwMHBAXZ2dkhJSWnzpcqvvvoKO3bs0L4+++yzNu2nKS2pb8eOHaioqMD8+fN1vg8AtN2R2vK9evLJJ3W+3rp1K9zc3DBmzBidn7GIiAg4Oztrf8bIvDBMyeJ07txZ5+vGS4V//U/U399fJ+QAIDs7G8eOHUOnTp10Xj179gQAFBYWAmgImKFDh2LWrFnw9vbG1KlT8cUXXzQZrLeq5/z58wCAXr163bBtSEiI9v2mnD9/HnK5HMHBwTrLm9rXypUrcfToUQQGBmLw4MFYsmTJDX9gNGXcuHFwc3NDQkKCdllCQgLCw8O150WpVGLFihX48ccf4e3tjREjRmDlypXIz8+/5f4bjRgxAjExMdrX0KFDW7xto9raWuTn5+u81Gp1i+o7ffo0ANz04abWfq9sbGwQEBCgsyw7OxtlZWXw8vK64eessrJS+zNG5oVhShZHoVA0uVwIofO1g4PDDetoNBr069dPp4X059ecOXO02+7atQs7d+7EQw89hMOHD2PKlCkYM2YM1Gp1m+ppb5MnT8aZM2fwzjvvwM/PD//5z3/Qp08f/PjjjzfdTqlUYuLEifj6669RX1+PixcvYs+ePdpWaaN58+bh5MmTiI+Ph729PV5++WWEhoYiPT29PQ9Lx++//w5fX1+dV25urmT1KZXKGx6a0mg08PLyavZnbOnSpe1WD7UfPoBE9CfBwcE4dOgQRo8efcuRhuRyOUaPHo3Ro0dj9erVeP3117Fw4UIkJycjJiamxZ/ZpUsXAEBWVpb2cnKjrKws7fvNbavRaHD69Gmd1lJWVlaT6/v6+mLOnDmYM2cOCgsLMXDgQLz22mu48847b1rjlClTsHHjRiQlJSEzMxNCiBvCFGg4f88++yyeffZZZGdnIzw8HG+88QY+/fTTm+7fUPr3748dO3boLPPx8WlRfY2t+6NHj6J79+5N7l+f79Wfa9i5cyeGDh3a5B90ZJ7YMiX6k8mTJ+PixYtYv379De9dvXoVVVVVAG588hQAwsPDAeCGLjS3EhkZCS8vL6xbt05n2x9//BGZmZlNPpnbqDEE3377bZ3la9as0flarVajrKxMZ5mXlxf8/PxaVG9MTAw6dOiAhIQEJCQkYPDgwejatav2/erqatTU1OhsExwcDBcXl1afD314eHjoXCqOiYmBvb19i+q744474OLigvj4+BvWbbyKoM/3qtHkyZOhVquxbNmyG96rr69HaWlpaw+bTABbpkR/8tBDD+GLL77AE088geTkZAwdOhRqtRonTpzAF198gZ9++gmRkZFYunQpdu3ahfHjx6NLly4oLCzEe++9h4CAAAwbNqxVn2lra4sVK1YgNjYWI0eOxLRp07TdLYKCgvDMM880u214eDimTZuG9957D2VlZRgyZAiSkpJu6MdaUVGBgIAA3H///ejfvz+cnZ2xc+dO7N+/H2+88UaLapw0aRK2bNmCqqoqrFq1Suf9kydPYvTo0Zg8eTJ69+4NGxsbfP311ygoKMDUqVNbdT6as2vXLuzatQsAUFRUhKqqKrz66qsAGu63jhgxotltW1Kfq6sr3nzzTcyaNQuDBg3CAw88AA8PDxw6dAjV1dXYuHGjXt+rRiNHjsTjjz+O+Ph4ZGRk4I477oCtrS2ys7OxdetWvPXWW7j//vsNcMbIqKR9mJiodVrSNWbr1q06y8+ePSsAiA0bNmiXjRw5UvTp06fJ/dTW1ooVK1aIPn36CKVSKTw8PERERIR45ZVXRFlZmRBCiKSkJDFhwgTh5+cn7OzshJ+fn5g2bZo4efJkm+oRQoiEhAQxYMAAoVQqRYcOHcT06dPFhQsXdNb5a9cYIYS4evWqmDt3rujYsaNwcnISd999t8jNzdXpGqNSqcTzzz8v+vfvL1xcXISTk5Po37+/eO+995o8B03ZsWOHACBkMpnIzc3Vea+4uFg89dRTIiQkRDg5OQk3NzcRFRUlvvjii1vut/GYbtbN6M/rNfX6cxegprSmvu+++04MGTJEODg4CFdXVzF48GDxv//9T2edlnyvZs6cKZycnJqt6YMPPhARERHCwcFBuLi4iH79+okXXnhB5OXl3fRYyDTJhDDyUxBEREQWhvdMiYiI9MQwJSIi0hPDlIiISE8MUyIiIj0xTImIiPTEMCUiItITB21ogkajQV5eHlxcXG45pBwREVkmIQQqKirg5+d3y4npGaZNyMvLQ2BgoNRlEBGRCcjNzb1h9p+/Ypg2wcXFBUDDCXR1dZW4GiIikkJ5eTkCAwO1mXAzDNMmNF7adXV1ZZgSEVm5ltzu4wNIREREemKYEhER6YlhSkREpCeGKRERkZ4YpkRERHpimBIREemJYUpERKQnhikREZGeGKZERER6YpgSERHpiWFKRESkJ4YpERGRnhimREREeuKsMUREBlJ2tQ4nCypwIr8CWfnlOJlfCZVaAw9HW3RwtIO7ox08HG3h7mSnu8zJFh6OdrC3VUh9CNRGDFMiolaqrdfgdFElsvKvB2dWfgXyymr02q+9rRwejnbo5KLEqF5emDTAH0GeTgaqmtqTTAghpC7C1JSXl8PNzQ1lZWWcz5SIcL6kCt8fvqQNzjNFVajXNP1fp5+bPXr5uKCXjytCfFzgaKdAaXUdrlTX4kp1HUqra3G5qvaGZc3tL6KLB+4bGIDx/Xzh5mjbnodJf9GaLGCYNoFhSkQAkF1QgfdSTuPbjIv4a9a5KG2uhaYLQq6FZy9vlzYFnhACFap6lFY1BOyZ4kp8k56H37KLtJ9rZyPHmFBvTBrojxE9O8FWwUde2hvDVE8MUyLrdvRiGdYmn0LisXw0/g85vIcnooM7aoPTz80eMpmsXesoLK/Btxl5+OrgBZzIr9Au7+hkh3vC/XDfwAD08XNt9zqsFcNUTwxTIuuUdv4K3v0lG8lZRdpl4/r44J+3d0dffzcJKwOO55Xjq4MX8G3GRRRX1mqX9/R2xqSBAbh3gD+8Xe0lrNDyMEz1xDAlsh5CCKSeLsE7v5xC6pkSAIBcBtzT3w9zbuuOnt4uEleoq16twW/Zxfjq4AX8fLwAtfUaAA013x7ihRfHhaCHidVsrhimemKYElk+IQSSswrx7i+ncDCnFABgq5DhvoEBeGJksFk8RVt2tQ4/HrmEbQcvYt+5ywAAhVyGB6M645kxPeHuaCdxheaNYaonhimR5dJoBBKP5WNt8ikcyysHACht5Jg2uDMeG9ENfu4OElfYNmeKKrEi8QR+OlYAAHBzsMUzMT0w/W9d+LBSGzFM9cQwJbJMp4sq8dRnB7UP8zjZKfBgdBfMGtYNnVyUEldnGL+fKsbS749rj7G7lzNevqs3RvbsJHFl5odhqieGKZHl2XOqGE9+mobymnq42tvg4aFdETskCB5OlncptF6twZb9uVi94yQuVzU8rHR7iBcWjg9FcCdniaszHwxTPTFMiSzL53tzsOjbo6jXCAzs7I4PZkTC09kyWqI3U3a1Dm8nZWPj7+dQrxGwkcswc0gQ5o7uATcHDgBxKwxTPTFMiSyDWiPw+vZMfLT7LABgYrgflt8XZnVj4J4uqsRrP2TilxOFAIAOTnaIG9MT0wZ3hkLOPqrNYZjqiWFKZP4qVfV4+n/pSLoWIM+O6Yl/3t7dqgc4+PVkEZZ9fxynCisBACE+Llh0V28M6e4pcWWmiWGqJ4YpkXm7cKUaszYewIn8Ciht5Hhjcn/cFeYndVkmoU6twad/nMeandkou1oHALg/IgBLJ/SBox3nPvkzhqmeGKZE5is95wpmb0pDcaUKns5KfDgzEuGB7lKXZXKuVNXizZ0n8ekf56ERQA8vZ6ydPtDkBqmQUmuygJ2PiMhifHcoD1M++APFlSqE+Ljg238OZZA2w8PJDksn9MVns/4GLxclsgsrcc+7u5GwPwdsY7Uew5SIzJ4QAmt2nsTc/6Wjtl6DmFAvfPnkEPib6QAMxhQd3BHbnx6O4T08UVOnwYtfHcEzCRmoUtVLXZpZYZgSkVmrqVPj6S0ZWLMzGwAwe3hX/PehSDgref+vpTydldgYOxgvjOsFhVyGbzLycPc7u3H82ghRdGsMUyIyW0UVKkxb/we+O5QHG7kM8ZP6YeH43uzu0QZyuQxzRnXHlsf+Bl83e5wprsLE9/bgs73nedm3BRimRGSWThZUYOLaPUjPKYWrvQ02PTIY0wZ3lrosszcoqAN+mDsct4d4obZeg4VfH8W//peOipo6qUszaQxTIjI7heU1eOijvbhYehVdPZ3wzVND2VfSgDo42eHDGZF46e8hsJHL8P3hS7jrnd04erFM6tJMFsOUiMyKql6NJz5NQ0G5Ct29nPH1nCHoxvFmDU4ul+GxEcH44olo+Ls74HxJNSa99zs2/n6Ol32bwDAlIrMhhMCib47h4LVLu+tnRHLOznY2sLMHfpg7DGN6e6NWrcHi747hyU8Pagd8oAYMUyIyG5tSzyPhQC7kMuCdBwaiqxlM4G0J3B3t8MFDEVh0V2/YKmRIPJaPu975DaeLKqUuzWQwTInILPx+umGeTgBYcGco5+c0MplMhkeGdcWXTwxBYAcH5F6+ivvf/x0ZuaVSl2YSGKZEZPJyL1fjqc8OQq0RuHeAP2YN7yp1SVarf6A7vpkzFP0D3HClug7TPvgDKVmFUpclOYYpEZm06tp6zN50AFeq6xAW4Ib4Sf2seuYXU9DRWYnPZ/8Nw3t44mqdGrM2HsC2gxekLktSDFMiMllCCDy39RBO5FfA01mJ/z4UYXVzkZoqJ6UNPpo5CBPD/VCvEYj74hDW7zojdVmSYZgSkclam3wK24/kw1Yhw7oHB8LXjWPtmhI7GzlWTw7Ho8MaLru/tj0Tr2/PhEZjfV1nGKZEZJJ2HC/Aqp9PAgCWTuiLyKAOEldETZHLZfj3+FAsuDMEAPDBrjN4dush1Kk1EldmXAxTIjI52QUVeCYhAwAwI7oLhwk0cTKZDI+PDMYb/+gPhVyGr9MvYtbGA6iutZ6ZZximRGRSyqrrMHvTAVSq6hHVtQNevqu31CVRC90XEYAPZ0TC3laOX08WYdr6vbhcVSt1WUbBMCUik6HWCPxrSzrOlVTD390B700fCFsF/5syJ7eFeOHz2X+Du6MtDuWW4v51v+PClWqpy2p3/CklIpOxMvEEdp0sgoOtAh/MiEBHZ6XUJVEbDOzsgS+fiIafmz3OFFXhvvd/x4l8y54blWFKRCbhm/SL+O+1rhX/+UcY+vi5SVwR6aO7lwu+mjMEPb2dUVCuwj/WpWLf2ctSl9VuGKZEJLnDF0rx4leHAQBP3RaMu8L8JK6IDMHXzQFbHx+CyC4eqKipx4Mf7cXO4wVSl9UuGKZEJKmSShUe35wGVb0Go0O88OyYXlKXRAbk5miLT2dFISbUG7X1Gsz5/CD+OFMidVkGxzAlIkm99kMmLpXVoFsnJ7w5NRxyOYcKtDT2tgqse3Ag7ujdEKizNx7A8TzLuofKMCUiyew5VYxt6RchkwGrJ4fD1d5W6pKondgo5Hh72gAMDuqAClU9Zm7Yh5wSy3nKl2FKRJKoqVNj4ddHAAAz/tYF4YHu0hZE7c7eVoH1MyMR4uOCogoVZny8F8WVKqnLMgiGKRFJYm3yKZwrqYa3qxLPjeV9Umvh5mCLjY8MRoCHA86VVCN2w35Uqsx/pCSGKREZXXZBBdb9ehoA8Mo9feDCy7tWxdvVHpseGYwOTnY4crEMj28+AFW9Wuqy9MIwJSKj0mgEXvr6COrUAjGhXhjbx0fqkkgC3To545PYQXCyU2DPqRLEfXEIajOebYZhSkRG9cWBXOw/dwWOdgq8MqEvJ/q2YmEB7vjvQ5GwVcjww+FLeOX/jkEI8wxUhikRGU1RhQqvb88EAMSN6Ql/d85Pau2G9fDE6snhkMmATann8c4vp6QuqU0kD9O1a9ciKCgI9vb2iIqKwr59+1q03ZYtWyCTyTBx4kSd5UIILFq0CL6+vnBwcEBMTAyys7PboXIiaq1XfziO8pp69PV3xcNDgqQuh0zE3f39sOTuPgCA1TtO4rO95yWuqPUkDdOEhATExcVh8eLFOHjwIPr374+xY8eisLDwptudO3cOzz33HIYPH37DeytXrsTbb7+NdevWYe/evXBycsLYsWNRU1PTXodBRC3w68kifJuRB7kMiL83DDacDYb+ZOaQIPzr9u4AgJe/OYrEo5ckrqh1JP1pXr16NWbPno3Y2Fj07t0b69atg6OjIz7++ONmt1Gr1Zg+fTpeeeUVdOvWTec9IQTWrFmDf//735gwYQLCwsKwadMm5OXl4ZtvvmnnoyGi5lytVePf3zT0KZ05JAj9AjiIPd0obkxPTBscCI0A5m7JMKthByUL09raWqSlpSEmJuZ6MXI5YmJikJqa2ux2S5cuhZeXFx599NEb3jt79izy8/N19unm5oaoqKib7lOlUqG8vFznRUSG8/Yv2ci9fBW+bvZ49g72KaWmyWQyLJvQV2fYwWN5ZVKX1SKShWlxcTHUajW8vb11lnt7eyM/P7/JbXbv3o2PPvoI69evb/L9xu1as08AiI+Ph5ubm/YVGBjYmkMhops4kV+O9demVnvlnj5wVtpIXBGZMu2wg12vDTv48X6zGHbQbG5aVFRU4KGHHsL69evh6elp0H0vWLAAZWVl2ldubq5B909krTQagZe2HUG9RmBsH2/cwT6l1AL2tgqsn9Ew7GBxpQoPmcGwg5L9iejp6QmFQoGCAt257QoKCuDjc+Mv3OnTp3Hu3Dncfffd2mUajQYAYGNjg6ysLO12BQUF8PX11dlneHh4s7UolUoolUp9DoeImvD5vhwczCmFk50CS+7pI3U5ZEbcHGyx6ZHBuG/d7zhfUo3HNh3A57P/BntbhdSlNUmylqmdnR0iIiKQlJSkXabRaJCUlITo6Ogb1g8JCcGRI0eQkZGhfd1zzz247bbbkJGRgcDAQHTt2hU+Pj46+ywvL8fevXub3CcRtZ/C8hqsSDwBAHhubC/4urFPKbWOl6s9Njw8GK72NjiYU4oXvjxssoM6SHrzIi4uDjNnzkRkZCQGDx6MNWvWoKqqCrGxsQCAGTNmwN/fH/Hx8bC3t0ffvn11tnd3dwcAneXz5s3Dq6++ih49eqBr1654+eWX4efnd0N/VCJqX698fxwVNfUIC3DDjOggqcshM9XdyxnrHozAjI/34btDeejq6YRnxvSUuqwbSBqmU6ZMQVFRERYtWoT8/HyEh4cjMTFR+wBRTk4O5PLWNZ5feOEFVFVV4bHHHkNpaSmGDRuGxMRE2Nvbt8chEFETkk8U4ofDl6CQy/D6vf2g4ITfpIch3T3x2r198eJXR/BWUja6ejph4gB/qcvSIROm2maWUHl5Odzc3FBWVgZXV1epyyEyK9W19Rizehcull7F7OFdsXB8b6lLIgsR/2Mm/vvrGdgp5PhsdhQGBXVo189rTRaYzdO8RGQe3tqZjYulV+Hv7oB5MaZ3OY7M14tjQzCujw9q1Ro8tukAzpdUSV2SFsOUiAzmeF45Ptx9FgCwdEIfOLFPKRmQXC7Dm1PCERbghivVdYj9ZD/KquukLgsAw5SIDEQIgUXfHoVaI/D3fj4YHep9642IWsnBToEPZ0TC180eZ4qq8ORnaahTa6Qui2FKRIaReroEB85fgZ2NHIvuYp9Saj9ervb4aGbDxOK/ny7Bv78+KnmXGYYpERnEu8kN81BOHRQIHzc+PU/tq7efK955YADkMiDhQC4+uDZkpVQYpkSkt7TzV/D76RLYyGV4fGSw1OWQlbg9xBsv39XwtPjyxBOSTtvGMCUiva291iq9d4A//N050hEZz8NDgjAjuguEAOYlZODwhVJJ6mCYEpFejuWV4ZcThZDLgCdHsVVKxiWTybDort4Y2bMTauo0mLXxAPJKrxq9DoYpEenlveTTAIC/9/NFt07OEldD1shGIce7DwxAL28XFFao8OjGA6hU1Ru1BoYpEbXZqcJKbL92n+qp27pLXA1ZMxd7W3z0cCQ8nZXIvFSOuf9Lh1pjvCd8GaZE1Gbvp5yGEEBMqDdCfTn0JkkrwMMR62dEQGkjxy8nCvHqD8eN9tkMUyJqk9zL1fgm4yIA4J+3s1VKpmFAZw+snhwOAPj5WAFKq2uN8rkc64uI2uS/u05DrREY1t0T4YHuUpdDpDU+zBc1df0xomcnuDvaGeUzGaZE1GoF5TX44sAFALxXSqbpvogAo34eL/MSUaut33UGtfUaRHTxwN+6te80WETmgGFKRK1yuaoWn+3NAQD887bukMk48TcRw5SIWmXDnrO4WqdGHz9XjOrVSepyiEwCw5SIWqy8pg6f/H4OAFulRH/GMCWiFtuceh4VNfXo7uWMsX18pC6HyGQwTImoRa7WqvHx7rMAgDmjgiGXs1VK1IhhSkQt8r99OSipqkVgBwfc099P6nKITArDlIhuSVWv1k6+/MTIYNgo+F8H0Z/xN4KIbmnbwYvIL6+Bt6sS9xu5MzyROWCYEtFN1as1eD+lYZq1x0YEQ2mjkLgiItPDMCWim/q/w3nIuVyNDk52mDY4UOpyiEwSw5SImqXRCO3k348O6wpHOw7nTdQUhikRNevn4/nILqyEi70NHoruInU5RCaLYUpETRJC4N3kUwCAmdFBcLW3lbgiItPFMCWiJv16sghHL5bDwVaBR4Z1lbocIpPGMCWiJq291iqdHtUZHZyMM8EykblimBLRDfafu4z9567ATiHH7BHdpC6HyOQxTInoBv/b1zBf6b0D/OHtai9xNUSmj2FKRDqqVPVIPJoPAJg8iKMdEbUEw5SIdPx4NB/VtWp09XTCwM4eUpdDZBYYpkSk46u0CwCASQP8Ofk3UQsxTIlI68KVaqSeKQEA3DvQX+JqiMwHw5SItL5JvwgAiO7WEQEejhJXQ2Q+GKZEBKBhxKOvDjaE6SS2SolahWFKRACAgzmlOFtcBQdbBe7s5yt1OURmhWFKRACArw42PHh0Z18fOCs5OwxRazBMiQg1dWp8fygPAHBfBPuWErUWw5SIkJRZiPKaevi62eNv3TpKXQ6R2WGYEpH2Eu+9A/yhkLNvKVFrMUyJrFxRhQq/niwCwEu8RG3FMCWyct9mXIRaIxAe6I7gTs5Sl0NklhimRFausW8pW6VEbccwJbJix/PKkXmpHHYKOe4OY99SorZimBJZscYHj0aHesHd0U7iaojMF8OUyErVqTX4NuPaJd6BvMRLpA+GKZGV+i27CMWVtejoZIeRvTpJXQ6RWWOYElmpr9IaWqUTwv1hq+B/BUT64G8QkRUqq67DjuMFADhDDJEhMEyJrND/Hc5DrVqDEB8X9PFzlbocIrPHMCWyQo1P8d43MAAyGYcPJNIXw5TIypwpqkR6TikUchkmDPCTuhwii8AwJbIy266NeDSihye8XOwlrobIMjBMiayIRiPwdXpDmE5i31Iig5E8TNeuXYugoCDY29sjKioK+/bta3bdbdu2ITIyEu7u7nByckJ4eDg2b96ss87DDz8MmUym8xo3blx7HwaRWfjjTAkull6Fi70NxvT2lrocIothI+WHJyQkIC4uDuvWrUNUVBTWrFmDsWPHIisrC15eXjes36FDByxcuBAhISGws7PD999/j9jYWHh5eWHs2LHa9caNG4cNGzZov1YqlUY5HiJT9+W1B4/uCvODva1C4mqILIekLdPVq1dj9uzZiI2NRe/evbFu3To4Ojri448/bnL9UaNG4d5770VoaCiCg4Px9NNPIywsDLt379ZZT6lUwsfHR/vy8PAwxuEQmbQqVT0Sj+YDAO6PYN9SIkOSLExra2uRlpaGmJiY68XI5YiJiUFqauottxdCICkpCVlZWRgxYoTOeykpKfDy8kKvXr3w5JNPoqSk5Kb7UqlUKC8v13kRWZrEo/morlUjqKMjBnbmH5hEhiTZZd7i4mKo1Wp4e+vet/H29saJEyea3a6srAz+/v5QqVRQKBR47733MGbMGO3748aNw6RJk9C1a1ecPn0aL730Eu68806kpqZCoWj6slZ8fDxeeeUVwxwYkYlq7Fs6iX1LiQxO0numbeHi4oKMjAxUVlYiKSkJcXFx6NatG0aNGgUAmDp1qnbdfv36ISwsDMHBwUhJScHo0aOb3OeCBQsQFxen/bq8vByBgYHtehxExnSx9CpSzzRcobl3AC/xEhmaZGHq6ekJhUKBgoICneUFBQXw8fFpdju5XI7u3bsDAMLDw5GZmYn4+HhtmP5Vt27d4OnpiVOnTjUbpkqlkg8pkUX7+uAFCAH8rVsHBHZwlLocIosj2T1TOzs7REREICkpSbtMo9EgKSkJ0dHRLd6PRqOBSqVq9v0LFy6gpKQEvr6+etVLZK6EENqBGjhvKVH7kPQyb1xcHGbOnInIyEgMHjwYa9asQVVVFWJjYwEAM2bMgL+/P+Lj4wE03NuMjIxEcHAwVCoVtm/fjs2bN+P9998HAFRWVuKVV17BfffdBx8fH5w+fRovvPACunfvrtN1hsiapOeW4kxxFRxsFbizH/+oJGoPkobplClTUFRUhEWLFiE/Px/h4eFITEzUPpSUk5MDufx647mqqgpz5szBhQsX4ODggJCQEHz66aeYMmUKAEChUODw4cPYuHEjSktL4efnhzvuuAPLli3jZVyyWl+lNTx4NK6vD5yVZveYBJFZkAkhhNRFmJry8nK4ubmhrKwMrq6cnorMV71ag8jXdqK0ug6fPhqFYT08pS6JyGy0JgskH06QiNpPRm4pSqvr4O5oi+jgjlKXQ2SxGKZEFiw5qxAAMKJHJyjk7FtK1F4YpkQWLPlEEQDgtpBOEldCZNkYpkQWKr+sBscvlUMma2iZElH7YZgSWahfTzZc4g0LcEdHZz7NTtSeGKZEFiol69ol3l5slRK1N4YpkQWqU2vwW3YxAOC2XjfODUxEhsUwJbJAB85dQaWqHh2d7NDP303qcogsHsOUyAKlXOsSM7JXJ8jZJYao3TFMiSxQY//SUbzES2QUDFMiC3Ox9CpOFlRCLgNGcPhAIqNgmBJZmMZLvAM7e8Dd0U7iaoisA8OUyMJcH/WIl3iJjIVhSmRBVPVq7DnV0CVmFPuXEhkNw5TIguw7exlX69TwclGity+nDyQyFoYpkQVpvMQ7qlcnyGTsEkNkLAxTIguScm08Xo56RGRcDFMiC3G+pApniqpgI5dhKLvEEBkVw5TIQjQObB8Z5AFXe1uJqyGyLgxTIgvROOoRL/ESGR/DlMgC1NSpkXq6BAD7lxJJgWFKZAFSz5RAVa+Bn5s9eng5S10OkdVhmBJZgJQT1wa2D/FilxgiCTBMicycEALJ1x4+4v1SImkwTInM3JniKuRcroadQo4hwR2lLofIKukVpqdOncJPP/2Eq1evAmj4C5mIjCv52iXeqG4d4KS0kbgaIuvUpjAtKSlBTEwMevbsib///e+4dOkSAODRRx/Fs88+a9ACiejmGvuXciJwIum0KUyfeeYZ2NjYICcnB46OjtrlU6ZMQWJiosGKI6Kbq1LVY9/ZywA4SwyRlNp0Tejnn3/GTz/9hICAAJ3lPXr0wPnz5w1SGBHd2u+nS1Cr1qBzB0d083SSuhwiq9WmlmlVVZVOi7TR5cuXoVQq9S6KiFrm+qhHnCWGSEptCtPhw4dj06ZN2q9lMhk0Gg1WrlyJ2267zWDFEVHzhBA6/UuJSDptusy7cuVKjB49GgcOHEBtbS1eeOEFHDt2DJcvX8aePXsMXSMRNeFkQSXyymqgtJEjuhu7xBBJqU0t0759++LkyZMYNmwYJkyYgKqqKkyaNAnp6ekIDg42dI1E1ISUa5d4hwR3hL2tQuJqiKxbq1umdXV1GDduHNatW4eFCxe2R01E1AKN90vZJYZIeq1umdra2uLw4cPtUQsRtVB5TR0OnLsCgEMIEpmCNl3mffDBB/HRRx8ZuhYiaqE92cWo1wh06+SEzh1vfLKeiIyrTQ8g1dfX4+OPP8bOnTsREREBJyfd/m2rV682SHFE1DROBE5kWtoUpkePHsXAgQMBACdPntR5j33diNqXEEI7hCDDlMg0tClMk5OTDV0HEbXQ8UvlKKxQwdFOgUFdPaQuh4hggCnYLly4gAsXLhiiFiJqgcZW6ZBgTyht2CWGyBS0KUw1Gg2WLl0KNzc3dOnSBV26dIG7uzuWLVsGjUZj6BqJ6E8ap1y7LYQD2xOZijZd5l24cCE++ugjLF++HEOHDgUA7N69G0uWLEFNTQ1ee+01gxZJRA1Kq2txMKehSwz7lxKZjjaF6caNG/Hhhx/innvu0S4LCwuDv78/5syZwzAlaie7souhEUAvbxf4uztIXQ4RXdOmy7yXL19GSEjIDctDQkJw+fJlvYsioqZdH9iel3iJTEmbwrR///549913b1j+7rvvon///noXRUQ30mgEfj3Z8PDRqJ68xEtkSto8a8z48eOxc+dOREdHAwBSU1ORm5uL7du3G7RAImpw5GIZSqpq4ay0QWQQu8QQmZI2tUxHjhyJrKws3HvvvSgtLUVpaSkmTZqErKwsDB8+3NA1EhGAXddapcO6e8JWoXevNiIyoDa1TAHA39+fDxoRGdH+8w1P8UYHc+5SIlPTpj9vN2zYgK1bt96wfOvWrdi4caPeRRGRLrVGIP1amEZ04SVeIlPTpjCNj4+Hp6fnDcu9vLzw+uuv610UEek6WVCBClU9nOwUCPFxkbocIvqLNoVpTk4OunbtesPyLl26ICcnR++iiEjXgWut0gGdPWDD+6VEJqdNv5VeXl5NThB+6NAhdOzI+zlEhpZ2rqH/Ni/xEpmmNoXptGnTMHfuXCQnJ0OtVkOtVuOXX37B008/jalTpxq6RiKr19gyZZcYItPUpqd5ly1bhnPnzmH06NGwsWnYhUajwYwZM3jPlMjACsprcOHKVchlDZd5icj0tClM7ezskJCQgFdffRUZGRlwcHBAv3790KVLF0PXR2T1DpxraJWG+LjCWdnm3mxE1I70+s3s0aMHevToAbVajSNHjsDV1RUeHvzLmciQDpxvuF/KS7xEpqtN90znzZuHjz76CACgVqsxcuRIDBw4EIGBgUhJSTFkfURWL439S4lMXpvC9Msvv9QOaP9///d/OHPmDE6cOIFnnnkGCxcubNW+1q5di6CgINjb2yMqKgr79u1rdt1t27YhMjIS7u7ucHJyQnh4ODZv3qyzjhACixYtgq+vLxwcHBATE4Ps7OzWHySRCaiurcexvHIAQGRQB4mrIaLmtClMi4uL4ePjAwDYvn07Jk+ejJ49e+KRRx7BkSNHWryfhIQExMXFYfHixTh48CD69++PsWPHorCwsMn1O3TogIULFyI1NRWHDx9GbGwsYmNj8dNPP2nXWblyJd5++22sW7cOe/fuhZOTE8aOHYuampq2HCqRpDJyS6HWCPi62XP+UiIT1qYw9fb2xvHjx6FWq5GYmIgxY8YAAKqrq6FQKFq8n9WrV2P27NmIjY1F7969sW7dOjg6OuLjjz9ucv1Ro0bh3nvvRWhoKIKDg/H0008jLCwMu3fvBtDQKl2zZg3+/e9/Y8KECQgLC8OmTZuQl5eHb775pi2HSiSptHO8xEtkDtoUprGxsZg8eTL69u0LmUyGmJgYAMDevXubnDS8KbW1tUhLS9NuCwByuRwxMTFITU295fZCCCQlJSErKwsjRowAAJw9exb5+fk6+3Rzc0NUVNRN96lSqVBeXq7zIjIF2v6lDFMik9amp3mXLFmCvn37Ijc3F//4xz+gVCoBAAqFAvPnz2/RPoqLi6FWq+Ht7a2z3NvbGydOnGh2u7KyMvj7+0OlUkGhUOC9997Ttozz8/O1+/jrPhvfa0p8fDxeeeWVFtVNZCwajcDBnMbBGni/lMiUtblrzP333w8AuHDhAjQaDeRyOWbOnGmwwprj4uKCjIwMVFZWIikpCXFxcejWrRtGjRrV5n0uWLAAcXFx2q/Ly8sRGBhogGqJ2u5kYQUqaurhyMHtiUye3j3Ae/fujYyMDHTr1q1V23l6ekKhUKCgoEBneUFBgfbhpqbI5XJ0794dABAeHo7MzEzEx8dj1KhR2u0KCgrg6+urs8/w8PBm96lUKrWtayJT0ThYw4DO7hzcnsjE6f0bKoRo03Z2dnaIiIhAUlKSdplGo0FSUhKio6NbvB+NRgOVSgUA6Nq1K3x8fHT2WV5ejr1797Zqn0Sm4Hr/Ul7iJTJ1ko5NFhcXh5kzZyIyMhKDBw/GmjVrUFVVhdjYWADAjBkz4O/vj/j4eAAN9zYjIyMRHBwMlUqF7du3Y/PmzXj//fcBADKZDPPmzcOrr76KHj16oGvXrnj55Zfh5+eHiRMnSnWYRG3SOPIRn+QlMn16h+lLL72EDh3a9pfzlClTUFRUhEWLFiE/Px/h4eFITEzUPkCUk5MDufx647mqqgpz5szBhQsX4ODggJCQEHz66aeYMmWKdp0XXngBVVVVeOyxx1BaWophw4YhMTER9vb2+h0okREVltcg9/JVyGQNl3mJyLTJRFuv01qw8vJyuLm5oaysDK6urlKXQ1boxyOX8ORnBxHi44LEeSOkLofIKrUmCwz6VENubi4eeeQRQ+6SyCpx/lIi82LQML18+TI2btxoyF0SWaXrgzXw4SMic9Cqe6bffffdTd8/c+aMXsUQEXC1Vo1jF8sA8OEjInPRqjCdOHEiZDLZTbvDyGQyvYsismaHLpSiXiPg7apEgAcHtycyB626zOvr64tt27ZBo9E0+Tp48GB71UlkNdL+dImXf5wSmYdWhWlERATS0tKaff9WrVYiurUD59i/lMjctOoy7/PPP4+qqqpm3+/evTuSk5P1LorIWmk04nrLlE/yEpmNVoWpv78/unbt2uz7Tk5OGDlypN5FEVmrU0WVKK+ph4OtAqG+7ONMZC5adZm3R48eKCoq0n49ZcqUGwaqJ6K2axzcPjzQHbYc3J7IbLTqt/Wv90O3b99+08u+RNQ6jePx8hIvkXnhn75EJuT6TDEMUyJz0qowlclkNzyqz0f3iQyjqEKF8yXVkMmAgQxTIrPSqgeQhBB4+OGHtRNp19TU4IknnoCTk5POetu2bTNchURWIu3aJd5e3i5wtbeVuBoiao1WhenMmTN1vn7wwQcNWgyRNWt8+IiXeInMT6vCdMOGDe1VB5HV40wxROaLDyARmYCaOjWO5TUMbs+ZYojMD8OUyAQcyi1FnVqgkwsHtycyRwxTIhNwff5SDz4hT2SGGKZEJoD9S4nMG8OUSGK6g9vzfimROWKYEknsdFElyq7Wwd5Wjj5+HNyeyBwxTIkk1ni/tH8AB7cnMlf8zSWSWONgDexfSmS+GKZEEmscRpD9S4nMF8OUSELFlSqcK6kGAAzszJYpkblimBJJqPEp3p7eznBz5OD2ROaKYUokoev9S3mJl8icMUyJJHTgXOP9Ul7iJTJnDFMiidTUqXH0YjkAPslLZO4YpkQSOXKxDLVqDTydlejcwVHqcohIDwxTIolo+5dycHsis8cwJZKItn8pL/ESmT2GKZEEhBCcKYbIgjBMiSRwuqgKV6rroLSRo4+fm9TlEJGeGKZEEmi8xNs/0B12Nvw1JDJ3/C0mkkDjw0e8xEtkGRimRBLQTgbOMCWyCAxTIiMrq67DmeIqABzcnshSMEyJjOxEfsOoR/7uDvBwspO4GiIyBIYpkZGdyK8AAIT6ukhcCREZCsOUyMgaW6YhPq4SV0JEhsIwJTKyzEsNLdMQtkyJLAbDlMiINBqBrGuXedkyJbIcDFMiI8q5XI2rdWoobeQI6siZYogsBcOUyIga75f29HaBjYK/fkSWgr/NREakvV/qw/ulRJaEYUpkRNoneX15v5TIkjBMiYxI28eULVMii8IwJTKSKlU9zpdUAwB6MUyJLArDlMhIsgoaWqVeLkp0dFZKXA0RGRLDlMhITmgHa+D9UiJLwzAlMpLrwwjyEi+RpWGYEhnJCXaLIbJYDFMiIxBCIJMD3BNZLIYpkRFcKqtBRU09bOQyBHs5SV0OERkYw5TICBrvlwZ3cobSRiFxNURkaAxTIiPgtGtElk3yMF27di2CgoJgb2+PqKgo7Nu3r9l1169fj+HDh8PDwwMeHh6IiYm5Yf2HH34YMplM5zVu3Lj2PgyimzrBadeILJqkYZqQkIC4uDgsXrwYBw8eRP/+/TF27FgUFhY2uX5KSgqmTZuG5ORkpKamIjAwEHfccQcuXryos964ceNw6dIl7et///ufMQ6HqFknLjWOycuWKZElkjRMV69ejdmzZyM2Nha9e/fGunXr4OjoiI8//rjJ9T/77DPMmTMH4eHhCAkJwYcffgiNRoOkpCSd9ZRKJXx8fLQvDw8PYxwOUZNq6tQ4U1wFAAhly5TIIkkWprW1tUhLS0NMTMz1YuRyxMTEIDU1tUX7qK6uRl1dHTp06KCzPCUlBV5eXujVqxeefPJJlJSU3HQ/KpUK5eXlOi8iQzlVWAm1RsDd0RberhxGkMgSSRamxcXFUKvV8Pb21lnu7e2N/Pz8Fu3jxRdfhJ+fn04gjxs3Dps2bUJSUhJWrFiBX3/9FXfeeSfUanWz+4mPj4ebm5v2FRgY2LaDImrC9fulLpDJZBJXQ0TtwUbqAtpq+fLl2LJlC1JSUmBvb69dPnXqVO2/+/Xrh7CwMAQHByMlJQWjR49ucl8LFixAXFyc9uvy8nIGKhmM9n4pL/ESWSzJWqaenp5QKBQoKCjQWV5QUAAfH5+bbrtq1SosX74cP//8M8LCwm66brdu3eDp6YlTp041u45SqYSrq6vOi8hQtHOY8uEjIoslWZja2dkhIiJC5+GhxoeJoqOjm91u5cqVWLZsGRITExEZGXnLz7lw4QJKSkrg6+trkLqJWusEhxEksniSPs0bFxeH9evXY+PGjcjMzMSTTz6JqqoqxMbGAgBmzJiBBQsWaNdfsWIFXn75ZXz88ccICgpCfn4+8vPzUVlZCQCorKzE888/jz/++APnzp1DUlISJkyYgO7du2Ps2LGSHCNZt6IKFYorayGTAT292TIlslSS3jOdMmUKioqKsGjRIuTn5yM8PByJiYnah5JycnIgl1/P+/fffx+1tbW4//77dfazePFiLFmyBAqFAocPH8bGjRtRWloKPz8/3HHHHVi2bBmUSj5FScbX2Crt2tEJDnYcRpDIUsmEEELqIkxNeXk53NzcUFZWxvunpJf1u87gte2Z+Hs/H7w3PULqcoioFVqTBZIPJ0hkyTjtGpF1YJgStSNOCE5kHRimRO2kTq3BqcKGh+NCfdkyJbJkDFOidnK2uAq1ag2clTbwd3eQuhwiakcMU6J2knlt5KNePi6QyzmMIJElY5gStZM/j8lLRJaNYUrUTq7PYcr7pUSWjmFK1E60Y/KyZUpk8RimRO2gtLoWl8pqAAA9GaZEFo9hStQOGlulAR4OcLW3lbgaImpvDFOidpClffiI90uJrAHDlKgdNA5wzzlMiawDw5SoHWReYsuUyJowTIkMTKMR2su8vfjwEZFVYJgSGVjO5WpcrVNDaSNHUEdHqcshIiNgmBIZWOP90p7eLrBR8FeMyBrwN53IwDI57RqR1WGYEhlYY8uUwwgSWQ+GKZGBcRhBIuvDMCUyoCpVPc6XVAPgk7xE1oRhSmRAWQUNrVIvFyU6OislroaIjIVhSmRAJxofPuL9UiKrwjAlMiDtMIK8xEtkVRimRAZ0vWXKMCWyJgxTIgMRQiCzsVsMx+QlsioMUyIDySurQUVNPWzkMgR3cpa6HCIyIoYpkYGcuNTQKu3u5Qw7G/5qEVkT/sYTGciJfA4jSGStGKZEBpJ5icMIElkrhimRgbBlSmS9GKZEBlBTp8aZokoAQChbpkRWh2FKZACnCiuhEYCHoy28XDiMIJG1YZgSGYD2fqmPK2QymcTVEJGxMUyJDEB7v5QjHxFZJYYpkQFkaecw5f1SImvEMCUygMYB7tkyJbJODFMiPRVVqFBcWQu5DOjhxTAlskYMUyI9NbZKgzyd4GCnkLgaIpICw5RIT43TrvF+KZH1YpgS6en6tGu8xEtkrRimRHq6PiE4W6ZE1ophSqSHOrUGpwobhhFky5TIejFMifRwtrgKtWoNnJU2CPBwkLocIpIIw5RID9eHEXThMIJEVoxhSqSH49fCtBcv8RJZNYYpkR52ZxcDAAZ09pC4EiKSEsOUqI0ulV3FsbxyyGTAbb06SV0OEUmIYUrURr+cKAQADOzsgY7OnMOUyJoxTInaKCmzIUxvD/GSuBIikhrDlKgNrtaqsedUw/3SmFBviashIqkxTInaYM+pYqjqNfB3d0BPb2epyyEiiTFMidog6UQBACAm1Iv9S4mIYUrUWkKI6/dLeYmXiMAwJWq1oxfLUVihgpOdAn/r1kHqcojIBDBMiVqp8RLv8B6doLThZOBExDAlarXrl3jZJYaIGjBMiVqhoLwGRy6WXRv1iGFKRA0kD9O1a9ciKCgI9vb2iIqKwr59+5pdd/369Rg+fDg8PDzg4eGBmJiYG9YXQmDRokXw9fWFg4MDYmJikJ2d3d6HQVaicdSj/gHu6OTCUY+IqIGkYZqQkIC4uDgsXrwYBw8eRP/+/TF27FgUFhY2uX5KSgqmTZuG5ORkpKamIjAwEHfccQcuXryoXWflypV4++23sW7dOuzduxdOTk4YO3YsampqjHVYZMGSMq93iSEiaiQTQgipPjwqKgqDBg3Cu+++CwDQaDQIDAzEv/71L8yfP/+W26vVanh4eODdd9/FjBkzIISAn58fnn32WTz33HMAgLKyMnh7e+OTTz7B1KlTW1RXeXk53NzcUFZWBldX17YfIFmUmjo1wpf+jJo6DbbPHY7efvzZILJkrckCyVqmtbW1SEtLQ0xMzPVi5HLExMQgNTW1Rfuorq5GXV0dOnRo6J5w9uxZ5Ofn6+zTzc0NUVFRN92nSqVCeXm5zovor34/XYyaOg383OwR6sv5S4noOsnCtLi4GGq1Gt7eup3evb29kZ+f36J9vPjii/Dz89OGZ+N2rd1nfHw83NzctK/AwMDWHApZiT8/xctRj4jozyR/AKmtli9fji1btuDrr7+Gvb29XvtasGABysrKtK/c3FwDVUmWQgihffhoNEc9IqK/sJHqgz09PaFQKFBQUKCzvKCgAD4+PjfddtWqVVi+fDl27tyJsLAw7fLG7QoKCuDr66uzz/Dw8Gb3p1QqoVTyyUxq3rG8clwqq4GDrQLR3TpKXQ4RmRjJWqZ2dnaIiIhAUlKSdplGo0FSUhKio6Ob3W7lypVYtmwZEhMTERkZqfNe165d4ePjo7PP8vJy7N2796b7JLqVxlbpsB6esLflqEdEpEuylikAxMXFYebMmYiMjMTgwYOxZs0aVFVVITY2FgAwY8YM+Pv7Iz4+HgCwYsUKLFq0CJ9//jmCgoK090GdnZ3h7OwMmUyGefPm4dVXX0WPHj3QtWtXvPzyy/Dz88PEiROlOkyyAOwSQ0Q3I2mYTpkyBUVFRVi0aBHy8/MRHh6OxMRE7QNEOTk5kMuvN57ff/991NbW4v7779fZz+LFi7FkyRIAwAsvvICqqio89thjKC0txbBhw5CYmKj3fVWyXoUVNTh0oQwARz0ioqZJ2s/UVLGfKf1Zwv4cvPjVEfQPcMO3/xwmdTlEZCRm0c+UyFxou8SE8CleImoaw5ToJmrq1PgtuxgAMJr3S4moGQxToptIPVOCq3Vq+Ljaow+HDySiZjBMiW7iF456REQtwDAlaoYQgl1iiKhFGKZEzTiRX4G8shrY28oxJNhT6nKIyIQxTIma0dgqHdadox4R0c0xTImakXSCXWKIqGUYpkRNKK5UISO3FABwewjvlxLRzTFMiZqQfKIQQgB9/V3h48ahKIno5himRE1oHPVoNC/xElELMEyJ/kJVr8Zv2UUAgBhOBE5ELcAwJfqLvWcuo6pWDS8XJUc9IqIWYZgS/UVjl5jRoV6QyznqERHdGsOU6E+EEOwSQ0StxjAl+pOTBZW4cOUqlDZyDOvOUY+IqGUYpkR/knSi4RLvkOCOcLDjqEdE1DIMU6I/0XaJ4VO8RNQKDFOia0oqVTiYcwUARz0iotZhmBJdk5JVBCGA3r6u8HN3kLocIjIjDFOiaxrvl3LuUiJqLRupC7BUG/acRcL+XKnLoFY4U1QFALid90uJqJUYpu2kqEKFE/kVUpdBrdSloyPC/N2kLoOIzAzDtJ1MjgzEkGD2UzQ3ob4uHPWIiFqNYdpOgjydEOTpJHUZRERkBHwAiYiISE8MUyIiIj0xTImIiPTEMCUiItITw5SIiEhPDFMiIiI9MUyJiIj0xDAlIiLSE8OUiIhITwxTIiIiPTFMiYiI9MQwJSIi0hPDlIiISE8MUyIiIj1xCrYmCCEAAOXl5RJXQkREUmnMgMZMuBmGaRMqKioAAIGBgRJXQkREUquoqICbm9tN15GJlkSuldFoNMjLy4OLiwtkMpnU5RhFeXk5AgMDkZubC1dXV6nLkRzPx3U8F7p4PnRZ8vkQQqCiogJ+fn6Qy29+V5Qt0ybI5XIEBARIXYYkXF1dLe4XQh88H9fxXOji+dBlqefjVi3SRnwAiYiISE8MUyIiIj0xTAkAoFQqsXjxYiiVSqlLMQk8H9fxXOji+dDF89GADyARERHpiS1TIiIiPTFMiYiI9MQwJSIi0hPDlIiISE8MUyt2+fJlTJ8+Ha6urnB3d8ejjz6KysrKm27zwQcfYNSoUXB1dYVMJkNpaalxim0Ha9euRVBQEOzt7REVFYV9+/bddP2tW7ciJCQE9vb26NevH7Zv326kSttfa87FsWPHcN999yEoKAgymQxr1qwxXqFG0przsX79egwfPhweHh7w8PBATEzMLX+WzE1rzse2bdsQGRkJd3d3ODk5ITw8HJs3bzZitdJgmFqx6dOn49ixY9ixYwe+//577Nq1C4899thNt6mursa4cePw0ksvGanK9pGQkIC4uDgsXrwYBw8eRP/+/TF27FgUFhY2uf7vv/+OadOm4dFHH0V6ejomTpyIiRMn4ujRo0au3PBaey6qq6vRrVs3LF++HD4+Pkautv219nykpKRg2rRpSE5ORmpqKgIDA3HHHXfg4sWLRq68fbT2fHTo0AELFy5EamoqDh8+jNjYWMTGxuKnn34ycuVGJsgqHT9+XAAQ+/fv1y778ccfhUwmExcvXrzl9snJyQKAuHLlSjtW2X4GDx4snnrqKe3XarVa+Pn5ifj4+CbXnzx5shg/frzOsqioKPH444+3a53G0Npz8WddunQRb775ZjtWZ3z6nA8hhKivrxcuLi5i48aN7VWiUel7PoQQYsCAAeLf//53e5RnMtgytVKpqalwd3dHZGSkdllMTAzkcjn27t0rYWXtr7a2FmlpaYiJidEuk8vliImJQWpqapPbpKam6qwPAGPHjm12fXPRlnNhyQxxPqqrq1FXV4cOHTq0V5lGo+/5EEIgKSkJWVlZGDFiRHuWKjmGqZXKz8+Hl5eXzjIbGxt06NAB+fn5ElVlHMXFxVCr1fD29tZZ7u3t3eyx5+fnt2p9c9GWc2HJDHE+XnzxRfj5+d3wx5c5auv5KCsrg7OzM+zs7DB+/Hi88847GDNmTHuXKymGqYWZP38+ZDLZTV8nTpyQukwii7R8+XJs2bIFX3/9Nezt7aUuRzIuLi7IyMjA/v378dprryEuLg4pKSlSl9WuOAWbhXn22Wfx8MMP33Sdbt26wcfH54YHCOrr63H58mWLfKjkzzw9PaFQKFBQUKCzvKCgoNlj9/HxadX65qIt58KS6XM+Vq1aheXLl2Pnzp0ICwtrzzKNpq3nQy6Xo3v37gCA8PBwZGZmIj4+HqNGjWrPciXFlqmF6dSpE0JCQm76srOzQ3R0NEpLS5GWlqbd9pdffoFGo0FUVJSER9D+7OzsEBERgaSkJO0yjUaDpKQkREdHN7lNdHS0zvoAsGPHjmbXNxdtOReWrK3nY+XKlVi2bBkSExN1nkMwd4b6+dBoNFCpVO1RoumQ+gkoks64cePEgAEDxN69e8Xu3btFjx49xLRp07TvX7hwQfTq1Uvs3btXu+zSpUsiPT1drF+/XgAQu3btEunp6aKkpESKQ2izLVu2CKVSKT755BNx/Phx8dhjjwl3d3eRn58vhBDioYceEvPnz9euv2fPHmFjYyNWrVolMjMzxeLFi4Wtra04cuSIVIdgMK09FyqVSqSnp4v09HTh6+srnnvuOZGeni6ys7OlOgSDau35WL58ubCzsxNffvmluHTpkvZVUVEh1SEYVGvPx+uvvy5+/vlncfr0aXH8+HGxatUqYWNjI9avXy/VIRgFw9SKlZSUiGnTpglnZ2fh6uoqYmNjdf4DOHv2rAAgkpOTtcsWL14sANzw2rBhg/EPQE/vvPOO6Ny5s7CzsxODBw8Wf/zxh/a9kSNHipkzZ+qs/8UXX4iePXsKOzs70adPH/HDDz8YueL205pz0fhz8dfXyJEjjV94O2nN+ejSpUuT52Px4sXGL7ydtOZ8LFy4UHTv3l3Y29sLDw8PER0dLbZs2SJB1cbFKdiIiIj0xHumREREemKYEhER6YlhSkREpCeGKRERkZ4YpkRERHpimBIREemJYUpERKQnhikREZGeGKZEJi4lJQUymQylpaVG/dxPPvkE7u7ueu3j3LlzkMlkyMjIaHYdqY6PyJAYpkQmZtSoUZg3b57UZRBRKzBMiSxQbW2t1CUQWRWGKZEJefjhh/Hrr7/irbfe0k7mfu7cOQBAWloaIiMj4ejoiCFDhiArK0u73ZIlSxAeHo4PP/wQXbt21U5MXVpailmzZqFTp05wdXXF7bffjkOHDmm3O3ToEG677Ta4uLjA1dUVEREROHDggE5NP/30E0JDQ+Hs7Ixx48bh0qVL2vc0Gg2WLl2KgIAAKJVKhIeHIzEx8abHuH37dvTs2RMODg647bbbtMfX6Pz587j77rvh4eEBJycn9OnTB9u3b2/L6SQyGoYpkQl56623EB0djdmzZ+PSpUu4dOkSAgMDAQALFy7EG2+8gQMHDsDGxgaPPPKIzranTp3CV199hW3btmnvUf7jH/9AYWEhfvzxR6SlpWHgwIEYPXo0Ll++DACYPn06AgICsH//fqSlpWH+/PmwtbXV7rO6uhqrVq3C5s2bsWvXLuTk5OC5557TqfeNN97AqlWrcPjwYYwdOxb33HMPsrOzmzy+3NxcTJo0CXfffTcyMjIwa9YszJ8/X2edp556CiqVCrt27cKRI0ewYsUKODs7631uidqV1NPWEJGukSNHiqefflr7dXJysgAgdu7cqV32ww8/CADi6tWrQgihnV+1sLBQu85vv/0mXF1dRU1Njc7+g4ODxX//+18hhBAuLi7ik08+abKODRs2CADi1KlT2mVr164V3t7e2q/9/PzEa6+9prPdoEGDxJw5c4QQ16drS09PF0IIsWDBAtG7d2+d9V988UUBQFy5ckUIIUS/fv3EkiVLmj0/RKaILVMiMxEWFqb9t6+vLwCgsLBQu6xLly7o1KmT9utDhw6hsrISHTt2hLOzs/Z19uxZnD59GgAQFxeHWbNmISYmBsuXL9cub+To6Ijg4GCdz238zPLycuTl5WHo0KE62wwdOhSZmZlNHkNmZiaioqJ0lkVHR+t8PXfuXLz66qsYOnQoFi9ejMOHD9/8xBCZAIYpkZn48+VXmUwGoOGeZSMnJyed9SsrK+Hr64uMjAydV1ZWFp5//nkADfdajx07hvHjx+OXX35B79698fXXXzf5mY2fK9p5CuRZs2bhzJkzeOihh3DkyBFERkbinXfeadfPJNIXw5TIxNjZ2UGtVuu9n4EDByI/Px82Njbo3r27zsvT01O7Xs+ePfHMM8/g559/xqRJk7Bhw4YW7d/V1RV+fn7Ys2ePzvI9e/agd+/eTW4TGhqKffv26Sz7448/blgvMDAQTzzxBLZt24Znn30W69evb1FNRFJhmBKZmKCgIOzduxfnzp1DcXGxTuuzNWJiYhAdHY2JEyfi559/xrlz5/D7779j4cKFOHDgAK5evYp//vOfSElJwfnz57Fnzx7s378foaGhLf6M559/HitWrEBCQgKysrIwf/58ZGRk4Omnn25y/SeeeALZ2dl4/vnnkZWVhc8//xyffPKJzjrz5s3DTz/9hLNnz+LgwYNITk5uVU1EUmCYEpmY5557DgqFAr1790anTp2Qk5PTpv3IZDJs374dI0aMQGxsLHr27ImpU6fi/Pnz8Pb2hkKhQElJCWbMmIGePXti8uTJuPPOO/HKK6+0+DPmzp2LuLg4PPvss+jXrx8SExPx3XffoUePHk2u37lzZ3z11Vf45ptv0L9/f6xbtw6vv/66zjpqtRpPPfUUQkNDMW7cOPTs2RPvvfdem84BkbHIRHvfACEiIrJwbJkSERHpiWFKRESkJ4YpERGRnhimREREemKYEhER6YlhSkREpCeGKRERkZ4YpkRERHpimBIREemJYUpERKQnhikREZGe/h/huvSlpJ5jDwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.lineplot(x=threshold_values , y=f1_scores)\n",
    "plt.xlabel(\"thresholds\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"Thresholds vs F1-score\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58f336fd-0e81-4954-9d12-3f250646fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold:  0.21589096821794715\n",
      "Best F1-Score:  0.43576234462310415\n"
     ]
    }
   ],
   "source": [
    "print('Best threshold: ', threshold_values[np.argmax(f1_scores)])\n",
    "print('Best F1-Score: ', np.max(f1_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2fbc9c0-dbb1-4bd3-bb36-8cc2000b79f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Info] Number of positive: 116035, number of negative: 1070417\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5965\n",
      "[LightGBM] [Info] Number of data points in the train set: 1186452, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097800 -> initscore=-2.221912\n",
      "[LightGBM] [Info] Start training from score -2.221912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5733670416719333, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5733670416719333\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8385841711914909, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8385841711914909\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9701151207124787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9701151207124787\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.12850035323391018, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.12850035323391018\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "Train F1 Score: 0.44587037057847406\n",
      "Validation F1 Score: 0.4356410709320201\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93   2219245\n",
      "           1       0.50      0.39      0.44    323154\n",
      "\n",
      "    accuracy                           0.87   2542399\n",
      "   macro avg       0.71      0.66      0.68   2542399\n",
      "weighted avg       0.86      0.87      0.87   2542399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a light gradient boosting model.\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Initializing the model\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    num_leaves=74,  # En iyi parametre olarak belirlenen yaprak sayısı\n",
    "    max_depth=7,    # En iyi parametre olarak belirlenen maksimum derinlik\n",
    "    learning_rate=0.0820643967899203,  # En iyi parametre olarak belirlenen öğrenme oranı\n",
    "    min_data_in_leaf=31,  # En iyi parametre olarak belirlenen her yaprakta olması gereken minimum veri sayısı\n",
    "    bagging_fraction=0.9701151207124787,  # En iyi parametre olarak belirlenen bagging oranı\n",
    "    bagging_freq=8,  # En iyi parametre olarak belirlenen bagging sıklığı\n",
    "    feature_fraction=0.8385841711914909,  # En iyi parametre olarak belirlenen özellik oranı\n",
    "    lambda_l1=0.5733670416719333,  # En iyi parametre olarak belirlenen L1 düzenlileştirme\n",
    "    lambda_l2=0.12850035323391018,  # En iyi parametre olarak belirlenen L2 düzenlileştirme\n",
    "    max_bin=242  # En iyi parametre olarak belirlenen maksimum bin sayısı\n",
    ")\n",
    "\n",
    "# Modeli eğitim verileriyle eğitme\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "# Taking the best threshold as 0.21 from above\n",
    "y_pred_train = (lgbm.predict_proba(X_train)[:, 1] >= 0.215).astype('int') #setting a threshold.\n",
    "y_pred_test = (lgbm.predict_proba(X_test)[:, 1] >= 0.215).astype('int') #setting a threshold.\n",
    "\n",
    "# Evaluation\n",
    "print('Train F1 Score: {}'.format(f1_score(y_pred_train, y_train)))\n",
    "print('Validation F1 Score: {}'.format(f1_score(y_pred_test, y_test)))\n",
    "print(\"-\"*50)\n",
    "print(classification_report(y_pred_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad7c929c-ae23-4d67-a687-68e504175644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion, Precision and Recall matrix\n",
    "plot_confusion_matrix(y_test,y_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6d2d4ba-ae5f-41da-b35c-6e9058233c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lgbm, open(\"/Users/data/Desktop/Features@Library/lgbm_model_less.15:27\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e54b10-0d14-4263-89f2-b1a70451e4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e512ce-14c0-4c52-a926-9be4286c559f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee088486-0cb6-4f88-ab3a-992fe55cdca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911223b8-7f11-4b5b-82a6-6628c72046a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac89e58-d6a1-468d-bf3b-d3ed1871ac47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cfad1-2a96-4b37-8ea8-b0cff31099d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd74fe-0960-4e85-99b3-532b22ead806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4727fa5-fab0-4386-9860-7fc324671140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22346267-adc6-40a5-8231-bed372d3d913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a1cf0-9ace-4bdc-8506-3dcef6049072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677bd917-3fce-4cb6-89cf-3323db8b1d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49620b9d-af12-4bc8-9f88-1be3c68136d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410826d-d3d8-4e90-8431-33cd54df5360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e51f68-9f79-4b88-b702-4ac3ee8bdd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739e625-b3ce-4766-847c-ea4d85492afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8891950-e78e-47a4-baaf-938a6ea323eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1269d1-2d2a-4eb4-a35d-e2d0c38daf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d65874-73cc-4824-a5db-0b7a0340daf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213ee6c-d5ee-496d-8c9d-e0acfbb63bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa40e9-7f8a-4ba0-ba1a-939fee88bd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40628c-5028-404b-8f4c-b899c5243ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c3491-feb7-4ad8-845c-5bffb9ca3e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0f0ee-6339-45d8-80e8-7aabd6999e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abff19-f41e-429a-9b89-e5f44139c249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221b628-768e-467d-87cc-e2531f0991a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85899d6e-6bda-4928-8364-8847fa7f2508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490922f2-fea4-4112-a515-c13e883a4cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b301e71-793d-4588-b4b2-0809e6b3d9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5664fea-fbb8-4f4e-bab1-6d42b339fcca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623089db-4582-4399-94b6-0ba0a48ebd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b662327-c845-451d-b42b-47ee7cfa0343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d930482-7502-4cd1-b8ff-b7a3b2ea6dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233994b-796c-4cd3-834a-11fb2da403c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa7221-ad96-408a-92ec-645cbc89f3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576a0d6-b5c3-4833-b3d4-e377d2b0f331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420eef41-ae0b-4d95-b673-522a781a14c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49d0f7-c992-47be-8db6-cdafce34c1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca35713-e204-47d0-95a8-e0ebc0ddfc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408be04-5625-4859-8fde-376cfaee2d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650f564-3811-491d-afe4-db22e6f3105a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc6dd5-505e-44e0-92b1-7570294c2132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795c492-6599-418d-b831-7227725c5cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf39b6c-e0a2-4d79-ae10-f243dc3f176f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10286172-d958-4bb7-8200-0979f276c052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c319b-dca6-4c0a-9ae9-3207a1790e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d3be6-d8c4-4797-8206-320c83af44c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b598e71-8620-4a9b-a8a9-94ea8ce54efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc12e6-1820-49c9-9ae1-54e7378e039f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178069c5-6b8a-4b5a-bf92-ba524d9be1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472f280-f8b5-4606-83b2-79b7fb898655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ab31d-9077-4d16-a854-f0d8939d57dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95e8e8-f163-439d-aa57-abd0022160cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733a73d-f395-4ec0-a60e-802bf8cae2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b1404-3b68-4431-9fc7-8b802dd73c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360874e-7163-4fca-9904-f2b01ae61dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47be43d-89fc-4f46-8332-42c6ab2f71ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551ce15-6761-4511-b887-3e28bda00455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e92a44-5c6b-4c6f-a2d1-859d026b0fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c3dd9-6268-4c1e-ba45-1240c83bd6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad77a10-074c-421b-8450-296c1491908d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33440890-8760-4b46-bdc7-b38c4ee6e61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1bf68-e2f5-4f9c-bdfb-f6a9842e70a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78e5cc-59dd-4532-9d68-6441b529e51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f2cb4-abb3-47f8-afd1-0c11a36cbc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7150cb-7bbd-4ab3-8eb4-c63bf74b5ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3586c7f-c34e-4ab0-83ce-afe0228527b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32e9bc-0550-47db-86eb-55b9ded7c1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054684a1-386b-417d-a80f-a2e2483c9ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417a20a-0353-4ab5-8dc3-d3ef1277b53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa16cae-0980-4bfc-8313-f9a5d911dfba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835849e-1a7e-48e4-89b4-23e8206c72cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea01a9-07ef-43af-84f5-6dc610779549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0e336-23e9-4215-a8c1-78f57f01b335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96be26b-1ef0-4c53-bdd0-8cdfe5ec29eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3a802-7015-4dea-a460-a0e0cf86678e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbf69f-b96c-4602-b2bd-ae85808b7e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df143cae-c112-48e4-8dd4-9fb50cdc9d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3efa0-6fd0-4866-a961-4edeffcfbd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd92ab-5bb3-40b4-80fb-11622a08a797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0db6f-9dd4-4a92-9cd8-73c574583535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85049f4-66f9-4d28-b568-b0c4ca936ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01c4a2-e46f-423d-aa6b-d52a7b7a8d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
